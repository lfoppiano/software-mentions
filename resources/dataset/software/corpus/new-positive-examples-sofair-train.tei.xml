<?xml version='1.0' encoding='UTF-8'?>
<teiCorpus xmlns="http://www.tei-c.org/ns/1.0" version="3.3.0">
  <tei>
<teiHeader>
<fileDesc id="f577073435"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T10:07+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>For pore size and silica aerogel particle dispersion analysis, μCT 100, Scanco Medical AG, Switzerland scanner was used. The scanning voltage was applied to 70 kV, and the X-ray current was set at 200 mA. A 0.5 mm aluminium filter was used to lower the beam's hardness. 205 images with a 2048 × 2048 resolution were generated in a single measurement, each sample was measured three times at different specimen heights, with a voxel size of 10 μm. In preparing binarization processes and separating air voids and aerogel silica particles from the denser geopolymeric matrix, <rs id="a12893280" type="software" subtype="implicit">3D-reconstruction program</rs> was utilized. As a result, in the examination of pore size distribution and open porosity, the two media were regarded as a single-air-derived medium.</p>
<p>The software <rs id="a12893281" type="software">IPLFE</rs> v <rs id="a12893282" type="version" corresp="#a12893281">1.16</rs> ( <rs id="a12893283" type="publisher" corresp="#a12893281">Scanco Medical AG</rs>, Switzerland) is used to analyze the pore distribution of the developed composites on a 3D slice view. The pixel intensity is related to the density of the object, making it possible to separate the surrounding air, air voids and aerogel particles from the denser matrix. However, the density difference between air and silica aerogel is small, making the distinguishment between air and aerogel rather difficult.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f326516503"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:09+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>We used <rs id="a12972666" type="software">STATA</rs> v <rs id="a12972667" type="version" corresp="#a12972666">15</rs> for all statistical analysis. Follow-up time was person-age. Subjects entered and exited at their age measured in days. Time extended from the date of study entry to the date of death, or end of follow-up on 31 December 2017. We used Cox proportional hazard regression to estimate HRs of death using scores for exact age in days at the first survey in which participants took part in addition to SRH, BMI, comorbid disease, and physical examination scores as time-dependent covariates. All scores were updated in 2001 and 2007/8 for those who participated. The primary model in Table 3 includes repeated measures of SRH in an attempt to capture changes in self-reported health over time.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f111009878"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T14:15+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Recent advances in segmentation <rs type="software" subtype="implicit" id="a12971468A">software</rs> have made it increasingly easy to automatically or semi-automatically extract the surface of structures of interest from three-dimensional (3D) medical imaging data. This has made it possible to generate anatomical models using a standard personal computer with little prior anatomical knowledge. At the same time 3D printers, traditionally used in industrial applications, are now available for home use thanks to low-cost desktop alternatives. This technology enables fast creation of 3D models without the need for classical manufacturing expertise.</p>
<p>There is a wide range of software that is capable of performing image segmentation, ranging from multi-purpose commercial platforms with integrated physics simulations (e.g. <rs id="a12971468" type="software">Mimics</rs> <rs id="a12971469" type="bibr">[20]</rs>, or <rs id="a12971470" type="software">Simpleware</rs> <rs id="a12971471" type="bibr">[21]</rs>), to open-source tools targeted to specific organs (e.g. <rs id="a12971472" type="software">FreeSurfer</rs> <rs id="a12971473" type="bibr">[22]</rs> for the brain). In this work, we have used the freeware software packages called <rs id="a12971474" type="software">Seg3D</rs> <rs id="a12971475" type="bibr">[23]</rs> and <rs id="a12971476" type="software">3D Slicer</rs> <rs id="a12971477" type="bibr">[24]</rs>, as they are capable of processing a range of medical imaging data. Furthermore, we provide a summary of comparable freeware software available at the time of writing in the discussion.</p>
<p>The models of the ribs and liver were segmented using <rs id="a12971478" type="software">Seg3D</rs> (v. <rs id="a12971479" type="version" corresp="#a12971478">2.2.1</rs>) while the lung was segmented using <rs id="a12971480" type="software">3D Slicer</rs> (v. <rs id="a12971481" type="version" corresp="#a12971480">4.6</rs>). We smoothed the models using <rs id="a12971482" type="software">MeshMixer</rs> <rs id="a12971483" type="bibr">[27]</rs> (v. <rs id="a12971484" type="version" corresp="#a12971482">3.0</rs>) and printed all of them using Filament Deposition Modelling (FDM). All processing was done using
Windows 10 as operating system.</p>
<p>Image segmentation. <rs id="a12971485" type="software">Seg3D</rs> v. <rs id="a12971486" type="version" corresp="#a12971485">2.2.1</rs> was used to generate the rib model from the CT MECANIX dataset (Siemens Sensation 64, 3 mm slice thickness, 0.56 mm by 0.56 mm pixel size, 120 kV peak kilo-voltage, 100 mAs exposure) available on the <rs id="a12971487" type="software">OSIRIX</rs> website <rs id="a12971488" type="bibr">[28]</rs>. The ribs were segmented using thresholding, manual modification (cropping), a connected-component-filter as well as a fill-holes-filter. The segmentation was exported as a stereolithography (STL) file using the "Export Isosurface" command. • Rippled and porous surface • Fragile along z-axis</p>
<p>• Vessels [12,25] • Spine [26] Powder Binding: Binder Jetting (BJ) • Brain [16] • Heart [10] https://doi.org/10.1371/journal.pone.0178540.t001 <rs id="a12971489" type="software">3D
Slicer</rs> v. <rs id="a12971490" type="version" corresp="#a12971489">4.6</rs> was used to create a model of the liver and the right lung from the CT ARTI-FIX dataset (Siemens Sensation 64, 1.5 mm slice thickness, 0.59 mm by 0.59 mm pixel size, 120 kV peak kilo-voltage, 300 mAs exposure) from the <rs id="a12971491" type="software">OSIRIX</rs> website <rs id="a12971492" type="bibr">[28]</rs>. This was done using the level tracing algorithm as well as manual modification. After segmentation, the "make model" tool was used to export the volume as a STL file.</p>
<p>Mesh refinement. The ribs model was refined using <rs id="a12971493" type="software">Meshmixer</rs> to improve its topology. This was done by adjusting the mesh density of the surface and by applying a global smoothing filter that removes step artefacts due to the finite voxel size. Furthermore, we have used <rs id="a12971494" type="software">Free-CAD</rs> (v. <rs id="a12971495" type="version" corresp="#a12971494">0.16</rs>) <rs id="a12971496" type="bibr">[29]</rs> to design a holder for a tissue phantom. As <rs id="a12971497" type="software">FreeCAD</rs> has limitations working with large mesh files, the holder was attached to the ribs model structure in another software package called <rs id="a12971498" type="software">Blender</rs> (v. <rs id="a12971499" type="version" corresp="#a12971498">2.6</rs>) <rs id="a12971500" type="bibr">[30]</rs>.</p>
<p>The lung model was also smoothed using <rs id="a12971501" type="software">MeshMixer</rs>. The different segmentation method demanded a local smoothing approach utilising the "RobustSmooth" brush provided by the software. Furthermore, the "Flatten" and "Inflate" brushes were used to remove unphysiological holes in the model.</p>
<p>3D printing. We have used an Ultimaker 2 (Ultimaker, Chorley, England) FDM printer to create our models. They were prepared for the printer using the open-source slicing software <rs id="a12971502" type="software">Cura</rs> (v. <rs id="a12971503" type="version" corresp="#a12971502">15.04.6</rs>), which is provided for free by Ultimaker. All models were printed with a layer height of 0.12 mm and a shell, bottom, and top thickness of 0.8 mm with a nozzle size of 0.4 mm. The prints were created at 20% infill, except for the rib model, which was printed with 100% infill to be functionally similar to bone when viewed under a medical ultrasound scanner (Siemens Acuson S1000 with a 16 MHz ultrasound probe). The material used for printing was "enhanced Polymax" polylactic acid (PLA) (PolyMax; Polymakr, Changshu, China). To estimate the print accuracy, the dimensions of the models were quantified at different sites in silico using <rs id="a12971504" type="software">Meshmixer</rs> and compared to the dimensions of the 3D prints, which were measured using calipers and a micrometer.</p>
<p> <rs id="a12971505" type="software">Seg3D</rs> has both manual and automatic segmentation tools and provides a library of addons with additional algorithms and applications optimised for particular segmentation applications. A key feature is that the interface makes it possible to visualise images in 3D with multiple volumes managed as layers. This facilitates the manipulation of several segmentations, which is particularly useful when it is necessary to use a combination of segmentation techniques to obtain the final surface. For example, the images may be cropped prior to more advanced segmentation processes in order to isolate the volume of interest. Furthermore, it provides Boolean transforms to combine multiple segmentations into a single surface. <rs id="a12971506" type="software">Seg3D</rs> also provides the option of exporting the final segmentation to the STL file format.</p>
<p> <rs id="a12971507" type="software">3D Slicer</rs> has a multitude of other image manipulation options and can be used to register different scans to each other. Because the large range of tools provided by the software, the interface is more difficult to master. However, it provides a range of powerful segmentation algorithms and has a unique selection of extensions available, which can be utilised for more specific tasks.</p>
<p>The datasets used in this publication ("
MECANIX" and "
ARTIFIX" from the
Osirix Website:
http://www.osirix-viewer.com/ resources/dicom-image-library/) were freely available at the time of writing. <rs id="a12971508" type="software">Osirix</rs> has since changed access rights and requires a paid membership. As such, we are not authorised to distribute the datasets used for this study. However, the data is still publicly available but not free of charge.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f193057378"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T05:59+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>We recruited 61 participants for each of Experiments 1a and 1b through Amazon's Mechanical Turk. The mean age was 32.9 years (SD = 11.6) in Experiment 1a and 31.7 years (SD = 11.1) in Experiment 1b. There were 35 females (57.4%) in Experiment 1a and 34 (55.7%) in Experiment 1b. In this and all experiments, participants were paid $0.50 for taking part, and the experiments were programmed in <rs id="a12901052" type="software">Adobe Flex</rs> <rs id="a12901053" type="version" corresp="#a12901052">4.6</rs> and conducted over the Internet (all experiments and the collected raw data can be seen at http://goo.gl/0rMOQd).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f148332242"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:52+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Although we work in angular units, we performed an analysis that is as close as possible to the analysis presented in vdB16, where data from the Canada-France-Hawaii Telescope (CFHT) were analysed. As a first step, we used <rs id="a12971915" type="software">SExtractor</rs> <rs id="a12971916" type="bibr">(Bertin &amp; Arnouts 1996)</rs> on the r-band stack to detect sources, using a Gaussian filter with a FWHM of 5 pixels (∼1 with our detector) to improve our sensitivity towards faint extended galaxies. The KiDS pixel scale is slightly coarser than CFHT MegaCam (0.214 /pix versus to 0.185 /pix), so we require sources to have at least 15 adjacent pixels that are at least 0.90 sigma above the background. These choices keep the purity of detecting real objects (and not noise fluctuations) high (cf. vdB16). We detected a total of 12 243 224 sources in the KiDS fields.</p>
<p>We applied liberal selection criteria to these <rs id="a12971917" type="software">SExtractor</rs> output catalogues to perform a first selection of large extended galaxies (and thus potential UDGs at the redshifts of the GAMA groups). We selected against stars and other compact objects, requiring that r 2 &gt; 0.9 + r 7 , where r x is the r-band magnitude within a circular diameter of x arcsec. This pre-selection was merely intended to speed up the overall analysis. We performed several tests to ensure that this cut did not affect the final selection of galaxies that we aimed to study, even if a fraction of UDGs is nucleated (cf. Yagi et al. 2016). After applying the KiDS survey mask, we were left with 873 702 sources in the considered KiDS area.</p>
<p>These sources were our input to <rs id="a12971918" type="software">GALFIT</rs> <rs id="a12971919" type="bibr">(Peng et al. 2002)</rs>, which we used to estimate morphological parameters while masking any pixels belonging to neighbouring sources. We then selected galaxies with best-fit Sérsic parameters in the range 24.0 ≤ µ(r, r eff ) ≤ 26.0 mag arcsec -2 , circularised effective radii 1 . 5 ≤ r eff ≤ 8 . 0 that are fit within 7 pixels from the <rs id="a12971920" type="software">SExtractor</rs> position, and have a Sérsic index n ≤ 4. We note that the surface brightness criterion is somewhat shallower (26.0 versus 26.5) than the criterion applied in vdB16 because we work with shallower data here. Some examples of selected galaxies are shown in Appendix B.</p>
<p>We find that <rs id="a12971921" type="software">GALFIT</rs> provides reasonable fits to the data in most cases; we therefore trust the <rs id="a12971922" type="software">GALFIT</rs> parameters, and the selection can be considered as being quite pure. We did not make any additional subjective or by-eye checks for individual sources, so that this study remains reproducible. We assume that any residual impurity does not correlate with the locations of groups and is eventually removed (in a statistical sense) by our background subtraction. Given that KiDS is untargeted, this is a reasonable assumption. To test the performance of our procedure and to gauge the completeness of our final UDG candidate selection, we also performed all processing steps on a set of tailored image simulations, as described next.</p>
<p>Figure 2 shows the completeness of the simulated sources. The completeness is defined as the fraction of injected sources, in bins of size and surface brightness, that are 1) detected with <rs id="a12971923" type="software">SExtractor</rs>, 2) pass the pre-selection based on the <rs id="a12971924" type="software">SExtractor</rs> output, and 3) have best-fit morphological parameters corresponding to the selection criteria we outlined above. These simulated sources were thus processed through exactly the same pipeline as the "real" sources. While only 2% of "real" sources that were fed to <rs id="a12971925" type="software">GALFIT</rs> satisfy these criteria, the recovery rate of simulated sources is ∼66%. Most "real" sources that did not match the final selection criteria are either too small (the pre-selection criteria on the <rs id="a12971926" type="software">SExtractor</rs> output catalogues were very inclusive, as outlined above) or too bright (there was no pre-selection on brightness).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f186800531"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T07:02+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>To enable the use of the sensor for developers and researchers, the official <rs id="a12970087" type="publisher" corresp="#a12970088">Microsoft</rs> <rs id="a12970088" type="software">SDK</rs> <rs id="a12970089" type="version" corresp="#a12970088">2.0</rs> (<rs id="a12970090" type="software">Software Development Kit</rs>) is free downloadable. It provides not only the drivers, but also a set of functions or code samples that can be used for own implementations. One should also note the introduction of the <rs id="a12970091" type="software" subtype="component" corresp="#a12970092">Kinect Fusion</rs> tool in this <rs id="a12970092" type="software" subtype="environment">SDK</rs>, which was initially released by <rs id="a12970093" type="publisher" corresp="#a12970092">Microsoft</rs> for the Kinect v1 because of the success of its device. This tool enables thanks to ICP algorithms the direct creation of 3D meshes under different formats (.stl, .obj or .ply) by using the Kinect as a scanning device moved slowly around the object.</p>
<p>To determine the necessary intrinsic parameters, our dataset was treated by a <rs id="a12970094" type="software" subtype="component" corresp="#a12970096">Camera Calibration Toolbox</rs> proposed by (<rs id="a12970095" type="publisher" subtype="person" corresp="#a12970096">Bouguet, 2013</rs>) under the <rs id="a12970096" type="software" subtype="environment">Matlab</rs> software. This algorithm is mainly based on well-known camera models. However, one should underline the fact that changing or removing some of the images makes the computed results vary from a few pixels. This phenomenon is only due to the low sensor resolution. As a matter of fact, best calibration results are considered regarding the lower uncertainties on the parameters.</p>
<p>Note that the <rs id="a12970097" type="publisher" corresp="#a12970098">Microsoft</rs> <rs id="a12970098" type="software" subtype="environment">SDK</rs> mentioned earlier also provides a function that returns all these intrinsic parameters (namely <rs id="a12970099" type="software" subtype="component" corresp="#a12970098">GetDepthCameraIntrinsics()</rs> function). Deviations towards selfcomputed results are shown in Table 9.</p>
<p>Another advantage is the simultaneous release of Kinect sensor and of its official <rs id="a12970100" type="software">SDK</rs>, which allows development of solutions for many tasks such as algorithms for 3D reconstruction.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f571289584"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:35+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>While those with active infection use a saliva swab, those with previously undetected cases would use an "at-home" finger prick test to check for coronavirus antibodies. However, the availability of test kits for mass screening remains a challenge. Other African countries, such as South Africa, have adopted mobile text messages and the <rs id="a12965684" type="software">WhatsApp</rs> platform to assist in predictive modelling and planning an appropriate response for reported cases.</p>
<p>Human mobility tools [15] which provide location tracing services are some potential ways to collect self-reported data and for tracing infection within rural settings where network coverage and internet infrastructure are poor. The use of <rs id="a12965685" type="software">Google Location History (GLH)</rs> for instance, provides a way of collecting human mobility data [16]; however, the underutilization of <rs id="a12965686" type="software">GLH</rs> data by mobile apps is one of the challenges hindering the availability of human mobility data to understand human movement [16]. Additionally, there is a lack of interoperable apps to ensure contact tracing of people as they travel between different countries using different tools.</p>
<p>For distributed stream processing inspired by the <rs id="a12965687" type="software">MapReduce</rs> model <rs id="a12965688" type="software">Flink</rs> Supports batch processing <rs id="a12965689" type="software">Apache Spark</rs> For stream processing that can process large volumes of data in memory with limited time. Hybrid processing Supports both batch and stream processing Stream data processing requires real-time analytics and considering the geographical dimension of recorded cases of
COVID-19, big data platforms can enable data collection and storage in scattered datasets [32]. <rs id="a12965690" type="software">Storm</rs>, <rs id="a12965691" type="software">Splunk</rs> and <rs id="a12965692" type="software">Apache</rs> are real-time stream processing platforms with low latency response. <rs id="a12965693" type="software">Storm</rs> uses a mapping strategy to interconnect data from different data sources for real-time processing. The advantage is a stable distributed environment that it provides using "master" and "work node" to allow delegation of the parallel processing mechanism to its analytics framework [33]. <rs id="a12965694" type="software">S4</rs> is a general-purpose, distributed environment that has a pluggable computing framework for easy development of big data processing applications. <rs id="a12965695" type="software">Kafka</rs> is an open-source framework that is used for the collection of logs. <rs id="a12965696" type="software">Flink</rs> is a dispersed processing platform for stream processing where the processing of data is performed as "cyclic data flow with several iterations". By using the set of iterations, it reduces the data computation time. <rs id="a12965697" type="software">Apache Spark</rs> uses machine learning algorithms for clustering and classification. With <rs id="a12965698" type="software">Spark</rs>, data are mapped, reduced and filtered by passing information to the spark runtime framework. Finally, the hybrid processing is based on Lambda architecture, which is a data processing architecture for massive data processing. The architecture supports both batch and stream processing of data, and the results of processing are merged by combining batches and the real-time view of batch [34].</p>
<p>AI and ML have been adopted in the screening of populations of persons to assess the risk of infection [44]. For instance, such AI-powered temperature screening was deployed in public locations in the COVID-19 pandemic by China. Temperature screening has helped to detect symptoms and isolate persons who are suspected to be infected. Recently, thermal cameras have been used to provide thermal imaging for body temperature for COVID-19 case detection, which helps to quickly and accurately identify people who have "elevated body temperatures", which is one of the key symptoms of COVID-19 [45]. Additionally, an <rs id="a12965699" type="software" subtype="implicit">AI-powered smartphone app</rs> was developed to track the geographical spread of COVID-19. Such apps are aimed at predicting which population and communities are the most susceptible; to enable real-time information dissemination from the medical health providers; and to notify individuals of potential infection hotspots in real-time so as to avoid such areas [44].</p>
<p>Other AI-based technologies include <rs id="a12965700" type="software">BlueDot</rs> to find emerging risk of infection <rs id="a12965708" type="bibr">[46]</rs>, <rs id="a12965702" type="software" subtype="implicit">chatbots</rs> used as virtual assistants to provide information on the virus; and diagnostic robots used to diagnose disease.</p>
<p>In the study by Rahimzadeh and Attar [51], a training technique that combines <rs id="a12965703" type="software">Xception</rs> and the <rs id="a12965704" type="software">RasNet50V2</rs> network to improve the accuracy of detecting COVID-19 from 11,302 images yielded an accuracy of 99.56% and the overall accuracy for all classes was 91.4%.</p>
<p>Artificial intelligence technologies are used to track and identify the spread of the COVID-19 pandemic. The smartphone technologies that use AI have been deployed to trace a person's location and also to scan public space for potentially affected persons through the use of fever detecting infra-red cameras, computer vision surveillance and facial recognition systems. Countries such as South Korea and Singapore have implemented a contact tracing <rs id="a12971416" type="software" subtype="implicit">app</rs> for COVID-19. In the case of South Korea, GPS phone tracking, credit card history, surveillance video recording and personal interviews with patients were used for the contact tracing of COVID-19 infection. Additionally, both countries have implemented contact tracing using location information on smartphone apps. Meanwhile, in Singapore, <rs id="a12965706" type="software">TraceTogether</rs> is a mobile application that enables community-driven contact tracing where devices exchange their proximity information and duration through Bluetooth signal whenever their app detects another phone with the app installed on it. With such a community-driven app, "no location information" is the collection, however, the individual is legally bonded to give any information stored by their phone that might assist with contact tracing. As COVID-19 has spread very quickly, contact tracing technologies can help to speed up the process of tracing the infection. Further, AI models have been used to monitor crowd information to enforce a physical distancing guide. The challenge with AI-based technologies in terms of monitoring crowd information raises some privacy issues which should be addressed by the developers of AI technology.</p>
<p>ABC algorithm-based clustering approach for big data, which identifies the best cluster and performs the optimization for different dataset sizes. ABC algorithm minimizes the execution time and improves the accuracy of clustering when implemented on a map/reduce-based <rs id="a12965707" type="software">Hadoop</rs> environment. ABC algorithm has been applied in training neural networks for pattern recognition [83].</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f287586231"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T10:09+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Robotic μLESA. Derivatized oxysterols present in 10-μm thick slices of brain tissue were sampled using a TriVersa Nanomate (Advion) with a modified capillary extraction probe (LESA PLUS ). The <rs id="a12893271" type="publisher" corresp="#a12893272">Advion</rs> <rs id="a12893272" type="software">ChipsoftX</rs> software was used to capture an optical image of the prepared slide on a flatbed scanner prior to analysis. The same software was also used to define extraction points on the tissue prior to analysis. A 330-μm i.d./794-μm o.d. FEP sleeve was attached to a 200-μm i.d./360-μm o.d. fused silica capillary held in the mandrel of the Advion TriVersa Nanomate; this created a seal when in contact with tissue, preventing the dispersion of extraction solvent (50% methanol) and limiting the sampling area to the internal diameter of the FEP sleeve (SI Appendix, Fig. S3 A and B).</p>
<p>Statistics. For three WT mice, and separately for the three knockout mice, unreplicated two-way ANOVA was performed with sterol areal density as dependent variable and mouse and brain region as factors. The interaction between mouse and brain region was used as error variance. The residuals representing the interaction deviations were approximately normally distributed. Tukey's multiple comparisons test was used to identify significant differences between brain regions. Hierarchical cluster analysis using between-groups linkage was used to represent differences and similarities of average sterol areal densities between brain regions averaged across the three biological replicates for each type of mouse, and with the differences represented as squared Euclidean distances. The analyses were performed using IBM <rs id="a12893273" type="software">SPSS Statistics</rs> <rs id="a12893274" type="version" corresp="#a12893273">22</rs> ( <rs id="a12893275" type="publisher" corresp="#a12893273">IBM Corp</rs>) and <rs id="a12893276" type="software">GraphPad
Prism</rs> <rs id="a12893277" type="version" corresp="#a12893276">7.02</rs> software ( <rs id="a12893278" type="publisher" corresp="#a12893276">GraphPad Software Inc</rs>). *P &lt; 0.05, **P &lt; 0.01, ***P &lt; 0.001. Whiskers on bar graphs represent 1 SD. Data Availability. Datasets generated during this study are available in the Open Science Framework repository under the identifier https://osf.io/vud84/.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f81917551"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T05:59+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The known articles (Fig. 1a, bold circles) were identified in the Web of Science database. Articles that cited a known article (which Web of Science calls "citing articles"; Fig. 1a,squares A,B,C) were saved to the "Marked list". This list was downloaded with the full bibliographic details of each article, including the cited references (regular and dashed circles), and saved in a <rs id="a12900049" type="publisher" corresp="#a12900050">Microsoft</rs> <rs id="a12900050" type="software">Excel</rs> file. The list of citing articles naturally includes the published meta-analysis. We removed the meta-analysis itself and all articles with a more recent publication date from the list and excluded them from the rest of the analyses.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f162674716"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:47+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The statistical analyses were done with <rs id="a12951673" type="software" subtype="environment">R Studio</rs> ( <rs id="a12951674" type="publisher" corresp="#a12951673">R Core Team</rs>), using the packages <rs id="a12951675" type="software" subtype="component" corresp="#a12951673">lmer4</rs> <rs id="a12970836" type="bibr">(Bates et al. 2014)</rs> and <rs id="a12951677" type="software" subtype="component" corresp="#a12951673">lmerTest</rs> <rs id="a12970837" type="bibr">(Kuznetsova 2015)</rs> for Mixed Effects Models. The optimal models were identi ied in a step-up fashion, through ANOVA comparisons of the individual models. The dependent variables were the presence of pre-aspiration (with two levels: 'yes' for present, 'no' for absent), the presence of breathiness (with two levels, the same as those for pre-aspiration), the duration of pre-aspiration (raw as well as normalised as a percentage of the word duration), and the duration of breathiness (again raw and normalised), respectively.</p>
<p>As is visible from the igures, the difference in the duration of preaspiration and breathiness according to the type of C₁ is rather marginal in comparison to the difference found in Mongolian. This is the case even ¹⁴ Lexical frequency of the words analysed was determined via <rs id="a12951679" type="software">SUBTLEX-UK</rs> <rs id="a12970838" type="bibr">(Van Heuven et al. 2014)</rs>. when raw data are considered: the difference between the mean between the lenis and the [+spread glottis] categories is 10ms for pre-aspiration and 5ms for breathiness. This effect is stable across segmental contexts: there is no signi icant interaction with place of articulation. It is also constant across prosodic conditioning (with the exception of vowel length; see below): we found no difference between the word-medial and word-inal positions, nor between the words spoken in isolation and the words in a carrier sentence.¹⁵ Importantly, the frequency of occurrence of pre-aspiration and breathiness, as well as their duration, are not sensitive to lexical frequency in the data. Together with the fact that no word-speci ic effect is found, in a corpus of 190 word types, these results show that gradient aspiration dissimilation in Aberystwyth English presents a lexically regular, i.e. exceptionless, phenomenon.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f199221851"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:17+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>We measure the learnability of a language by counting the number of times (out of a maximum of 24) that the participant trained on that language used the correct noun affix. Figure 4 shows how the learnability of languages in the experiment increases over generations. We used a linear mixed effects model in <rs id="a12951812" type="software">R</rs> ( <rs id="a12951813" type="publisher" corresp="#a12951812">R Core Team</rs> <rs id="a12951814" type="version" corresp="#a12951812">2012</rs>) and <rs id="a12951815" type="software">lme4</rs> <rs id="a12951823" type="bibr">(Bates, Maechler &amp; Booker 2013)</rs> to analyse the relationship between learnability and generation. We entered a fixed effect of generation and a random intercept for chain, and by-chain random slopes for the effect of generations. Using a likelihood ratio test to compare the full model with a reduced model without the fixed effect revealed that generation affected the learnability of languages (χ 2 (1)=12.56, p&lt;.001).</p>
<p>This set of distances implies a space of noun meanings with very high dimensionality. We reduce these down to two dimensions using the <rs id="a12951817" type="software" subtype="component" corresp="#a12951818">isoMDS</rs> function in the <rs id="a12951818" type="software" subtype="component" corresp="#a12951819">MASS</rs> package in <rs id="a12951819" type="software" subtype="environment">R</rs>. Figure 6 shows the resulting position of the nouns in the 2d space found by the MDS algorithm. Several clusters can be seen clearly in this diagram that relate to animacy: humans, land animals, plants, and natural forces, for example. These clusters themselves appear to be organised on a larger dimension of something akin to "aliveness" from roughly right to left in the plot, with the natural forces being least "alive" and humans most alive. In addition, although the dimensions in an MDS plot are not meaningful, nevertheless some orthogonal subhierarchies are apparent in the plot. Within each cluster there appears to be a vertical dimension which we might describe as "potency", cross-cutting the "aliveness" distinction, yielding sub-hierarchies such as: adult &gt; teenager &gt; child &gt; baby, and wind &gt; fire &gt; lightning &gt; rain.</p>
<p>An alternative method for clustering the data is to use hierarchical cluster analysis on the distance matrix between nouns. Figure 7 shows the result of clustering using the Ward method (from the <rs id="a12951820" type="software" subtype="component" corresp="#a12951822">linkage</rs> function in the <rs id="a12951821" type="software" subtype="environment">Python</rs> <rs id="a12951822" type="software" subtype="component" corresp="#a12951821">SciPy</rs> cluster library). The major split that this method brings out of the data is the one between higher animates (including humans) vs all other nouns, but also visible are the clusters of humans and natural forces, seen in the MDS analysis as well. Teddy appears to be treated as a higher animate in this analysis, clustering alongside bear.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f288396461"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:35+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>One of the most used methods of hot spot analysis is the <rs id="a12897829" type="software">FTMap</rs> web server <rs id="a12899243" type="bibr">(Kozakov et al., 2015)</rs>. This algorithm places 16 small organic probe molecules of different shape, size, and polarity on the protein surface to find favorable positions for each probe. Then, each probe type is clustered and overlapping clusters of different probes, called consensus sites (CSs), represent the hot spots. The consensus sites are ranked by the number of probe clusters, and the main hot spot is, generally, where the fragment hit binds and secondary hot spots are used to extend the fragment in the best direction (Hall et al., 2012;Ngan et al., 2012;Kozakov et al., 2015).</p>
<p>As an example, we used the <rs id="a12897831" type="software">FTMap</rs> server for predicting the hot spots for the oncogenic B-RAF kinase, the target of the first marketed drug from fragment-based drug design, vemurafenib <rs id="a12899244" type="bibr">(Bollag et al., 2012)</rs>. Figure 5A shows the fragment hit experimentally bound to B-RAF kinase (PDB ID: 2UVX) (Donald et al., 2007), and the predicted hot spots around this fragment (shown in yellow dots) using the <rs id="a12897832" type="software">FTMap</rs> server. In Figures 5B-D, the iterative process of growing the fragment hit led to the discovery of the drug vemurafenib (PDB ID: 3OG7) (Bollag et al., 2010) with the hot spots shown in yellow dots. Although hot spot analysis was not used in the F2L process of vemurafenib, the results here showed that the predicted hot spots overlap the grown portions of vemurafenib.</p>
<p>De novo design software takes advantage of a known binding mode of a fragment, described experimentally or computationally, to propose modified analogs with improved binding affinities. The <rs id="a12897833" type="software">LUDI</rs> <rs id="a12899245" type="bibr">(Bohm, 1992)</rs> program was one of the first programs developed for de novo design. It calculates the interaction sites, maps the molecular fragments, and connects them using bridges, using an empirical scoring. Considering the vast chemical space, evolutionary algorithms are widely used (Srinivas Reddy et al., 2013). In this context, the program <rs id="a12897835" type="software">GANDI</rs> <rs id="a12899246" type="bibr">(Dey and Caflisch, 2008)</rs> connects pre-docked fragments with linker fragments using a genetic algorithm and a tabu search. The scoring function is a linear combination of force-field binding energy and similarity measures.
BREED (Pierce et al., 2004) is a computational method for merging fragments that is widely used. It aligns the 3D coordinates of two ligands and recombines the fragments or substructures into the overlapping bonds to generate new hybrid molecules in a strategy called fragment shuffling. <rs id="a12897837" type="software">LigBuilder</rs> <rs id="a12899247" type="bibr">(Wang et al., 2000;Yuan et al., 2011)</rs> is a program that uses a genetic algorithm to build up the ligands using a library of organic fragments. It contemplates the growing and linking approach. The <rs id="a12897840" type="version" corresp="#a12897837">2.0</rs> version includes the synthesis accessibility analysis through a chemical reaction database and retro-synthetic analysis. <rs id="a12897841" type="software">Autogrow</rs> <rs id="a12899248" type="bibr">(Durrant et al., 2009(Durrant et al., , 2013)</rs> ) is another growing approach algorithm that builds a fragment upon a "core" scaffold. The fragment is docked to the receptor. A genetic algorithm evaluates the docking score to select the best population which forms the subsequent generation. The last version considers the synthetic accessibility and the druggability. The program <rs id="a12897843" type="software">ADAPT</rs> <rs id="a12899249" type="bibr">(Pegg et al., 2001;Srinivas Reddy et al., 2013)</rs> applies a genetic algorithm which uses molecular interactions and docking calculations as a fitness function to reduce the search space. The initial sets of compounds are iteratively built until it reaches the predefined target value.</p>
<p>The ADMET properties and synthetic accessibility (SA) constitutes the secondary constraints whereas primary constraints are geometric and chemical constraints derived from the receptor or target ligand(s) and internal constraints to the geometry and chemistry of the lead compound being constructed. Issues with these points result in the majority of clinical trial failures (Dong et al., 2018). Numerous software and web platforms were developed to predicted ADMET parameters but presented limitations due to narrow chemical space coverage or expensive prices (Cheng et al., 2012). Recent works predominantly rely on ML methods, like random forest (RF), support vector machine (SVM), and tree-based methods (Ferreira and Andricopulo, 2019). The <rs id="a12897846" type="software">vNN
Web Server for ADMET predictions</rs> <rs id="a12899250" type="bibr">(Schyman et al., 2017)</rs> is a publicly available online platform to predict ADMET properties and to build new models based on the k-nearest neighbor (k-NN), which rest on the premise that compounds with similar structures have similar activities. <rs id="a12897847" type="software">vNN</rs> uses all nearest neighbors that are structurally similar to define the model's applicability domain. The similarity distance employed is Tanimoto's coefficient. The platform allows running pre-build ADMET models, and to build and run customized models. Those models assess cytotoxicity, mutagenicity, cardiotoxicity, drug-drug interactions, microsomal stability, and likelihood of causing drug-induced liver injury. Like all machine learning methods, the lack of training data is a limitation.</p>
<p><rs id="a12897848" type="software">Pred-hERG</rs> <rs id="a12899251" type="bibr">(Braga et al., 2015;Alves et al., 2018)</rs> is a web app that allows users to predict blockers and non-blockers of the hERG channels, and important drug anti-target associated with lethal cardiac arrhythmia (Mitcheson et al., 2000). The current version of the app (v. <rs id="a12897850" type="version" corresp="#a12897848">4.2</rs>) was developed using <rs id="a12897851" type="software">ChEMBL</rs> <rs id="a12899252" type="bibr">(Willighagen et al., 2013)</rs> version <rs id="a12897853" type="version" corresp="#a12897851">23</rs>, containing 8,134 compounds with hERG blockage data after curation, using robust and predictive machine learning models based on RF. This app is publicly available at <rs id="a12897854" type="url" corresp="#a12897848">http://labmol.com.br/predherg/</rs>.</p>
<p>In <rs id="a12897855" type="software">admetSAR</rs> <rs id="a12897856" type="version" corresp="#a12897855">2.0</rs> <rs id="a12899253" type="bibr">(Cheng et al., 2012;Yang et al., 2019a)</rs> tool, the predictive models are built using RF, SVM and kNN algorithms. It presents 27 endpoints and also includes ecotoxicity models and an optimization module called ADMETopt that optimize the query molecule by scaffold hopping based on ADMET properties. The <rs id="a12897858" type="software">ADMETlab</rs> platform <rs id="a12899256" type="bibr">(Dong et al., 2018)</rs> performs its evaluations based on a database of collected entries and assess drug-likeness evaluation, ADMET prediction, systematic evaluation and database/similarity searching. It uses 31 endpoints applying RF, SVM, recursive partitioning regression (RP), naive Bayes (NB), and decision tree (DT).</p>
<p><rs id="a12897860" type="software">SwissADME</rs> tool <rs id="a12899255" type="bibr">(Daina et al., 2017)</rs> uses predictive models for physicochemical properties, lipophilicity and water solubility. It also analyses pharmacokinetics models as BBB permeability, gastrointestinal absorption, P-gp binding, skin permeation (logKp), and CYP450 inhibition. Additionally, the tool presents five drug-likeness models (Lipinsky, Ghose, Veber, Egan, and Muegge) and medicinal chemistry alerts. It is integrated with the <rs id="a12897862" type="software">SwissDrugDesign</rs> workspace. The <rs id="a12897863" type="software">QikProp</rs> ( <rs id="a12897864" type="publisher" corresp="#a12897863">Schrödinger, LLC</rs>, NY, 2019) provides rapid predictions of ADME properties for molecules with novel scaffolds as for analogs of well-known drugs and display information about octanol/water and water/gas logPs, logS, logBBB, overall CNS activity, Caco-2 and MDCK cell permeabilities, log Kd for human serum albumin binding, and log IC 50 for HERG K+-channel blockage.</p>
<p>Even though large numbers of molecules are generated by de novo design, many of them are synthetically infeasible (Dey and Caflisch, 2008). To address this problem, methods to calculate the synthetic accessibility (SA) are being developed. SA can be addressed by estimating the complexity of the molecule or making a retrosynthetic approach, where the complete synthetic tree leading to the molecules needs to be processed (Ertl and Schuffenhauer, 2009). <rs id="a12897865" type="software">SYLVIA</rs> <rs id="a12899257" type="bibr">(Boda et al., 2007)</rs> is one of the programs that estimate the synthetic accessibility of an organic compound. It obtains the SA score by the addition of five variables as the molecular graph complexity, ring complexity, stereochemical complexity, starting material similarity and reaction center substructure, where the first three are structure-based and the other two utilize information from starting material catalogs and reaction databases. Ertl and Schuffenhauer (2009) developed another method that uses historical synthetic knowledge obtained by analyzing information from millions of already synthesized chemicals and also considers molecule complexity. The method is based on a combination of fragment contributions and a complexity penalty. Podolyan et al. (2010) presented two approaches to quickly predict the synthetic accessibility of chemical compounds by utilizing SVMs operating on molecular descriptors. The first approach (RSsvm) identifies compounds that can be synthesized using a specific set of reactions and starting materials and builds the model by training the compounds identified as synthetically or otherwise accessible by retrosynthetic analysis while the second approach (DRSVM) is constructed to generate a more general assessment. More recently, Fukunishi et al. (2014) designed a new method of predicting SA based on commercially available compound databases and molecular descriptors where the SA is estimated from the probability of the existence of substructures of the compound, the number of symmetry atoms, the graph complexity, and the number of the chiral center of the compound.</p>
<p>Given the difficulty of synthesis of most of the leads produced by de novo approaches, some programs added methods to score the SA. <rs id="a12897867" type="software">Lead+Op</rs> <rs id="a12899258" type="bibr">(Lin et al., 2018)</rs> is an example of these programs that takes an initial fragment, looks for associated reaction rules, virtually generate the reaction products and select the best binding conformation. Them it generates conformers and select one that becomes a reactant for another round. Also, programs mentioned above as <rs id="a12897869" type="software">LigBuilder</rs> and <rs id="a12897870" type="software">Autogrow</rs> include SA analysis on their current versions. In the medicinal chemistry component of <rs id="a12897871" type="software">SwissADME</rs>, a SA score is also included.</p>
<p>For the de novo design, the <rs id="a12897872" type="software">LigBuilder</rs> software was used. The
CAVITY procedure was employed to detect and analyze ligand-binding sites of the target. It classified the cavities' druggability, who would be used for docking studies. The BUILD procedure was used in the exploring and growing/linking modes. In the explore mode, fragments from the program's database were added in the protein site and their interaction was scored. Then, the fragments with the best scores were linked. In the growing mode, seeds molecules were put at the binding site and fragments were added to the seeds. At the linking mode, the seed was divided into fragments and other fragments were added to them. After the BUILD procedure, they got a library of 2.5 million compounds. The resulting library was filtered according to ADME properties with the software <rs id="a12897873" type="software">QUIKPROP</rs> where molecules that infringed more than five properties (physicochemical properties, lipophilicity, water solubility, pharmacokinetics models) were discarded. A library of 6,000 compounds results from this process. After this, the Tanimoto's coefficient was applied to measure the similarity among the molecules. Molecules below 0.85 were excluded and 1,500 molecules were considered for the next step. The final step consisted of docking studies, carried out with the <rs id="a12897874" type="software">GLIDE</rs> software and the Induced Fit Docking protocol. Afterward, they selected the three best complexes from the docking and used them as input structures for molecular dynamic studies. Finally, they obtained three compounds with high stability and good binding energies, some of them even better than the reference drugs.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f52708027"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:53+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>In most cases (for instance Fig. 2), the ejected dust plume arises from one area in the field of view of our cameras. We measure directly the 2D coordinates of the source pixel in the image, and project this position in 3 dimensions on the most accurate shape model of 67P, obtained from stereo-photogrammetry for the Northern hemisphere (Preusker et al. 2015), and stereo-photoclinometry for the southern hemisphere (Jorda 2016). The projection from image to shape is done using the spacecraft and comet reconstructed attitudes and trajectories provided by the <rs id="a12970080" type="software">SPICE</rs> library <rs id="a12970081" type="bibr">(Acton 1996)</rs>. We verify this position by producing synthetic images from the orbital parameters and camera descriptions, and matching the the synthetic view with the original image. The largest uncertainty on this type of source inversion comes from the resolution of the images (3-6 m/px for the NAC, 15-32 40m/px for the WAC, and 15-83 m/px for the NavCam). This is the typical error for most of our observations.</p>
<p>Knowing the time and location of each outburst, we can calculate the illumination conditions and local time on the comet, in order to understand whether events are more likely to occur under specific temperature conditions. Spacecraft and comet attitudes and trajectories were retrieved with the <rs id="a12970082" type="software">SPICE</rs> library <rs id="a12970083" type="bibr">(Acton 1996)</rs>. The local illumination is calculated for the best available shape model of comet 67P: a combination of the model by Preusker et al. (2015) for the northern hemisphere, and Jorda (2016) for the southern hemisphere.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f211233527"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:37+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p> <rs id="a12966126" type="software" subtype="component">Hyperledger Composer</rs> is an extensive, open development toolset and framework to make developing Blockchain applications easier. <rs id="a12966127" type="software" subtype="component" corresp="#a12966127b">Hyperledger Composer</rs> supports the existing <rs id="a12966127b" type="software" subtype="environment">Hyperledger Fabric</rs> , that transactions are validated according to policy by the designated business network participants <rs id="res1" type="bibr">(Hyperledger Composer, 2019)</rs>. <rs id="a12966127c" type="software" subtype="component" corresp="#a12966127b">Hyperledger Fabric</rs> offers a number of SDKs to support various programming languages. There are two officially released SDKs for <rs id="a12966130" type="software" subtype="environment">Node.js</rs> and <rs id="a12971425" type="software" subtype="environment">Java</rs> <rs id="res2" type="bibr">(
Hyperledger, 2019)</rs>. keeping the track of the records and in determining the ownership. By connecting the Land Records Department, Registration Department, Banks and other concerned Departments in a</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f360930977"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T12:54+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Denervated muscle fibers exhibit spontaneous, recurring single muscle fiber discharges detectable by single fiber electromyography 43 . Application of a single episode of electrical stimulus on the distal stump, therefore, provides an additional opportunity to investigate the mechanism of direct nerve stimulation via the implantable electrode on the distal nerve stump. The relative gastrocnemius muscle weight at 12 days reveals significant preservation of muscle weight ratio in the stimulation group (65 ± 7.5%) compared to the control (non-stimulation) group (50 ± 8.1%) (Fig. 6a). Gait function analysis demonstrates the preservation of SFI in the stimulation group (-58 ± 7.8), as Fig. 4 Effect of multiple episodes of electrical stimulation on functional motor recovery 6 weeks after nerve injury. a Schematic illustration of the implantation of a wireless electrical stimulator onto the distal stump of a nerve gap model in rats. A bioresorbable nerve conduit (10 mm) is bridged between two ends of the transected sciatic nerve to realize the nerve gap model, and the cuff electrode of the stimulator is implanted on the distal nerve stump. b Relative muscle weight (MW) recovery reveals a significant increase in gastrocnemius muscle mass by multiple episodes of distal nerve stimulation. n = 5 independent animals per group. c Functional gait analysis shows improved function of the injured left hindlimb, with an increase of toe spread in the group with multiple episodes of distal nerve stimulation (circle dotted line). d, e Dynamic gait analysis further verifies the improved sciatic function index (SFI) and static sciatic index (SSI) in the group with multiple episodes of distal nerve stimulation. n = 5 independent animals per group. f Electrophysiologic analysis reveals increased amplitude of compound muscle action potential (CMAP), with a significant increase in the group with multiple episodes of distal nerve stimulation. n = 5 independent animals per group. The boxplots show the median (center line), the third and first quartiles (upper and lower edge of the box, respectively), and the largest and smallest value that is ≤1.5 times the interquartile range (the limits of the upper and lower whiskers, respectively). <rs id="a12900002" type="software">Statistical</rs> software (Version <rs id="a12900003" type="version" corresp="#a12900002">6.0</rs>) was used for the analysis followed by a t-test and one-way ANOVA with Tukey multiple comparison analysis (*P &lt; 0.05; **P &lt; 0.01; ***P &lt; 0.001; ****P &lt; 0.0001). Data available in source data file. compared to the control (non-stimulation) ± 1.3) at 12 days (Fig. 6b). Gastrocnemius muscle fiber histological analysis at the same time point reveals a delayed decrease in muscle fiber surface area (65k ± 3k µm 2 ) in the stimulation group when comparing to the control (non-stimulation) group (52k ± 2k µm 2 ) (Fig. 6c). Taken together, the electrical stimulation delivered from the distal nerve stump to the target muscle contributes to preservation of innervated muscle from denervation, atrophy and functional loss in the early phase of recovery, thus providing more receptive muscle fiber for subsequent reinnervation by motor neuron regeneration at later phase.</p>
<p>Simulations of mechanical characteristics. The finite element analysis (FEA) commercial software <rs id="a12900004" type="software">ABAQUS</rs> (
Analysis User's Manual <rs id="a12900005" type="version" corresp="#a12900004">2016</rs>) was utilized to optimize the interconnect layout and to study the corresponding mechanical characteristics. The objective was to decrease the strain level in Mo wires in the interconnect under different typical loads. The b-DCPU was modeled by hexahedron elements (C3D8R) while the thin Mo layer (15 µm thick) was modeled by shell elements (S4R). The minimal element size was 1/8 of the width of the Mo wires (20 µm), which ensured the convergence, and the accuracy of the simulation results. The elastic modulus (E) and Poisson's ratio (ν) used in the analysis were E Mo = 330 GPa, ν Mo = 0.29, E PU = 0.8 MPa, ν PU = 0.5.</p>
<p>Simulation of the electromagnetic characteristics. The commercial software <rs id="a12900006" type="software">ANSYS HFSS</rs> was used to perform electromagnetic (EM) finite element analysis and determine the inductance, Q factor, and scattering parameters (Supplementary Fig. S2) of the bioresorbable, implantable wireless stimulator (2 layers of 17 turns each, 12 mm diameter). Lumped ports were used to obtain the scattering parameters S nm (Supplementary Fig. S2a,b) and port impedance Z nm (Fig. S2c) the coils. An adaptive mesh (tetrahedron elements) and a spherical radiation boundary (radius of 1000 mm) were adopted to ensure computational accuracy. The frequency-dependent impedance of the resonant system is given by</p>
<p>MicroCT imaging. Rats were imaged post mortem with a preclinical microCT imaging system (nanoScan PET/CT, Mediso-USA, Boston, MA). Data acquisition used 2.2x magnification, &lt;60 µm focal spot, 1 × 4 binning, with 720 projection views over a full circle, using 70 kVp/520 µA, with a 300 ms exposure time. The projection data was reconstructed with a voxel size of 68 µm and using filtered (Butterworth filter) <rs id="a12900007" type="software" subtype="implicit">backprojection software</rs> from <rs id="a12900008" type="publisher" corresp="#a12900007">Mediso</rs>. The reconstructed data was visualized and segmented in <rs id="a12900009" type="software">Amira</rs> <rs id="a12900010" type="version" corresp="#a12900009">2019
.2</rs> ( <rs id="a12900011" type="publisher" corresp="#a12900009">FEI</rs>, Houston, TX). A non-localmeans filter was used to reduce image artifacts.</p>
<p>Electrophysiology measurements. The rat sciatic nerve was re-exposed and dissected freely from surrounding soft tissue. Electrical stimuli (single-pulse shocks, 1 mA, 0.1 ms) were applied to the intact sciatic nerve trunk at the point 5 mm proximal to the conduit suturing point 51 . The amplitudes of compound muscle action potentials (CMAPs) were recorded on the innervated gastrocnemius muscle from 1 V to 12 V or until a supramaximal CMAP was reached. Normal CMAPs from the non-injured right side of sciatic nerve were also recorded for normalization. Grass Tech S88X Stimulator (Astro-Med Inc.) was used for the test and <rs id="a12900012" type="software">PolyVIWE
16</rs> data acquisition software (Version
Inc.) was used for recording. Recovery rate was the ratio of amplitude of injured hindlimb's CMAP to contralateral normal hindlimb's CMAP. Gait function analysis. These studies used a walking track equipped with a videobased system, modified from that used in a previous study 52 . The apparatus consisted of a Plexiglas chamber that was 80 cm long, 6 cm wide, and 12 cm high with transparent glass underneath the walking track. The footprint was recorded using an EX-F1 digital camera (Casio, Tokyo, Japan) from below the walking track. The task was repeated until 5 or 6 satisfactory walks of at least 4 steps without pause were obtained. In this study, only the hindlimb stepping patterns were analyzed. The digital images obtained from each trial were processed with a threshold setting to detect the boundary of the soles, and critical points for determining the derivation of paw indices were determined using <rs id="a12900013" type="software">Matlab</rs> software ( <rs id="a12900014" type="publisher" corresp="#a12900013">MathWorks</rs>, Natick, MA, USA). After identifying sequential footprints, sciatic function index 53 (SFI) and static sciatic index 54 (SSI) were calculated. Each parameter was averaged from at least 20 footsteps.</p>
<p>Muscles sections were collected on microscope slides and fixed by 4% PFA for 10 min and then washed by 1x PBS three times and penetrated by 0.5% triton X-100 in PBS for 30 min. Samples were incubated at room temperature for 2 h with 5% NDS for blocking. Afterward, samples were incubated with primary antibody against laminin (1:300, Sigma-Aldrich, L9393) or neurofilament (1:100, Sigma-Aldrich, N4142) in 4 °C overnight. Laminin and neurofilament antibody was diluted in 1% NDS and PBS, respectively. The following day, samples were washed three times for 15 min in PBS and incubated with corresponding AlexaFluor 546 secondary antibody (1:300, Donkey anti-rabbit, Life Technologies Corporation, A10040) and alpha-BTX (1:100, Thermo Fisher Scientific Company, B13422) in PBS for 1 h in room temperature. Finally, samples were cultured with DAPI for 5 min. Between these steps, slides were washed with PBS three times for 15 min to remove unbonded molecules or antibodies. Samples were mounted by fluormount media and sealed by microscope cover glass. The slides were then put in 4 °C refrigerator overnight to allow the cover glass to set before imaging. A Zeiss microscope was used to image laminin and NMJ staining. Both laminin and neurofilament were labeled with red and NMJ was labeled in green (alpha-BTX). Zstack was collected for the NMJ-neurofilament image to show the connections between them. <rs id="a12900015" type="software">ImageJ</rs> was used to measure the area of muscle fiber and to project the Z-stacks to a single image (image &gt; stacks &gt; Z-project &gt; Max intensity). The image obtained from this process was used to count NMJ in sections.</p>
<p>Statistical analysis. Results are reported as mean ± 6 SD, unless otherwise noted. Statistical analyses were performed using <rs id="a12900016" type="software">Statistical</rs> software (Version <rs id="a12900017" type="version" corresp="#a12900016">6.0</rs>, <rs id="a12900018" type="publisher" corresp="#a12900016">Statsoft</rs>, Tulsa, Oklahoma) followed by a t-test and one-way ANOVA (with Tukey multiple comparison analysis for the difference between groups). *P &lt; 0.05, *P &lt; 0.01, ***P &lt; 0.001.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f50714734"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:56+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Previous documented solutions with somewhat different approaches have shown the potential of automation of the planning process. (21,22,23,24,25) The current study validates the performance of a prototype version of the <rs id="a12971999" type="software" subtype="component" corresp="#a12972000">Auto-Planning</rs> module which recently has been productized for clinical use in the <rs id="a12972000" type="software" subtype="environment">Pinnacle 3</rs> treatment planning system from <rs id="a12972001" type="publisher" corresp="#a12972000">Philips Healthcare</rs> (Fitchburg, WI).</p>
<p>The <rs id="a12972002" type="software">Auto-Planning</rs> software was evaluated by replanning 26 previously delivered clinical head and neck IMRT treatment plans of the oropharynx. The plans were delivered over the 12 months prior to the study. The plans were created in accordance with the Danish Head And Neck Cancer Groups guidelines (DAHANCA -Version 2004), and each dose plan included three dose levels of 50 Gy, 60 Gy, and 66 or 68 Gy in 33 treatment fractions with a simultaneous integrated boost technique.</p>
<p>In the <rs id="a12972003" type="software">Auto-Planning</rs> software, a template of configurable parameters known as a Technique (details in Appendix A) can be defined for each treatment protocol. The Techniques include definition of beam parameters and planning goals. The <rs id="a12972004" type="software">Auto-Planning</rs> module uses the Technique definition to iteratively adjust IMRT planning parameters to best meet the planning goals. The Technique was defined according to local standards, including prioritization between target coverage and dose to organs at risk. The Technique definition was based on five additional pilot patients independent of the 26 study patients.</p>
<p>Each of the 26 treatment plans were replanned with <rs id="a12972005" type="software">Auto-Planning</rs> without knowledge of the clinically delivered treatment plans and without any dosimetrist postoptimization of the treatment plans. The only input to the replanning was the delineations of planning target and organs at risk and the positioning of the isocenter.</p>
<p>Quantitative dosimetric evaluation of the performance of the treatment plans was performed on dose volume histograms (DVH) extracted from the planning system. CT scans had a slice thickness of 3 mm and in plane voxel size of 1 mm × 1 mm. The dose plans were calculated using the <rs id="a12972006" type="software">Pinnacle 3</rs> collapsed cone algorithm with a dose grid resolution of 3 mm. Specific DVH values, as well as the overall shape of the DVH, was compared using average DVHs of the two types of treatment plans.</p>
<p>For all 26 patients it was possible to produce automatic plans of high plan quality. Small differences in the PTV dose coverage but a significant reduction in dose to OAR between automatic plans and clinical plans indicate that the Technique parameters used in the current study are biased towards normal tissue sparing relative to the clinical practice. It is likely that another set of Technique parameters could be determined which would focus more on dose coverage. In addition, in the clinical release of the <rs id="a12972007" type="software">Auto-Planning</rs> software, modifications have been made to enhance the priority of maximum dose constraints to OAR such as the cord, a need highlighted by the physicians during plan evaluation. No postoptimization of the plans was performed in the study to make a pure validation of the <rs id="a12972008" type="software">Auto-Planning</rs> software. If needed or requested, in a clinical situation automatic optimized plans could be further optimized just as any manually created plan since the automatic plans are delivered in exactly the same format as manually created plans. Thus, the automatic plans can either be a high quality starting point for further manual optimization or an attempt to produce plans of clinical quality without further user intervention.</p>
<p>A prerequisite to achieve high-quality automatic plans compared to manually created plans is obviously a sound optimization and tuning algorithm (the <rs id="a12972009" type="software">Auto-Planning</rs> engine). However, the quality of the manual plans is obviously also of importance in comparison with the automatic plans (poor quality manual plan would favor automatic plans). At the time the manually created plans were made, all clinical head and neck treatment plans in the department were created using IMRT; thus the department was experienced in creating "high quality" plans manually. As a result, given the significant experience with IMRT in the department, the interdosimetrist variation should be limited. Nevertheless, as seen in Fig. 3, there is quite a variation in manually optimized plans, a variation which is less for the automatic optimization. It therefore seems likely that one of the advantages of automatic plans is a reduction of the interdosimetrist variation which is present even within departments that use IMRT extensively. The reason of the interdosimetrist variation is related both to limited time to create the plan and to lack of knowledge of how much the plan in reality can be optimized. For a manually created plan, it is difficult to know the extent an OAR can be spared prior to actual plan optimization. Therefore, objectives for organs at risk are typically set relatively loose initially, in order to ensure dose coverage of the target. Having obtained dose coverage of target, the next step is to reduce the dose to organs at risk as much as possible. In a busy clinic, it can be hard to ensure that all constraints on organs at risk have been tightened as much as possible. This issue could be reduced significantly if an initial "high quality" plan -e.g., an automatic generated plan -were available such that the dosimetrist could focus on fine tuning of the treatment plan.</p>
<p>Another potential benefit of <rs id="a12972010" type="software">Auto-Planning</rs> could be a simple method to exchange planning knowledge and procedures between institutions since the Technique configuration of the <rs id="a12972011" type="software">Auto-Planning</rs> software can easily be shared between institutions. This could help institutions with, for example, limited resources to quickly create IMRT or VMAT plans with similar quality as in more advanced institutions.</p>
<p>Most previous work on automating the planning process has built on knowledge of previously treated patients. One approach of extracting information from previously treated patients is utilizing the overlap volume histogram method, which measures the position of an OAR relative to the target. (19,29,30,31) Knowledge of overlap volume histograms from previously treated patients can be used to predict the likely achievable irradiation level of specific OARs. A few published solutions on automating the planning process have been documented, (21,22,23,24,25) and all build on the <rs id="a12972012" type="software">Pinnacle 3</rs> planning system. The published systems did show the feasibility of automating the planning process. However, in terms of flexibility, it could be a potential issue that "knowledge based" approaches require a database of "high" quality plans for each protocol. Changes to planning techniques, prescriptions, OAR sparing goals, and contouring Journal of Applied Clinical Medical Physics, Vol. 17, No. 1, 2016 style could, if not implemented in a very smart way, require a new "high" quality database. Such a change could be quite labor-intensive to implement clinically, and might not be as flexible to interchange between institutions. This issue might be addressed within "knowledge based" algorithms, but is not present in the <rs id="a12972013" type="software">Auto-Planning</rs> solution evaluated in this study since it only relies on a small set of Technique parameters.</p>
<p>Finally, it should be mentioned that plan comparison studies are inherently difficult to perform since development of the treatment planning skill is continually ongoing within any department in order to optimize the treatments plans. Thus, if the current study was repeated, the results might be different since our dose planning team has learned new ways to improve the quality based on the results of this study. Similar statements could be made about the configuration of the automatic system. However, this does not change the fact that the current comparison between treatment plans that have been delivered clinically and the automatic treatment plans did show the autoplans to be superior at that time. Thus, the impact of <rs id="a12972014" type="software">Auto-Planning</rs> seems likely to be a tool to increase the overall quality of dose planning, rather than a tool that could remove the need of manual optimization.</p>
<p>Comparison of autoplans and previous delivered clinical plans showed only small dosimetric differences in target coverage, but significant reduction in dose to OAR for the autoplans. The blinded clinical evaluation of the plans showed that, for 94% of the evaluations, the autoplans were similar to or better than the clinical plans. <rs id="a12972015" type="software">Auto-Planning</rs> software will, therefore, be able to reduce the manual time spend per treatment plan since the most of the plans could potentially be used clinically without further optimization. Perhaps more importantly, <rs id="a12972016" type="software">Auto-Planning</rs> could be used as a high quality starting point for further plan optimization. This could increase the overall quality of the treatments and reduce the interobserver variation present in manually created treatment plans.</p>
<p>Appendix A. Description of <rs id="a12972017" type="software">Auto-Planning</rs>. This <rs id="a12972018" type="software">Auto-Planning</rs> module simplifies the planning process through the use of templates called Techniques, and automatic optimization tuning methods called the Auto-Planning engine (APE).</p>
<p>-Derived regions of interest (ROIs) (e.g., PTV or expanded cord) -Placement of points of interest (POIs) -Prescriptions -Beam geometries, settings, and optimization options -Prioritized optimization goals A single selection creates a new plan based on the Technique settings and runs the APE. The APE tuning method maps the prioritized optimization goals defined in the Technique to optimization objectives. Multiple optimization loops are performed that iteratively adjust the optimization parameters to meet the goals and further drive down organ at risk (OAR) sparing with minimal compromise to the target coverage. This is achieved by using objectives specific to driving down OAR dose to the point it significantly affects target coverage and separate objectives to achieve the desired goals. Target conformality is automatically controlled by a system-generated ring structure, and objectives and body dose is controlled by a systemgenerated normal tissue structure and objectives. The objective dose and weight parameters are tuned using a proprietary method. Target uniformity is controlled by reducing hot and cold spots using system-generated control structures and objectives similar to the process defined in the study by Xhaferllari et al. (32) The input to a Technique is clinical goals (e.g., maximum dose, mean dose, and DVH constrains), as well as the pertinent parameters listed above. Optimization of a Technique is an iterative process in which <rs id="a12972019" type="software">Auto-Planning</rs> is performed on test patients. If there are short-comings in the autoplan results, the optimization goals in the Technique are adjusted to account for them and saved. In the current work, the optimization of the Technique was performed on separate patients than those included in the study, and finalized before autoplanning was performed on any of the patients included in the current study.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f215531383"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T09:29+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Assemblies from 178 bulk soil metagenomes (collected from 2010-2012) 14 , 12 size-fractionated metagenomes (collected in 2014 for the enrichment of small microorganisms) and 7 viromes (collected in 2014), all from Stordalen soils, were screened using <rs id="a12884289" type="software">VirSorter</rs> <rs id="a12884290" type="bibr">18</rs> and manually curated (Supplementary Fig. 2 and Supplementary Table 1) to recover 7,547 viral contigs. These Stordalen contigs were combined with 15,220 viral genomes and large genome fragments from publicly available data sets (RefSeq prokaryotic viral genomes and viral genomes mined from microbial genomes 18,20 ) for a total of 22,767 viral contigs, which were clustered at 95% nucleotide identity to define 17,434 viral 'populations' that approximately represent species-level taxonomy 21 . To maximize the recovery of viral populations from 214 Stordalen bulk soil metagenomes 14 , we used this initial database of viral population sequences for detection through read mapping (Supplementary Fig. 2). A final set of 1,907 viral populations was detected in 201 of the 214 bulk soil metagenomes and used in downstream analyses. These populations are represented by contigs ≥ 10 kb in size and/or circular (assumed to be complete), and all of the detected populations were assembled from Stordalen metagenomes; no viruses from the considered public databases were detected. Because these viral populations were detected in bulk soil DNA, they presumably represent free viruses,</p>
<p><rs id="a12884296" type="software">VirSorter</rs> was used to recover viral contigs, based on the identification of viral hallmark genes, enrichment in hypothetical proteins and other viral signatures, as previously described 32 .</p>
<p><rs id="a12884297" type="software">VirSorter</rs> was run separately on each of the five assemblies: the palsa, bog and fen bulk soil co-assemblies, the virome assembly and the small-size-fraction-enriched assembly (Supplementary Table 1). Only contigs from <rs id="a12884298" type="software">VirSorter</rs> categories 1 and 2 (and 4 and 5, the provirus equivalents of categories 1 and 2) were retained, based on a previous benchmarking study that found that, prior to manual curation, 100% of viruses in category 1 and 92% of viruses in category 2 were confirmed to be unambiguously viral 32 . Specifically, category 1 (and 4) viruses contain sequences similar to known viruses, and category 2 (and 5) viruses contain viral hallmark genes and/or are enriched for viral or non-Caudovirales genes and have at least one other virus-like metric (depletion of PFAM hits, hypothetical gene enrichment and/or depletions in coding-strand switching). For contigs with predicted proviruses, only predicted proviral regions were retained. After read mapping (see below), the data set of detected viral populations was manually curated to a final set of 1,907 viral contigs (viral populations) by ensuring that the <rs id="a12884299" type="software">VirSorter</rs> protein families (PFAM) annotation for each contig (Supplementary Table 13) contained genes that would be consistent with a viral genome, as described previously 18 . These methods probably resulted in the recovery of free viruses, proviruses and/or intracellular and potentially actively infecting viruses. As with any viral metagenomic study, some non-viral elements (regions of contigs or possibly full contigs) may have remained in the data set, even after manual curation.</p>
<p>Viral population database compilation. To maximize our ability to recover viral populations from the 214 bulk soil metagenomes through read mapping, a fasta file combining viral contigs and genomes from different data sets was compiled (Supplementary Fig. 2). This initial viral database included the viral contigs recovered by <rs id="a12884300" type="software">VirSorter</rs> in this study from the bulk soil metagenomic assemblies (7,093 contigs; 3 phiX contigs were removed from the 7,096 initially recovered), the small-size-fraction-enriched assembly (401 contigs) and the virome assembly (53 contigs), along with the following publicly available data: 1,575 bacterial and archaeal viral genomes from the NCBI RefSeq database (v70, 26 May 2015) 20 , bacterial and archaeal viral genomes and genome fragments in Genbank but not in RefSeq as of July 2015 (1,147 contigs) 25 , and 12,498 viral contigs recovered and curated from publicly available bacterial and archaeal genomes 18 (the Paez-Espino et al. data set 19 was not published at the time of this mapping analysis but was incorporated into downstream analyses, see below). This viral database was dereplicated by clustering at 95% nucleic acid identity with Cd-Hit v4.6 33,34 . For clustering, all viral sequences recovered from Stordalen Mire were required to be ≥ 3 kb in length (smaller than the final data set of ≥ 10 kb, see below) and there was no length threshold for the publicly available sequences.</p>
<p>Reads from the 214 bulk soil metagenomes were quality trimmed using <rs id="a12884306" type="software">Trimmomatic</rs> v <rs id="a12884307" type="version" corresp="#a12884306">0.36</rs> <rs id="a12884308" type="bibr">35</rs> and then paired reads were mapped to the viral contig database with <rs id="a12884309" type="software">Bowtie2</rs> <rs id="a12884310" type="bibr">36</rs> , using default parameters. The output bam files were passed to <rs id="a12884311" type="software" subtype="environment">BamM</rs> '<rs id="a12884312" type="software" subtype="component" corresp="#a12884311">filter</rs>' v <rs id="a12884313" type="version" corresp="#a12884311">1.7
.2</rs> ( <rs id="a12884314" type="url" corresp="#a12884311">http://ecogenomics.github.io/ BamM/</rs>, accessed 15 December 2015) and reads that were aligned over ≥ 90% of their length at ≥ 95% nucleic acid identity were retained. A <rs id="a12884315" type="language" corresp="#a12884316">python</rs> <rs id="a12884316" type="software" subtype="implicit">script</rs> was used to further filter the bam files to ensure that all detected contigs had reads covering ≥ 70% of their length to minimize the potential for erroneous detection driven by small, potentially non-viral regions, and the coverage for a particular contig in a particular sample was converted to zero if this condition was not met. Finally, <rs id="a12884317" type="software" subtype="environment">BamM</rs> '<rs id="a12884318" type="software" subtype="component" corresp="#a12884317">parse</rs>' v<rs id="a12884319" type="version" corresp="#a12884317">1.7
.2</rs> was used to generate a coverage profile of viral contig abundances across samples, using the 'tpmean' coverage mode to account for anomalously low-coverage and high-coverage regions of each contig. The average per base-pair coverage for each contig in each sample was retained.</p>
<p>The final 'operational taxonomic unit (OTU) table' (Supplementary Table 7) of viral abundances was pulled from the <rs id="a12884320" type="software">BamM</rs> mapping coverage output, normalized by the number of metagenomic reads in each sample. The normalization calculation was performed as follows: the average number of reads across all samples was determined, and, for a given sample, the total number of reads was divided by the all-samples average, and the reciprocal of that number was used as a multiplier to bring the total number of reads for each sample up to or down to the average (for example, all coverage values from a sample with twice as many reads as the average would have been multiplied by 0.5). For virus/host abundance ratios, this table was used directly, to best approximate actual abundances. For other statistical analyses, the normalized viral OTU table was subsequently square-root transformed.</p>
<p>To place the 1,907 Stordalen Mire viral populations in the context of known viruses, predicted proteins were clustered with predicted proteins from viral sequences in public databases. Specifically, the 1,907 viral populations were compared to: 2,010 bacterial and archaeal viral genomes from the NCBI RefSeq database (v75, June 2016) 20 , 2,040 viral contigs &gt; 10 kb from microbial genomes isolated from soil (pulled by habitat for this study from a larger data set 18 ) and 3,112 viral contigs &gt; 10 kb from soil and soil-associated metagenomes (broadly defined soil-associated metagenomes were pulled by habitat for this study from a larger data set 19 , based on environmental labels in the original article as follows: 'Terrestrial (soil)' , 'Terrestrial (other)' , and 'Host associated (plants)'). Proteins were then subjected to an all-versus-all <rs id="a12884324" type="software">BLASTp</rs> search with an E-value threshold of 10 -4 and grouped into protein clusters, as previously described 25 . Based on the number of shared protein clusters between genomes and/or genome fragments (contigs), a similarity score for each pair was calculated as the negative logarithmic score by multiplying the hypergeometric similarity P value by the total number of pairwise comparisons, using <rs id="a12884325" type="software">vContact</rs> ( <rs id="a12884326" type="url" corresp="#a12884325">https://bitbucket.org/MAVERICLab/vcontact</rs>, accessed 13 November 2016). The stringency of the similarity score was evaluated through 1,000 randomizations by permuting protein clusters or singletons (proteins without significant shared similarity to other protein sequences) within pairs of genomes and/or contigs having a significance score of ≤ 1 (a negative control) 37 . None of the genome and/or contig pairs in this negative control produced significant scores of &gt; 1, indicating an appropriately defined similarity score threshold 38 . Subsequently, pairs of genomes and/or contigs with a similarity score of &gt; 1 were clustered into viral clusters with the Markov clustering algorithm with an inflation of 2, as previously described 25 . The resulting network was visualized with <rs id="a12884327" type="software">Cytoscape</rs> software (version <rs id="a12884328" type="version" corresp="#a12884327">3.4.0</rs>, <rs id="a12884329" type="url" corresp="#a12884327">http://cytoscape.org/</rs>), using an edge-weighted spring embedded model, which places the genomes and/or contigs that share more</p>
<p>Viral population abundances in metatranscriptomes. Metatranscriptomic data generation was described previously 14 . Briefly, most metatranscriptomic libraries were run on either a HiSeq (Illumina) or MiSeq (Illumina) to assess library quality before deeper NextSeq (Illumina) sequencing. Before metatranscriptomic read mapping to viral population sequences, part of the <rs id="a12884332" type="software">TranscriptM</rs> pipeline ( <rs id="a12884333" type="url" corresp="#a12884332">https:// github.com/elfrouin/transcriptM</rs>) was used to process metatranscriptomic reads, as follows: sequencing read files from the same metatranscriptomic libraries were concatenated across sequencing runs. Raw reads were trimmed via <rs id="a12884334" type="software">Trimmomatic</rs> <rs id="a12884335" type="bibr">35</rs> , using quality scores determined via <rs id="a12884336" type="software">FastQC</rs> ( <rs id="a12884337" type="url" corresp="#a12884336">http://www.bioinformatics.babraham. ac.uk/projects/fastqc/</rs>). PhiX contamination was removed by discarding reads that aligned to the PhiX genome via <rs id="a12884338" type="software">BamM</rs> mapping. <rs id="a12884339" type="software">SortMeRNA</rs> <rs id="a12884340" type="bibr">39</rs> was used to remove non-coding RNA sequences, including transfer RNA, transfer-messenger RNA, 5S, 16S, 18S, 23S and 28S rRNA sequences. The remaining total mRNA sequences were used for mapping to the 1,907 viral population contig sequences to infer the composition of the active viral community in 21 bulk soil metatranscriptomes collected from a subset of the same samples from which the 201 metagenomic samples were generated (Supplementary Table 1 and Supplementary Fig. 2). Mapping parameters were the same as for metagenomic read mapping, that is, ≥ 95% nucleotide identity, ≥ 90% of each read mapped and the 'tpmean' output from <rs id="a12884341" type="software">BamM</rs> v<rs id="a12884342" type="version" corresp="#a12884341">1.7.2</rs> ( <rs id="a12884343" type="url" corresp="#a12884341">http://ecogenomics.github.io/BamM/</rs>). <rs id="a12884344" type="software">Dirseq</rs> v <rs id="a12884345" type="version" corresp="#a12884344">0.0.2</rs> ( <rs id="a12884346" type="url" corresp="#a12884344">https://github.com/wwood/dirseq</rs>) with parameter -ignore-directions was used to determine the average coverage of each gene, using both the filtered bam files and <rs id="a12884347" type="software">PROKKA</rs> annotation.gff files as input. A positive average coverage value (indicating read mapping to a gene) for at least one gene for every 10 kb of viral genomic sequence was required for a viral population to be considered detected in a metatranscriptome (for example, for a 20-kb viral sequence, metatranscriptomic reads would need to map to at least two genes for detection). Supplementary Table 2 includes the final coverage table used in the metatranscriptomic data analyses, derived from average coverage values for all genes in each viral population, provided that the population met detection limits (if limits were not met, the coverage value for a particular population in a particular metatranscriptome was converted to zero). Although these methods are meant to provide an estimate of active viral community composition, there is no standardized method or biological precedent for assessing viral activity via detection in a bulk soil metatranscriptome, so the extent to which these methods yield an accurate estimate of viral activity is unknown.</p>
<p>Bacterial and archaeal population genomes (n = 1,529) were recovered from the same 214 bulk soil metagenomes through metagenomic assembly and differential coverage binning, as previously described 14 . For comparisons to viral abundances, bacterial and archaeal ('host') abundance profiles were generated from the 201 bulk soil metagenomes in which viral populations were detected, using a mapping approach similar to that used for viral abundance estimates. The 1,529 bacterial and archaeal population genomes were first dereplicated at a 95% average amino acid identity (AAI) to minimize multi-mapping (that is, reads mapping to more than one genome). The AAI was calculated between genomes using the <rs id="a12884348" type="software" subtype="environment">CompareM</rs> (v. <rs id="a12884349" type="version" corresp="#a12884348">0.0.17</rs>) AAI workflow ('<rs id="a12884350" type="software" subtype="component" corresp="#a12884348">comparem aai_wf</rs> ' , <rs id="a12884351" type="url" corresp="#a12884348">https://github.com/dparks1134/ CompareM</rs>). Genomes with &gt; 95% AAI were grouped together, and the highest quality genome in each group was chosen as the representative, where quality was estimated by <rs id="a12884352" type="software">CheckM</rs> <rs id="a12884353" type="bibr">40</rs> as 'completeness -4 × contamination' . To calculate the relative abundance of each population, reads from each of the 214 bulk soil metagenomes were mapped to the set of 630 dereplicated genomes using <rs id="a12884354" type="software" subtype="environment">BamM</rs> '<rs id="a12884355" type="software" subtype="component" corresp="#a12884354">make</rs>' ( <rs id="a12884356" type="url" corresp="#a12884354">http://ecogenomics.github.io/BamM/</rs>). Low-quality read mappings were removed with <rs id="a12884357" type="software" subtype="environment">BamM</rs> v <rs id="a12884358" type="version" corresp="#a12884357">1.7.3</rs> '<rs id="a12884359" type="software" subtype="component" corresp="#a12884357">filter</rs>' (minimum nucleotide identity of 95%, minimum aligned length of 75% of each read), and the coverage of each contig was calculated with <rs id="a12884360" type="software" subtype="environment">BamM</rs> '<rs id="a12884361" type="software" subtype="component" corresp="#a12884360">parse</rs>' using the 'tpmean' mode to calculate the coverage as the mean of the number of reads aligned to each position, after removing the highest 10% and the lowest 10% coverage regions. The coverage of each population genome was calculated as the average of all of its binned contig coverages, weighting each contig by its length in base pairs. The final host OTU table (Supplementary Table 8) of bacterial and archaeal abundances was pulled from the <rs id="a12884362" type="software">BamM</rs> mapping coverage output, normalized by the number of metagenomic reads in each sample, calculated as described above for the viral OTU table. Only average coverage values of ≥ 0.25× were retained; lower values were converted to zero. For virus-host abundance analyses, this table was used directly, and for Mantel correlations, this normalized OTU table was square-root transformed.</p>
<p>Virus-host linkage analyses. Where possible, the 1,907 Stordalen Mire viral populations were putatively linked to host population genomes 14 in silico, using similar methodology to previous work 25,29 , with a few enhancements. Broadly, these linkages were based on: (1) sequence similarity between spacers in microbial clustered regularly interspaced short palindromic repeat (CRISPR) regions and in the viral genomes (that is, viral protospacers), as previously described 19,25,29,41 , with the addition of a priority for linkages in which a protospacer adjacent motif 42 was recovered in the viral contig, (2) similarities in tetranucleotide frequency patterns 25 , and (3) shared genomic content, similar to previous work 19,25 . To recover CRISPR spacer and repeat elements from metagenomic reads, <rs id="a12884363" type="software">crass</rs> v<rs id="a12884364" type="version" corresp="#a12884363">0.3.6</rs> was used with default parameters, running on each of the 214 bulk soil metagenomes separately 29,43 . <rs id="a12884365" type="software">BLASTn</rs> was used to compare spacer sequences to the viral contigs, with matches retained if they contained ≤ 1 mismatch and had an E-value of ≤ 10 -5 . For any spacer with a match in a viral genome, the repeat sequence from the same assembled CRISPR region was compared to all bacterial and archaeal population genomes via <rs id="a12884366" type="software">BLASTn</rs> (E-value threshold of 10 -10 and 100% nucleotide identity) to link that CRISPR region (and, therefore, any viruses matching spacers in that CRISPR region) to a host. Tetranucleotide frequency patterns 25,44 were assessed independently for viral populations and hosts, and for each viral population, the host genome with the most similar tetranucleotide frequency pattern was identified as a putative host, with a threshold of 10 -3 on the distance between the host and the viral sequence 25 . Finally, <rs id="a12884367" type="software">BLASTn</rs> was used to link viral populations to hosts, based on shared genomic regions, which could indicate either shared genes between viruses and hosts (for example, auxiliary metabolic genes and/or tRNAs 19,25 ) and/or could be indicative of proviruses. A bit score threshold of 50, an E-value threshold of 10 -3 and a ≥ 70% average nucleotide identity were required, and only hits ≥ 2,500 bp were considered, as these have been previously shown to yield the most confident host predictions 45 . Among these hits, a <rs id="a12884368" type="software">BLASTn</rs> hit that covered ≥ 90% of a contig in a microbial genome bin was interpreted as a viral contig that was 'co-binned' with a microbial genome. These were considered less-confident host predictions than proviruses (defined as matching ≤ 90% of a contig in a microbial genome bin). All predictions from all metrics are available in Supplementary Table 5.</p>
<p>(1) CRISPR linkage with a protospacer adjacent motif, (2) CRISPR linkage without a protospacer adjacent motif, (3) <rs id="a12884369" type="software">BLASTn</rs> linkage to ≤ 90% of a contig in a microbial genome bin (putative provirus), (4) <rs id="a12884370" type="software">BLASTn</rs> linkage to ≥ 90% of a contig in a microbial genome bin (a putative virus co-binned with the host), and (5) the best-matched tetranucleotide frequency patterns. If multiple hosts were predicted in the best of those five categories, then the last universal common ancestor was chosen as the host, based on the lowest (most highly resolved) shared taxonomic level in the predicted host taxonomies. Host taxonomy information for all analyses was based on available reference database sequences as of August 2016 and has subsequently been updated 14 (Supplementary Table 8).</p>
<p>Of the 1,907 viral populations, 1,743 <rs id="a12884371" type="software">VirSorter</rs>-predicted non-proviral contigs (that is, <rs id="a12884372" type="software">VirSorter</rs> categories 1 and 2) were considered for these analyses, to minimize potential host genome contamination (for example, from imperfectly called proviral ends). All genes underwent a re-annotation, as described previously 46,47 . Briefly, open reading frames were predicted using <rs id="a12884373" type="software">MetaProdigal</rs> <rs id="a12884374" type="bibr">48</rs> , and predicted protein sequences were compared to the <rs id="a12884375" type="software">InterProScan</rs> <rs id="a12884376" type="bibr">49</rs> database via <rs id="a12884377" type="software">USEARCH</rs> <rs id="a12884378" type="bibr">50</rs></p>
<p>reverse best-hit matches of &gt; 60 bits retained. As previously described 46 , glycoside hydrolase genes were manually pulled using <rs id="a12884379" type="software">PFAM</rs> identification numbers from <rs id="a12884380" type="software">InterProScan</rs> associated with carbohydrate-active proteins. In total, 360 genes were annotated as glycoside hydrolases, 293 of which were identified as chitinase, lysozyme or putative cell wall-binding genes that have previously been shown to occur in viral genomes with predicted viral functions 51,52 . The remaining 65 glycoside hydrolases were predicted to degrade cellulose, hemicellulose, starches or pectin, and these were manually curated to a conservative set of 24 glycoside hydrolases that had unambiguous virus-like genomic contexts (Supplementary Table 14). For 'high-confidence' viral genomic contexts, common viral genes, for example, viral structural genes, terminases or integrases, were required to be found in genomic regions both upstream and downstream of the glycoside hydrolase, and for both high-confidence and medium-confidence viral genomic contexts, no other microbial metabolic genes could be found on any part of the contig, as those could indicate a possible microbial origin for the glycoside hydrolase, for example, resulting from mispackaging of the viral genome or incorrectly called proviral ends by <rs id="a12884381" type="software">VirSorter</rs>.</p>
<p>Protein sequences from the 24 glycoside hydrolases were structurally modelled using <rs id="a12884382" type="software">PHYRE2</rs> in expert batch submission mode (<rs id="a12884383" type="url" corresp="#a12884382">http://www.sbg.bio.ic.ac. uk/phyre2/html/page.cgi?id= index</rs>) to confirm and further resolve functional predictions. Of these, 14 had 100% confidence scores to bacterial or archaeal glycoside hydrolases and were further investigated for catalytic residues in active sites (the other 10 had potentially virus-like functional predictions or no match in the PHYRE2 database). Catalytic residues were compared with reference sequences, using the Catalytic Site Atlas 53 when available, otherwise crystal structures were manually identified using references from the Protein Data Bank (Supplementary Table 14).</p>
<p>We used a PLS regression analysis, implemented in the <rs id="a12884388" type="software" subtype="environment">R</rs> programming language via the package <rs id="a12884389" type="software" subtype="component" corresp="#a12884388">PLS</rs> and <rs id="a12884390" type="software" subtype="component" corresp="#a12884389">PLSR</rs> function <rs id="a12884391" type="bibr">9</rs>,<rs id="a12884392" type="bibr">59</rs>,<rs id="a12884393" type="bibr">60</rs> to predict measured geochemical variables (response variables) from biotic and abiotic variables (explanatory variables). <rs id="a128843888" type="software">PLS</rs> models a causal relationship between an explanatory variable (or variables; in this case, the abundances of subgroups of viruses and/or hosts and/or abiotic factors) and the response variable being predicted. We note that, in this analysis, Stordalen Mire is treated as a single system with varying biogeochemical, microbial and viral indices that can be related by statistical analysis as a means for characterizing their interrelationships within this site only. This <rs id="a128843889" type="software">PLS</rs> analysis assumes that different samples are statistically independent, and we believe that this assumption is met here because the distance</p>
<p>Code availability. The mapping pipeline is freely available on <rs id="a12884394" type="software">CyVerse</rs> <rs id="a12884395" type="bibr">66</rs> as '<rs id="a12884396" type="software">Read
2RefMapper</rs>- <rs id="a12884397" type="version" corresp="#a12884396">1.1.0</rs>' .</p>
<p>Computational workflow. Initial quality control of the 12 sequence data files was performed using <rs id="a12884398" type="software">SeqPrep</rs> ( <rs id="a12884399" type="url" corresp="#a12884398">https://github.com/jstjohn/SeqPrep</rs>) for adaptor trimming and merging of the paired-end reads, and <rs id="a12884400" type="software">Nesoni</rs> (<rs id="a12884401" type="url" corresp="#a12884400">https://github. com/Victorian-Bioinformatics-Consortium/nesoni</rs>) was used for trimming poor-quality bases. De novo assemblies of the quality-processed sequencing data were constructed using <rs id="a12884402" type="software">CLC Genomics Workbench</rs> ( <rs id="a12884403" type="publisher" corresp="#a12884402">CLC bio</rs>). A combination of all 12 metagenomes yielded the best assembly (the 'ultrasmall' assembly in Supplementary Table 1), and the contigs from this assembly were submitted to <rs id="a12884404" type="software">VirSorter</rs> to mine for viruses, as described above.</p>
<p>Bioinformatic processing. Reads were quality-trimmed using <rs id="a12884405" type="software">Trimmomatic</rs> <rs id="a12884406" type="bibr">35</rs> (adapters were removed and reads were trimmed from ends, starting from regions with an average per-base quality score below 20 on 4-nucleotide sliding windows; remaining reads shorter than 50 bp were discarded). <rs id="a12884407" type="software">IDBA_UD</rs> <rs id="a12884408" type="bibr">62</rs> was used for a single co-assembly of all seven viromes with default parameters. Contigs were processed with <rs id="a12884409" type="software">VirSorter</rs> <rs id="a12884410" type="bibr">32</rs> in virome decontamination mode and were also compared to putative laboratory contaminants (that is, phages cultivated in the M.B.S. laboratory: enterobacteria phage PhiX17, Alpha3, M13, Cellulophaga baltica phages and Pseudoalteromonas phages) via <rs id="a12884411" type="software">BLASTn</rs>. Contigs with &gt; 95% average nucleotide identity to these genomes were removed. Fifty-three contigs ≥ 10 kb in length were retained and used as part of the mapping database for viral population detection in the 214 Stordalen Mire bulk soil metagenomes (Supplementary Fig. 2).</p>
<p>(the viral OTU table) to environmental and geochemical parameters and to host community composition (the host OTU table), analyses were performed using <rs id="a12884412" type="software">PRIMER</rs> v<rs id="a12884413" type="version" corresp="#a12884412">6</rs> <rs id="a12884414" type="bibr">63</rs>,<rs id="a12884415" type="bibr">64</rs> . A Bray-Curtis dissimilarity matrix was generated from the square-root transformed viral OTU table for sample comparisons. Permutational multivariate analysis of variance (
PERMANOVA;
99,999 permutations) was used to test for significant differences in viral community composition among the three thaw habitats. Mantel tests with Spearman's rank correlations (999 permutations) were used to compare viral community composition to continuous and/or multidimensional variables, including environmental and geochemical data (Euclidean distance matrices) and host community composition (Bray-Curtis dissimilarity matrix constructed from the square-root transformed host OTU table).</p>
<p>Pearson correlations were performed using <rs id="a12884416" type="software">SigmaPlot</rs> <rs id="a12884417" type="version" corresp="#a12884416">11.0</rs> ( <rs id="a12884418" type="publisher" corresp="#a12884416">Systat Software</rs>) to assess the relationships between the abundances of viruses, their hosts and environmental variables in all samples, as well as in individual habitats (palsa, bog and fen) (Figs. 2c and3e, Supplementary Fig. 9 and Supplementary Table 11).</p>
<p>Relationships between viral population and host population abundances within and across habitats were investigated using linear regression analyses in <rs id="a12884419" type="software" subtype="environment">R</rs> v<rs id="a12884420" type="version" corresp="#a12884419">3.3.1</rs> <rs id="a12884421" type="bibr">65</rs> . The linear regression models for each lineage in each habitat were then compared using two-way analysis of variance (ANOVA) to test whether the interaction term in the linear regression models (that is, designating samples as palsa, bog or fen) was significantly different from not using an interaction term (that is, all samples included in the regression model) for a given host lineage (Supplementary Table 9).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f237676152"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:37+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Laser scanning confocal microscopy LSCM images were taken using an inverted confocal microscope (Leica TCS SP8 SMD, HCX PL APO 40×/0.85 CORR CS dry objective) using the <rs id="a12967881" type="publisher" corresp="#a12967882">Leica</rs> <rs id="a12967882" type="software">LAS X</rs> software. The surfaces were contaminated as described in the section on "Contamination experiments." LSCM images were taken before and after cleaning. For the visualization of the self-cleaning process, 10-ml water drops (dyed with ATTO 488, 1 mg liter -1 ) were placed onto sparsely contaminated superhydrophobic nanoporous glass slides. The drops were slowly dragged over the surface using a metal needle (0.26-mm outer diameter, 31 gauge) attached on a stage similar to the droplet adhesion force measurements. The self-cleaning process was recorded using the LSCM. The fluorescence of ATTO 488 in water and the fluorescence of the dyed particles were shown in navy blue and pink, respectively. The reflection of light was shown in light blue. Reflection and fluorescence signals were recorded simultaneously.</p>
<p>The LSCM images in Fig. 5 were processed according to the procedure illustrated in fig. S9. For the creation of the three-dimensional (3D) LSCM images, the <rs id="a12967883" type="publisher" corresp="#a12967884">Leica</rs> <rs id="a12967884" type="software">LAS X</rs> (3D viewer) software was used. Inten-sities of the reflection and fluorescence channels were adjusted for best clarity. In Figs. 2C and 3C, particle illustrations were added as described in fig. S9 (A andB).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f210897342"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:20+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Phylogenetic reconstruction methods are crucial for recent quantitative approaches in historical linguistics. While many scholars remain skeptical regarding the potential of methods for automatic sequence comparison, phylogenetic reconstruction, be it of networks using the popular <rs id="a12951855" type="software">SplitsTree</rs> software <rs id="a12951879" type="bibr">(Huson, 1998)</rs>, or family trees, using distance- (Sokal and Michener, 1958;Saitou and Nei, 1987) or character-based approaches (Edwards and Cavalli-Sforza, 1964;Fitch, 1971;Ronquist et al., 2012;Bouckaert et al., 2014), has entered the mainstream of historical linguistics. This is reflected in a multitude of publications and applications on different language families, from Ainu (Lee and Hasegawa, 2013) and Australian (Bowern and Atkinson, 2012) to Semitic (Kitchen et al., 2009) and Chinese (Ben Hamed and Wang, 2006). There is also a growing interest in the implications of phylogenetic analyses for historical linguistics, as can be seen from the heated debate about the dating of Indo-European (Gray and Atkinson, 2003;Atkinson and Gray, 2006;Bouckaert et al., 2014;Chang et al., 2015), and the recent attempts to search for deep genetic signals in the languages of the world (Pagel et al., 2013;Jäger, 2015).</p>
<p>Analyses were carried out using the <rs id="a12951867" type="software">MrBayes</rs> software <rs id="a12951882" type="bibr">(Ronquist et al., 2012)</rs>. Likelihoods were computed using ascertainment bias correction for all-absent characters and assuming Gamma-distributed rates (with 4 Gamma categories).</p>
<p>For each dataset, a maximum clade credibility tree was identified as the reference tree (using the software <rs id="a12951869" type="software" subtype="component" corresp="#a12951870">TreeAnnotator</rs>, retrieved on September 13, 2016; part of the software suite <rs id="a12951870" type="software" subtype="environment">Beast</rs>, cf. <rs id="a12951871" type="bibr">Bouckaert et al., 2014</rs>). Additionally, 100 trees were sampled from the posterior distribution for each dataset and used as tree sample for ASR.</p>
<p>Ancestral state reconstruction For our study, we tested three different established algorithms, namely (1) Maximum Parsimony (MP) reconstruction using the Sankoff algorithm (Sankoff, 1975), (2) the minimal lateral network (MLN) approach (Dagan et al., 2008) as a variant of Maximum Parsimony in which parsimony weights are selected with the help of the vocabulary size criterion (List et al., 2014b(List et al., , 2014c)), and (3) Maximum Likelihood (ML) reconstruction as implemented in the software <rs id="a12951872" type="software">BayesTraits</rs> <rs id="a12951883" type="bibr">(Pagel and Meade, 2014)</rs>. These algorithms are described in detail below.</p>
<p>While the improved versions were primarily used to infer borrowing events in linguistic datasets, List (2015) showed that the MLN approach can also be used for the purpose of ancestral state reconstruction, given that it is based on a variant of weighted parsimony. Describing the method in all its detail would go beyond the scope of this paper. For this reason, we refer the reader to the original publications introducing and explaining the algorithm, as well as the actual source code published along with the <rs id="a12951875" type="software">LingPy</rs> software package <rs id="a12951884" type="bibr">(List and Forkel, 2016)</rs>. To contrast MLN with the variant of Sankoff parsimony we used, it is, however, important to note that the MLN method does not handle singletons in the data, that is, words which are not cognate with any other words.7 It should also be kept in mind that the MLN method in its currently available implementation only allows for the use of binary characters states: multi-state characters are not supported and can therefore not be included in our test.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f343747157"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:31+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>the U-net model. For our analysis, we implemented a U-Net topology of a deep Convolutional Neural Network (CNN) 35 , in <rs id="a12966120" type="software">TensorFlow</rs>, and made the source <rs id="a12966123" type="software" subtype="implicit">code</rs> publicly available at: <rs id="a12966121" type="url" corresp="#a12966123">https ://githu b.com/Intel AI/ unet/tree/maste r/2D</rs> (commit: eaeac1fc68aa309feb00d419d1ea3b43b8725773). All experiments use a dropout parameter of 0.2, upsampling set to true, and args.featuremaps set to 32. training hyper-parameters. See "Supplementary Information" for a table summarizing all the hyperparameters considered in this study. All institutional training in our experiments use mini-batch stochastic optimization and the Adam optimizer 36 , thus require batch size and Adam optimizer hyper-parameters 36 (adam learning rate, adam first moment decay parameter, and adam second moment decay parameter). Additionally, our training loss function requires the smoothing parameter (Laplace smoothing) the 's' of Eq. ( 2) in "Model Quality Metric". These are the only hyper-parameters required for individual institutional training and CDS, and are shared by FL, IIL and CIIL.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f33344426"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:48+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>For research question one, both quantitative data and qualitative data were analyzed. Quantitative data were analyzed using the <rs id="a12951622" type="software">Statistical Package for the Social Sciences</rs> <rs id="a12951623" type="version" corresp="#a12951622">22.0</rs> (<rs id="a12951624" type="software">SPSS</rs> <rs id="a12951625" type="version" corresp="#a12951624">22.0</rs>), with descriptive statistics analysis, one-way repeated measures ANOVA, and paired sample t-tests. Descriptive data analysis was used to obtain the mean score, standard deviation, and other information pertaining to the data from the MDCT and WDCT. One-way repeated measures ANOVA and paired samples t-test were conducted to identify whether students" performance in the MDCT and the WDCT varied across speech acts, social power and ranking of imposition. The qualitative data from the interview were analyzed according to the content analysis method.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f206051508"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T08:15+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>For each indicator, V-Dem provides upper and lower bound estimates, which represent 68% of the highest posterior densities (distribution mass), i.e., a range of most probable values for a given observation. The intervals increase with the degree of ambiguity in the raw, expert-coded data. At the indicator-level, mainly three factors influence the size of the intervals: high levels of disagreement between expert coders, a low number of coders, and the presence of coders with relatively low estimated reliability (i.e., high stochastic error variance). V-Dem uses Bayesian Factor Analysis (BFAs implemented with the <rs id="a12951620" type="software" subtype="environment">R</rs> package <rs id="a12951621" type="software" subtype="component" corresp="#a12951620">MCMCpack</rs>) to aggregate indicators to mid-level indices, such as the Clean Election Index. In the BFA framework, the size of the area covered by the 68 highest posterior densities of mid-level indices increases in size if underlying indicators show low levels of correlation. The BFAs are run over 900 posterior draws from the indicators. As a result, uncertainty about indicators also influences the size of the interval in which the modeling places 68% of the probability mass of the mid-level indices. Similar logic applies for top-level indices (such as the Electoral Democracy Index, and the Liberal Component Index), which combine several mid-level indices(Marquardt &amp; Pemstein, 2017;Pemstein et al., 2017).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f187410236"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:49+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Participants filled out the English version of the <rs id="a12951696" type="software">LexTALE</rs> (<rs id="a12951697" type="bibr">Lemhöfer &amp; Broersma, 2012</rs>). This lexical decision test consists of 60 items, some of which are existing (British) English words while other are English-looking non-words. Participants are asked to distinguish the real words from the non-words. Research has demonstrated that this test provides a reliable measure of the lexical proficiency of learners with different cultural and linguistic backgrounds and is a good indicator of overall English proficiency, at least for learners with intermediate and advanced proficiency levels (Lemhöfer &amp; Broersma, 2012).</p>
<p>Students' <rs id="a12951698" type="software">LexTALE</rs> scores ranged from 37.5 to 98.7. The mean score was 57.4 (SD = 10.4), just within the lower intermediate level according to Lemhofer and Broersma's (2012) classification of general proficiency levels: upper and lower advanced/proficient users (80-100), upper intermediate users (60-79) and lower intermediate and lower (below 59). A Kolmogorov-Smirnov test revealed that the distribution was not quite normal (KS = 0.13, p &lt; .01). Although the distribution shows the distinctive bell curve, it is slightly skewed towards the lower end of the continuum (see Figure 2). As a consequence, the data were analyzed with non-parametric statistics.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f50710198"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T08:16+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>DNA was isolated from buffy coats using the salt precipitation method applying either a manual protocol (Miller et al., 1988) or a semiautomated protocol based on the Autopure System (Qiagen, Hilden, Germany). Bisulfite treatment of 500 ng genomic DNA was carried out with the EZ-96 DNA methylation kit (Zymo Research, Orange County, CA, USA) following the manufacturer's protocol. Genomewide DNA methylation was measured using the Infinium HumanMethylation450K BeadChip (Illumina, San Diego, CA, USA) at the Leiden University Medical Center and in accordance with the manufacturer's instructions. Data preprocessing was performed using the free <rs id="a12951538" type="software" subtype="environment">R</rs> package <rs id="a12951539" type="software" subtype="component" corresp="#a12951538">minfi</rs> <rs id="a12951540" type="bibr">(Maksimovic et al., 2012)</rs>, and a DNA methylation level was summarized for each CpG site by calculating a 'beta' value ranging from 0 to 100%. CpG probes were treated as missing if the detection P-value &gt; 0.01, and CpG sites with more than 5% missing data were excluded from the analysis.</p>
<p>In the case of the birthweight-discordant twin sample, we additionally tested the relationship between birthweight and DNAm age using a linear mixed regression model with birthweight, age, and sex as fixed effects and twin pairing as a random factor. A nonlinear relationship between birthweight and DNAm age was assessed by converting birthweight into dummy variables according to its 0.25, 0.5, and 0.75 quantiles and using the first group as reference. In the case of the LSADT, we estimated the population average (marginal) and within-twin pair (conditional) association of DNAm age with mortality. The twins included in the mortality study all participated in the 10-year follow-up, and therefore, the mortality assessment started after the follow-up sample was drawn. In all survival analyses, the possible effect of cell composition was estimated and adjusted for by the approach proposed by Houseman et al. (2012). To assess the conditional influence of estimated methylation age, first, we computed the proportion of the co-twins with a higher DNAm age who died first and compared it with the null hypothesis that both co-twins are equally likely to die first by an exact binomial test. Second, we used a conditional logistic regression stratified by twin pairs and adjusted for cell composition to measure the influence of within-twin pair DNAm age difference on the odds of dying first. To measure the marginal effect of DNAm age on survival, a Cox proportional hazards regression stratified by sex and adjusted for cell composition was employed. As some twins might have died before they could have been included in the study, all co-twins in the study were considered left truncated at the age of entering the follow-up. The Cox regression was performed using a robust sandwich estimator of standard errors to account for correlation between twin pairs. Schoenfeld residuals were checked for deviations from the proportional hazards assumption. All statistical analyses were carried out using <rs id="a12951532" type="software">R</rs> <rs id="a12951533" type="version" corresp="#a12951532">3.1.3</rs> ( <rs id="a12951536" type="publisher" corresp="#a12951532">R Core Team</rs>
2015 -<rs id="a12951609" type="url" corresp="#a12951532">https://www.r-project.org</rs>).</p>
<p><rs id="a12951537" type="bibr">(Marioni et al., 2015a)</rs></p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f300328188"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:35+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Scanning probe microscopy (SPM) has revolutionized the fields of materials, nano-science, chemistry, and biology, by enabling mapping of surface properties and surface manipulation with atomic precision. However, these achievements require constant human supervision; fully automated SPM has not been accomplished yet. Here we demonstrate an artificial intelligence framework based on machine learning for autonomous SPM operation (<rs id="a12972073" type="software">DeepSPM</rs>). <rs id="a12972074" type="software">DeepSPM</rs> includes an algorithmic search of good sample regions, a convolutional neural network to assess the quality of acquired images, and a deep reinforcement learning agent to reliably condition the state of the probe. <rs id="a12972075" type="software">DeepSPM</rs> is able to acquire and classify data continuously in multi-day scanning tunneling microscopy experiments, managing the probe quality in response to varying experimental conditions. Our approach paves the way for advanced methods hardly feasible by human operation (e.g., large dataset acquisition and SPM-based nanolithography). <rs id="a12972076" type="software">DeepSPM</rs> can be generalized to most SPM techniques, with the source code publicly available.</p>
<p>Here we present <rs id="a12972077" type="software">DeepSPM</rs>, an autonomous system capable of continuous SPM data acquisition. It consists of the following: (i) algorithmic solutions to select good imaging sample regions and perform measurements; (ii) a classifier CNN trained through supervised learning that assesses the state of the probe; and (iii) a deep RL agent that repairs the probe by choosing adequate conditioning actions. <rs id="a12972078" type="software">DeepSPM</rs> also addresses other typically arising issues (lost contact, crashed probe, and moving probe larger (macroscopic) distances to new approach areas).</p>
<p>To train and evaluate <rs id="a12972079" type="software">DeepSPM</rs>, we used a low-temperature STM with a metallic probe (Pt/Ir) to image a model sample: magnesium phthalocyanine (MgPc) molecules adsorbed on a silver surface (Fig. 1 and Methods). Such molecular systems are scientifically and technologically relevant, owing to their electronic, optical, and chemical properties 29,30 . SPM provides an ideal tool for their characterization but also presents challenges (e.g., imagealtering probe-molecule interactions). Although spatial resolution may be poorer in comparison with a semiconducting or functionalized probe 23 , a metallic probe is required for many SPM techniques [e.g., scanning tunneling spectroscopy (STS) 5 and Kelvin probe force microscopy (KPFM) 31 ].</p>
<p> <rs id="a12972080" type="software">DeepSPM</rs> overview. <rs id="a12972081" type="software">DeepSPM</rs> works as a control loop (Fig. 1a). The artificial intelligence system drives the SPM by selecting an appropriate scanning region (Supplementary Fig. 1), acquires an image, and assesses the acquired image data (Fig. 1b). If the image is deemed "good", it is processed and stored (Fig. 1c), and <rs id="a12972082" type="software">DeepSPM</rs> proceeds with the next loop iteration. If the image is labeled "bad", <rs id="a12972083" type="software">DeepSPM</rs> addresses the issues, maintaining continuous and stable operation (see Methods).</p>
<p>In Fig. 2, we give a graphical description of <rs id="a12972084" type="software">DeepSPM</rs>. The system is able to assess and identify causes of a defective acquisition (e.g., lost sample-probe contact, probe crash, bad sample region, bad probe). If sample-probe contact is lost or the probe crashes, <rs id="a12972085" type="software">DeepSPM</rs> re-establishes contact in a new scanning region (Methods). Images of bad sample regions (e.g., excessive roughness and contamination) can be identified algorithmically by the fact that measured heights span over a larger range compared with clean regions (see Methods). If a region is identified as a "bad sample", <rs id="a12972086" type="software">DeepSPM</rs> selects a new one and performs a new measurement.</p>
<p>Intelligent probe quality assessment. If <rs id="a12972087" type="software">DeepSPM</rs> concludes that the sample imaging region is "good", it assesses the state of the probe: the classifier CNN (Supplementary Table 1) inspects the recorded image and predicts the probability of it being recorded with a bad probe (Fig. 3a). To train the classifier, we used a dataset of 7589 images of the MgPc/Ag(100) sample, labeled as acquired either with a "good" or "bad probe". In addition, we used data augmentation to increase the amount of training data (see Methods). It is noteworthy that the category "bad probe" includes various kinds of probe defects (Fig. 1b). We tested its performance on an unseen test dataset, achieving an accuracy of ~94% (Supplementary Table 1), a positive predictive value ~87% and a negative predictive value ~96%. As point of reference, classification accuracy of a human in a benchmark visual object recognition challenge 32 (ImageNet) ranges from 88% to 95%, on par with our CNN classifier. It is noteworthy that classification performance is intrinsic to the type of data and the specific datasets considered. We are, in concurrence with other work 33 , among the first to present a dataset of this kind and there are currently no available baselines for comparison. However, the performance achieved by <rs id="a12972088" type="software">DeepSPM</rs> enables autonomous data acquisition and long-term operation of <rs id="a12972089" type="software">DeepSPM</rs>.</p>
<p>Intelligent probe conditioning. If the classifier CNN concludes that the probe is bad, <rs id="a12972090" type="software">DeepSPM</rs> uses a deep RL agent to condition it (Fig. 3b). This RL agent is controlled by a second CNN (action CNN), which is trained by interacting with the SPM setup: the RL agent inspects the last recorded image and performs a probeconditioning action, selected from a list of 12 actions. We determined this list by considering actions commonly used for probe conditioning by expert human operators (Methods, Supplementary Note 1, and Supplementary Table 2); they consist of either a voltage pulse applied between probe and sample, or a dip of the probe into the sample 9 . After each conditioning step, <rs id="a12972091" type="software">DeepSPM</rs> evaluates the outcome of the conditioning process by acquiring the next image, which is then assessed by the classifier CNN. If the new image is classified as "bad probe", the agent receives a negative reward (r = -1) and proceeds with another action. If the image is classified as "good probe", the conditioning episode (sequence of conditioning steps; Fig. 3c) is terminated and the agent receives a positive reward (r = 10; Methods).</p>
<p>Autonomous SPM operation. We next demonstrate long-term autonomous operation of the entire <rs id="a12972092" type="software">DeepSPM</rs> system. For a period of 86 h, we let <rs id="a12972093" type="software">DeepSPM</rs> control the microscope. Figure 4a shows <rs id="a12972094" type="software">DeepSPM</rs>'s behavior in an approach area, highlighting occurrences of bad probe detection and conditioning, and avoiding "bad sample" regions. In Fig. 4b, we show the area scanned by the system as a function of time. In total, <rs id="a12972095" type="software">DeepSPM</rs> scanned a sample area of 1.2 μm 2 (Fig. 4), recorded &gt;16,000 images, handled 2 lost contacts, identified and avoided 1075 regions of excessive roughness, and repaired the probe 117 times (Supplementary Table 3).</p>
<p>To evaluate the overall performance of <rs id="a12972096" type="software">DeepSPM</rs>, we manually inspected the recorded images (Supplementary Note 3). Out of all images labeled "good" by <rs id="a12972097" type="software">DeepSPM</rs>, ~87% were found to be without defects or imaging artifacts. Out of all conditioning episodes initiated by <rs id="a12972098" type="software">DeepSPM</rs>, ~86% were found to be really necessary (Supplementary Fig. 5, Methods, and Supplementary Fig. 6). It is noteworthy that these performance metrics are not related to static classification (as for the classifier CNN testing), as the state of the STM/sample system and the recorded images depend dynamically on the decisions made by <rs id="a12972099" type="software">DeepSPM</rs>.</p>
<p>In our specific case here, the single images that <rs id="a12972100" type="software">DeepSPM</rs> records and that determine the RL agent's conditioning action selection do not enable the retrieval of the atomistic morphology of the probe. Therefore, the conditioning process behaves effectively as if it had memory; the probe state depends on the specific sequence of previous actions and images. Continuous training of the RL agent during operation allows it to follow the evolution of the probe state and is hence essential to achieve better-thanrandom performance during operation (Supplementary Fig. 4).</p>
<p>Automation of complex experimental procedures such as SPM frees valuable researcher time. <rs id="a12972101" type="software">DeepSPM</rs> brings state-of-the-art SPM closer to a turnkey application, enabling non-expert users to achieve optimal performance.</p>
<p><rs id="a12972102" type="software">DeepSPM</rs> can be applied directly to any sample, probe material, or STM/AFM setup, as long as specific training datasets are available (see Methods). It can further be expanded to other SPM spectroscopy techniques (e.g., STS and KPFM), where probe quality conditions would need to include additional spectroscopic requirements 5,31 (Supplementary Note 5). Fully autonomous SPM also opens the door to high-throughput and scalable atomically precise nano-fabrication 35 , hardly feasible via manual operation.</p>
<p>After moving the probe macroscopically to a new sample area, <rs id="a12972103" type="software">DeepSPM</rs> extends the z piezo scanner to ~80% of its maximum extension, to maximize the range of tunneling current feedback controlling the z position of the scanner without crashing or losing contact. If tunneling contact is lost, <rs id="a12972104" type="software">DeepSPM</rs> reapproaches the probe. <rs id="a12972105" type="software">DeepSPM</rs> also handles probe crashes (see section below "Detection and fixing of lost contact/probe crash").</p>
<p>After approaching the probe to the sample, <rs id="a12972106" type="software">DeepSPM</rs> waits ~120 s before starting a new scan, to let thermal drift and (mainly) creep of the z piezo settle. New measurements start at the neutral position of the xy scan piezo (that is, x = y = 0), where no voltages are applied to the xy piezo scanner, to minimize the piezo creep in the xy scanning plane. <rs id="a12972107" type="software">DeepSPM</rs> selects the next imaging region by minimizing the distance that the probe travels between regions (see Fig. 4, section "Finding the next imaging region" and Supplementary Fig. 1), further reducing xy piezo creep. After moving the probe to a new imaging region, <rs id="a12972108" type="software">DeepSPM</rs> first records a partial image that is usually severely affected by distortions due to xy piezo creep. This image is discarded and a second image, not or only minimally affected, is recorded at the same position. During autonomous operation, the RL agent of <rs id="a12972109" type="software">DeepSPM</rs> continues to learn about probe conditioning (see main text and below). To avoid damaging a good probe by unnecessary conditioning during autonomous operation, <rs id="a12972110" type="software">DeepSPM</rs> initiates a probe-conditioning episode only after ten consecutive images have been classified as "bad probe" by the classifier CNN (Supplementary Fig. 6). The episode is terminated as soon as the first image is classified as "good probe" by the classifier CNN. Images that are not part of a conditioning episode (including the ten consecutive images that triggered it), or that have been disregarded due to "bad sample", lost probe-sample contact or crashed probe, are labeled as "good image".</p>
<p>Finding the next imaging region. For each new approach area, <rs id="a12972111" type="software">DeepSPM</rs> starts acquiring data at the center of the scanning range (Fig. 4a). <rs id="a12972112" type="software">DeepSPM</rs> uses a binary map to block the imaging regions that have already been scanned (Supplementary Fig. 1). If a region is identified as "bad sample" (e.g., excessive roughness is detected), <rs id="a12972113" type="software">DeepSPM</rs> defines a larger circular area around it (as further roughness is expected in the vicinity), and avoids scanning this area. The radius r forbidden of this region is increased as consecutive imaging regions are identified as "bad sample":</p>
<p>As probe-conditioning actions can cause debris and roughness on the sample, <rs id="a12972114" type="software">DeepSPM</rs> blocks a similar circular area to avoid around the location of each performed conditioning action. The size of this area depends on the executed action (Supplementary Table 2). <rs id="a12972115" type="software">DeepSPM</rs> chooses the next imaging region centered at position v t ¼ x t x þ y t ŷ (x t , y t are coordinates with respect to the center of the approach area) that minimizes</p>
<p>provided that this is a valid imaging region according to the established binary map. Here, v t-1 denotes the position of the center of the last imaging region, ||…|| 1 is the Manhattan norm, and ||…|| 2 is the standard Euclidian norm. The parameter α controls the relative weight of the two distances, i.e., to the center of the last scanned region (to minimize travel distance), and to the center of the approach area (to efficiently use the entire available area). We found α = 1 works well. This algorithm minimizes the distance the probe travels between consecutive imaging regions, reducing the impact of xy piezo creep. Once the area defined by the fine piezo scanner range has been filled, or the distance to the center of the next available scanning region is larger than 500 nm, <rs id="a12972116" type="software">DeepSPM</rs> moves the probe (macroscopically, with the coarse positioning system) to a new approach area.</p>
<p><rs id="a12972117" type="software">DeepSPM</rs> architecture. The <rs id="a12972118" type="software" subtype="environment">DeepSPM</rs> framework consists of two components: (i) the <rs id="a12972119" type="software" subtype="component" corresp="#a12972118">controller</rs>, written in <rs id="a12972120" type="language" corresp="#a12972119">Python</rs> and <rs id="a12972121" type="software" corresp="#a12972118">TensorFlow</rs>, and (ii) a TCP server, written in
Labview. The <rs id="a12972122" subtype="component" type="software">controller</rs> contains the image processing, classifier CNN, and RL agent. The TCP server creates an interface between the <rs id="a12972123" subtype="component" type="software">controller</rs> and the <rs id="a12972124" type="software">Nanonis SPM</rs> software. The <rs id="a12972125" subtype="component" type="software">controller</rs> sends commands via TCP, e.g., for acquiring and recording an image, executing a conditioning action at a certain location. The server receives these commands and executes them on the <rs id="a12972126" type="software">Nanonis/SPM</rs>. It returns the resulting imaging data via TCP to the <rs id="a12972127" subtype="component" type="software">controller</rs>, where it is processed to determine the next command. Based on this design, the agent can operate on hardware decoupled from the <rs id="a12972128" type="software">Nanonis SPM</rs> software.</p>
<p>Classifier CNN. The classifier CNN uses the architecture above. It has a single neuron output layer with a sigmoid activation function. This output (ranging from 0 to 1) gives the classifier CNN's estimate of the probability that the input image was recorded with a "good probe". The decision threshold was set to 0.9. It is noteworthy that <rs id="a12972129" type="software">DeepSPM</rs> requires ten consecutive images classified as "bad probe" to start a conditioning episode (Supplementary Fig. 6). We trained the classifier CNN using the <rs id="a12972130" type="software">ADAM</rs> <rs id="a12972131" type="bibr">39</rs> optimizer with a cross-entropy loss and L2 weight decay with a value of 5 × 10 -5 and a learning rate of 10 -3 . To account for the imbalance of our training set ("good probe" 25% and "bad probe" 75%), we weighed STM images labeled as "good probe" by a factor of 8 when computing the loss 40 . In addition, we increased the available amount of training data via data augmentation, randomly flipping the input SPM images horizontally or vertically. It is noteworthy that all training data consisted of experimental data previously acquired and labeled manually.</p>
<p>Reinforcement learning agent and action CNN. Our RL agent responsible for the selection of probe-conditioning actions is based on double DQN 34 , which is an extension of DQN 28 . We modified the double DQN algorithm to suit the requirements of <rs id="a12972132" type="software">DeepSPM</rs> as follows. The action CNN controlling the RL agent uses the architecture above, with a single constant-current STM image as input. It is noteworthy that the original DQN uses a stack of four subsequent images. Our action CNN has an output layer consisting of 12 nodes, one for each conditioning action. The output of each node is interpreted as the Q-value of the corresponding action, i.e., the expected future reward to be received after executing it. We initialized the weights of the action CNN (excluding the output layer) with those of the previously trained classifier CNN, based on the assumption that the features learned by the latter are useful for the action CNN 41 . The output layer, which has a different size in both networks, is initialized with the Xavier initialization 38 . To train the action CNN, we let it operate the SPM, acquiring images, and selecting and executing probe-conditioning actions repeatedly when deemed necessary (Figs. 1 and2). Once sufficient probe quality was reached (i.e., the probability predicted by the classifier CNN exceeded 0.9), the conditioning episode was terminated-a conditioning episode consists of the sequence of probe-conditioning actions required to obtain a good probe. Random conditioning actions (up to five) were then applied to reset (i.e., re-damage the probe), until the predicted probability drops below 0.1. The RL agent received a constant reward of -1 for every executed probe-conditioning action. It received a reward of +10 for each terminated training episode, i.e., each time the probe was deemed good again. We chose these reward values heuristically by testing them in a simulated environment. In these simulations, the RL agent executed conditioning actions and the reward protocol was applied based on images resulting from the convolution of a good, clean synthetic image with a model kernel representing the probe morphology. Following a conditioning action, this kernel was updated stochastically. In this reward scheme, the RL agent receives a positive cumulative reward for and favors short conditioning episodes, whereas it receives a negative cumulative reward and is punished for longer episodes.</p>
<p>The RL agent uses ε-greedy exploration to gather experience 25 . For each conditioning step, the agent chooses a conditioning action probabilistically based on parameter ε (0 &lt; ε &lt; 1): it chooses randomly with a probability ε, and it chooses the action with the largest predicted future reward (Q-value) with probability (1 -ε). For example, if ε = 1, action selection is strictly random; if ε = 0, action selection is based strictly on predicted Q-value. We start training (Supplementary Fig. 2) with 500 random steps (ε = 1) that are used to pre-fill an experience replay buffer 28 . This buffer contains all experiences the agent has gathered so far, each consisting of an input image, the chosen action and its outcome (the next image assessed by the classifier CNN, as well as the reward received). We used data augmentation, adding four experiences to the buffer for each step. These additional experiences consisted of images flipped horizontally and vertically. After 500 steps (i.e., 2000 experiences in the buffer), we started training the action CNN with the buffer data. We used the <rs id="a12972133" type="software">ADAM</rs> optimizer <rs id="a12972134" type="bibr">39</rs> with a batch size of 64 images processed simultaneously and with a constant learning rate of 5 × 10 -4 . We limited the buffer size to 15,000, with new experiences replacing the old ones (first-in, first out). To allow parallel execution and increase the overall performance of the training, we decoupled the gathering of experience and the learning into separate threads. During training, we decreased ε linearly over 500 steps, from 1.0 to 0.05. After reaching ε = 0.05, we continued training with additional 4360 steps, during which we kept ε = 0.05 constant 34 . We used a constant discount factor of γ = 0.95 25 .</p>
<p>STM image pre-processing. The scanning plane of the probe is never perfectly parallel to the local surface of the sample. This results in a background gradient in the SPM images that depends on the macroscopic position and, to a lesser extent, on the nanoscopic shape of the probe. This gradient was removed in each image by fitting and subtracting a plane using <rs id="a12972135" type="software" subtype="component" corresp="#a12972138">RANSAC</rs> <rs id="a12972136" type="bibr">42</rs> (<rs id="a12972137" type="software" subtype="environment">Python</rs> <rs id="a12972138" type="software" subtype="component" corresp="#a12972137">scikit-learn</rs> implementation; polynomial of degree 1, residual threshold of 5 × 10 -12 , max trials of 1000). The acquired STM data were further normalized and offset to the range [-1; 1], i.e., such that pixels corresponding to the flat Ag(100) had values of -1 and those corresponding to the maximum apparent height of MgPc (~2 Å) had values of 1. In addition, we limited the range of values to [-1.5, 1.5], shifting any values outside this range to the closest one inside the interval.</p>
<p>Finding an appropriate action location. For a given acquired STM image, <rs id="a12972139" type="software">DeepSPM</rs> executes a probe-conditioning action at the center of the largest clean Ag (100) square area (Fig. 3). This center is found by calculating a binary map from the pre-processed image (see above), where pixels close (≤0.1 Å) to the surface fitted plane are considered empty, i.e., belong to a clean Ag(100) patch, and all others as occupied. The center of the largest clean Ag(100) square area within this binary map was chosen as the conditioning location. We defined an area requirement for each conditioning action (Supplementary Table 1). A conditioning action is allowed and can be selected by the agent only if the available square area is within this specified requirement.</p>
<p>Detection and fixing of lost contact/probe crash. <rs id="a12972140" type="software">DeepSPM</rs> is able to detect and fix any potential loss of probe-sample contact during scanning. It does so by monitoring the extension (z-range) of the fine piezo scanner; if the fine piezo scanner extends in the z-direction beyond a specified threshold (towards the sample surface), <rs id="a12972141" type="software">DeepSPM</rs> prevents the potential loss of probe-sample contact by re-approaching the probe towards the sample with the coarse probe-positioning system (until the probe is within an acceptable distance range from the sample). Data acquisition can then continue at the same position. Similarly, <rs id="a12972142" type="software">DeepSPM</rs> can prevent probe-sample crashes, i.e., by increasing the probe-sample distance with the coarse probe-positioning system if the fine piezo scanner retracts in the z-direction beyond a specified threshold (away from the sample surface).</p>
<p>The source <rs id="a12972143" type="software" subtype="implicit">code</rs> will be made available at <rs id="a12972144" type="url" corresp="#a12972143">https://github.com/abred/DeepSPM</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f481114053"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:23+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>DGG System. All previous global historical population and land use reconstructions have used 5 arc minute geographic units designed for compatibility with climate models. This use of nonequal area units, which vary from ∼85 km 2 at the equator to ∼10 km 2 at the poles, causes biases in global statistical analyses toward Northern latitudes where grid cells are smaller and therefore more numerous per unit land area while also distorting proportional area measures, like those used in anthrome classification, and scale-dependent variables, like species richness. In this study, all data were assessed in an equal area DGG system recently used in policy-relevant global biodiversity assessments to represent regional landscapes across Earth's land surface outside Antarctica (49)(50)(51) Visualization and Statistical Analysis. All spatial and statistical analyses were done in <rs id="a12972879" type="software">R</rs> version <rs id="a12972880" type="version" corresp="#a12972879">3.6.3</rs> <rs id="a12972881" type="bibr">(68)</rs>. Global-, regional-, and biome-level anthrome area changes over time (anthrome trajectories) were computed and charted based on DGG land areas. For protected areas, KBAs, and other variables that occupied only part of DGG cells, anthrome sums were weighted by the relative areas of each respective variable in each DGG cell. HYDE 3.2 data were also used to compute total populations at the global, regional, and biome levels over time, and these were overlaid on anthrome trajectory charts after scaling them to their maximum value (typically the population in 2017 CE).</p>
<p>Data Availability. Source <rs id="a12972882" type="software" subtype="implicit">code</rs> to reproduce these analyses and visualizations is available as an R research compendium on the Harvard Dataverse [<rs id="a12972883" type="bibr">(77)</rs>, <rs id="a12972884" type="url" corresp="#a12972882">https://doi.org/10.7910/DVN/6FWPZ9</rs>]. Although the entire analysis can be made from the source <rs id="a12972885" type="software" subtype="implicit">code</rs> using publicly available data, intermediate data products such as the full Level 12 DGG anthrome classification, maps, and summary statistics are available on the Harvard Dataverse [(78), https://doi. org/10.7910/DVN/E3H3AK].</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f186698023"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T14:17+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Single-crystal X-ray diffraction datasets were collected on a conventional X-ray diffractometer at Aarhus University as well as at the BL02B1 beamline at the SPring8 synchrotron in Japan. For the conventional data, a high-quality single crystal with dimensions 0.24 Â 0.22 Â 0.18 mm was selected under a polarizing microscope and mounted using Paratone-N oil on an Agilent Technologies SuperNova diffractometer fitted with a microfocus Mo K X-ray tube. The crystal was cooled to 100 K at a rate of 60 K h À1 using an Oxford Cryosystems Cryostream 700. High-resolution X-ray data up to (sin /) max = 1.1 A ˚À1 with high redundancy ($ 10) and completeness ($ 100%) were obtained. The complete details of data collection and reduction procedures were published elsewhere (Jørgensen et al., 2014). Synchrotron X-ray data was collected to a resolution of (sin /) max = 1.51 A ˚À1 at 20 K on a single crystal of maximum dimension 0.10 Â 0.09 Â 0.08 mm using a wavelength of 0.35312 A ˚. The BL02B1 beamline is equipped with a Rigaku kappa diffractometer and a cylindrical imageplate detector. Integration of all Bragg reflections and Lorentz-polarization corrections were carried out with the software <rs id="a12971510" type="software">RAPID-AUTO</rs> <rs id="a12971511" type="bibr">(Rigaku, 2004)</rs>. Sorting, scaling, merging and empirical absorption correction were carried out using the <rs id="a12971512" type="software">SORTAV</rs> program <rs id="a12971513" type="bibr">(Blessing, 1995)</rs>. The crystal structure was solved by direct methods in <rs id="a12971514" type="software">SHELXS</rs> <rs id="a12971515" type="bibr">(Sheldrick, 2008)</rs> and refined using <rs id="a12971516" type="software" corresp="#a12971519">SHELXL</rs> <rs id="a12971517" type="version" corresp="#a12971516">97</rs> <rs id="a12971518" type="bibr">(Sheldrick, 2008)</rs> in the <rs id="a12971519" type="software" subtype="environment">WinGX</rs> package <rs id="a12971520" type="bibr">(Farrugia, 2012)</rs>. All H atoms were located from the difference-Fourier analysis. Full crystallographic details are listed in the supporting information (Table S1).</p>
<p>Single-crystal neutron diffraction data on rubrene were collected at 100 K using a block-shaped crystal with dimensions 1.5 Â 1.5 Â 1.0 mm on the single-crystal time-of-flight Laue diffractometer, TOPAZ, located at the Spallation Neutron Source at Oak Ridge National Laboratory. The integration of collected data was carried out using the program Mantid (Taylor et al., 2012;Schultz et al., 2014). The incident beam spectrum and detector efficiency corrections were performed in the program <rs id="a12971521" type="software">ANVRED</rs> <rs id="a12971522" type="bibr">(Schultz et al., 1984)</rs>. The crystal structure was refined using the <rs id="a12971554" type="software">GSAS</rs> program <rs id="a12971555" type="bibr">(Larson &amp; Von Dreele, 1994;Toby, 2001)</rs>. Detailed description of the data collection and reduction procedures have been reported elsewhere (Jørgensen et al., 2014).</p>
<p>Gas-phase quantum-mechanical simulations were performed at the experimental geometry using the B3LYP (Becke, 1993) functional with 6-311G(d,p) basis set using the <rs id="a12971523" type="software">GAUSSIAN</rs> <rs id="a12971524" type="version" corresp="#a12971523">09</rs> package <rs id="a12971525" type="bibr">(Frisch et al., 2009)</rs>. The topological analysis of the ED, (r), was performed with a modified version of the program package <rs id="a12971526" type="software">PROAIM</rs> <rs id="a12971527" type="bibr">(Bieglerkonig et al., 1982)</rs>. Basis-set superposition error (BSSE) corrected interaction energies of molecular dimers at the crystal geometry were also evaluated. Periodic quantum-mechanical simulations at the experimental geometry were performed with the Linear Combination of Gaussian-Type Functions (LCGTF) approach as implemented in <rs id="a12971528" type="software">CRYSTAL</rs> <rs id="a12971529" type="version" corresp="#a12971528">14</rs> <rs id="a12971530" type="bibr">(Dovesi et al., 2014)</rs> at the B3LYP/6-31G(d,p) level of theory. The reciprocal space was sampled with a 4 Â 4 Â 4 grid in the irreducible Brillouin zone. A 30% mixing of the Fock matrices was applied to accelerate convergence, while the tolerances determining the level of accuracy of the Coulomb and exchange series were set to 10 À7 (ITOL1 to ITOL4) and 10 À14 (ITOL5). Theoretical structure factors with the same indices as observed in the respective experiment were computed separately and employed to derive a theoretical multipoleprojected ED distribution in XD2006 (Volkov et al., 2006). Furthermore, the topological analysis and the evaluation of the integral properties were computed directly on the LCGTF ED using the <rs id="a12971556" type="software">TOPOND</rs> <rs id="a12971531" type="bibr">(Gatti et al., 1994)</rs> package interfaced with the <rs id="a12971532" type="software" subtype="environment">CRYSTAL</rs> <rs id="a12971533" type="version" corresp="#a12971532">14</rs> <rs id="a12971534" type="software" subtype="implicit" corresp="#a12971532">code</rs>. The experimentally obtained geometry was used as input for the calculation of the lattice energy and intermolecular interaction energy using the <rs id="a12971535" type="software" subtype="component" corresp="#a12971536">PIXELC</rs> module of the <rs id="a12971536" type="software" subtype="environment">CLP</rs> computer program package (version <rs id="a12971537" type="version" corresp="#a12971536">June 2013</rs>; <rs id="a12971538" type="bibr">Gavezzotti, 2011</rs>). For this purpose, an accurate ED of the molecule was obtained independently by the MP2 and B3LYP calculations with a 6-31G(d,p) basis set in the <rs id="a12971539" type="software">GAUSSIAN</rs> <rs id="a12971540" type="version" corresp="#a12971539">09</rs> package <rs id="a12971541" type="bibr">(Frisch et al., 2009)</rs>. The interaction energies of the selected molecular pairs were extracted from the analysis of crystal packing along with involved intermolecular interactions using the.mlc file generated by the
PIXEL calculations. The contribution of Coulombic, polarization, dispersion and repulsion components were obtained for both the lattice energy and total intermolecular interaction energies.</p>
<p>For the 100 K model the anisotropic displacement parameters (ADPs) for the H atoms were obtained from the neutron experiment. The used ADPs of H atoms were scaled based on a least-squares fit between the ADPs of the C atoms from the X-ray ED model and the neutron model, respectively, using the program <rs id="a12971542" type="software">UIJXN</rs> <rs id="a12971543" type="bibr">(Blessing, 1995)</rs>. In the absence of neutron data measured at 20 K, the ADPs for hydrogen at this temperature were estimated using the <rs id="a12971544" type="software">SHADE2</rs> webserver <rs id="a12971545" type="bibr">(Madsen, 2006)</rs>, where the C-H bond distances were constrained to the values used in the 100 K ED model. In order to compare the ED results from the two temperatures, the same multipole modelling procedure was followed for both datasets and a detailed description of multipole modelling can be found elsewhere (Jørgensen et al., 2014). The topological analysis of the ED was carried out in the framework of Bader's Quantum Theory of Atoms in Molecules (QTAIM) (Bader, 1990). The NCI analysis in rubrene was carried out on the experimentally obtained ED using the <rs id="a12971546" type="software">NCImilano</rs> program <rs id="a12971547" type="bibr">(Saleh et al., 2013)</rs>. The multipole models for both datasets were examined by the Hirshfeld rigid bond test (Hirshfeld, 1976) to confirm successful deconvolution of thermal and electronic effects. The largest differences of mean-square displacement amplitudes (DMSDA) of all covalent bonds involving non-hydrogen atoms were found to be 3 Â 10 À4 A ˚2 for the C3-C4 bond in both 100 K and 20 K ED models. The minimum and maximum residual ED peaks in the multipole model [calculated for I &gt; 3(I)] were À0.18 and 0.18 e A ˚À3 at 100 K and À0.19 and 0.23 e A ˚À3 at 20 K. In addition, normal probability plots, variation of scale factor with resolution, and the fractal dimension plots of the residual densities were used to confirm the high quality of the ED models (see the supporting information and our previous publication; Jørgensen et al., 2014; for more details). The ADPs obtained for non-H atoms from the neutron-diffraction data and multipole model against high-resolution X-ray data at 100 K were compared to gauge the quality of the obtained datasets. The ADPs refined against the two 100 K datasets were found to be in excellent agreement with minimum deviations in mean ADPs and among the smallest mean average differences, h|ÁU|i, ever reported for an organic compound at liquid N 2 temperatures (Morgenroth et al., 2008). To validate the experimental ED results, theoretical calculations were performed on the geometry obtained from the multipole models. The topological parameters obtained for all covalent bonds from the experiment are in good agreement with theoretical values (Table S2 in the supporting information). It is noteworthy that the agreement between theory (multipole projected) and experiment for the bond topology consistently is better for the 20 K data than for the 100 K data. Thus, for the C-C bonds the average difference of the electron density at the bond critical point is hÁ C-C i = 0.053 e A ˚À3 at 20 K and 0.099 e A ˚À3 at 100 K. For the C-H bonds the values are hÁ C-H i = 0.035 e A ˚À3 at 20 K and 0.060 e A ˚À3 at 100 K. This also indicates that the improved accuracy of the thermal deconvolution at 20 K is more important than the lack of unbiased neutron ADPs for the H atoms at 20 K. In general, the most accurate experimental EDs for organic crystals can be obtained at the lowest possible temperature using the X-N procedure.</p>
<p>The values reported in the first, second and third lines correspond to the experimental multipole model, theoretical multipole model and the theoretical LCGTF from <rs id="a12971548" type="software">TOPOND</rs>, respectively; G, V and H are kinetic, potential and total energy densities at b.c.p., respectively. Errors on the experimental r 2 b are not available, but combined random (least squares) and systematic (model) errors are at least on the second digit after the decimal point.</p>
<p>In addition to the PIXEL calculations, the lattice energy and intermolecular interaction energies for selected molecular pairs were calculated from the ED model using the <rs id="a12971549" type="software" subtype="component" corresp="#a12971550">XDPROP</rs> module in <rs id="a12971550" type="software" subtype="environment">XD2006</rs> <rs id="a12971551" type="bibr">(Volkov et al., 2006)</rs>. The resulting total energy is composed of electrostatic, exchange-repulsion and dispersion terms. The electrostatic term is estimated using the Exact Potential and Multipole Method (EP/MM) (Volkov et al., 2004), while the exchange-repulsion and dispersion terms are approximated by Williams and Cox atom-atom potentials (Williams &amp; Cox, 1984). Obtained values from the ED models and corresponding theoretical models are listed in Table 3. The derived energy values from the ED models are in good agreement with the
PIXEL values for all interacting molecular dimers (I to IV), whereas the lattice energy deviates significantly in the ED models (see Tables 2 and3). In comparison with the
PIXEL values, the maximum deviation of $ 7 kJ mol À1 was observed for the molecular dimer with C Á Á ÁC interactions (dimer I) in the theoretical ED model at 100 K. In contrast, a difference of $ 44 kJ mol À1 was found for lattice energies obtained in the ED models. The two methods use different schemes to evaluate lattice and intermolecular dimer energies and significant differences arise in the estimation of polarization energy contribution to the total energy (Gavezzotti, 2011;Volkov et al., 2004). In the multipole model (both experimental and theoretical models), the ED of one rubrene molecule is computed within the crystal, and therefore it inherently contains effects of polarization from the surrounding molecules in the crystal. Thus, the electrostatic energy (E es ) calculated for a pair of molecules is the sum of the unperturbed electrostatic interaction between the two molecules along with the polarization contributions due to the entire crystal. The E es and E pol quantities cannot be retrieved separately from each other in the multipole method. However, in the
PIXEL calculation, the electrostatic energy is that of the unperturbed molecules and the polarization energy is the mutual interaction of just the two molecules of each dimer. The E pol energy does not include the polarization contributions from the surrounding molecules in the crystal. Hence, the polarization energy contributes significantly to the observed deviations in the energy values from the PIXEL and multipole methods. Further, the small differences in the E es contribution from the experimental and theoretical ED models is due to the multipole populations of atoms in the multipole model. The electrostatic energy calculation in the EP/MM method depends on the multipole parameters of the ED model (Volkov et al., 2004). The slightly different multipole populations result in a deviation of estimated electrostatic energy in the ED model. Similarly, a detailed analysis of the discrepancy in estimating lattice energies from different Table 3 Lattice energy and intermolecular interaction energies of selected molecular dimers (kJ mol À1 ) in rubrene obtained using the ED models in XD2006. multipole refinement models and thermal motion analysis for H atoms was recently reported in a study of sulfathiazole polymorphs (Sovago et al., 2014). In this study, there are no significant deviations in the calculated energies from both the experimental and theoretical ED models at 100 K and 20 K using the EP/MM method. It is important to note that the entropic effects on energetics of intermolecular interactions are not accounted for in the comparison of energetics calculated using the 100 K and 20 K ED models. Further, the use of different schemes for the estimation of dispersion energy in PIXEL and XD methods also contributes to differences in the obtained lattice and interaction energies. To study the effect of the hydrogen modeling on the interaction energy of dimers, we refined a 100 K model using the ADPs of hydrogen obtained from the SHADE2 webserver (Madsen, 2006), i.e. an identical approach as the 20 K ED model. However, this did not change the interaction energy values of dimers significantly.</p>
<p>The electron density, (r), between interacting atoms can be determined by the experimental measurements and theoretical calculations. The reduced density gradient [RDG = r j j=2ð3 2 Þ 1=3 4=3 , a dimensionless quantity, is derived using the ED and its first derivative in real space (Johnson et al., 2010;Saleh et al., 2012). Low ED and low RDG values normally correspond to the non-covalent interactions (NCIs) between two interacting atoms. The NCI descriptor based on sign( 2 )(r) is useful to characterize NCIs at each RDG isosurface point, where 2 is the second largest eigenvalue of the ED Hessian matrix. The mapping of the quantity, sign( 2 )(r) on RDG isosurfaces can distinguish stabilizing [sign( 2 )(r) &lt; 0] and destabilizing [sign( 2 )(r) &gt; 0] interactions. Here, they have been visualized by plotting the RDG isosurfaces using the <rs id="a12971552" type="software">MolIso</rs> program <rs id="a12971553" type="bibr">(Hubschle &amp; Luger, 2006)</rs> see in Fig. 4. Red isosurfaces correspond to stabilizing interactions and blue isosurfaces represent steric repulsive interaction regions. The C Á Á ÁC stacking interactions [d = 3.708 (1) A ˚at 100 K] between adjacent rubrene molecules is clearly seen as the isosurfaces filling the interlayer spaces between tetracene backbones in Fig. 4(a) obtained from the experimental ED model at 100 K. The observation of low density and low RDG isosurfaces between the stacked aromatic rings indicate the overall balance of steric (destabilizing) and dispersive (stabilizing) contributions (Johnson et al., 2010;Saleh et al., 2012). The red and green regions on the NCI isosurfaces in Fig. 4(a) correspond to negative sign( 2 )(r) values, which indicate the presence of stabilizing contributions provided by C Á Á ÁC interactions between the tetracene backbones. This is also supported by the QTAIM analysis where topological properties correspond to closed shell van der Waals interactions with significant contributions to the total interaction energy of molecular pairs connected by C Á Á ÁC interactions coming from the dispersion energy (Table 1, dimer I in Tables 2 and3). Additionally, the large surface area of the NCI isosurface corresponds to the delo-calized nature of C Á Á ÁC stacking interactions. The NCI surfaces were related to the b.c.p.s of interaction obtained by the QTAIM analysis in the literature (Lane et al., 2013;Saleh et al., 2012) to obtain a global description of chemical bonding by the NCI analysis. In Fig. 4(a), the red and green isosurface corresponds to the regions surrounding the b.c.p. of C Á Á ÁC stacking interaction, whereas the blue isosurface coincides with ring critical points of the aromatic ring. The nature of Á Á Á interactions has been explored by a large number of experimental and theoretical studies (Hunter &amp; Sanders, 1990;Hunter et al., 2001;Meyer et al., 2003;Sinnokrot et al., 2002). Hunter &amp; Sanders (1990) proposed a model to describe the nature of Á Á Á interactions in porphyrin where the Á Á Á interactions was favourable due tointeractions that overcomerepulsions. However, when a large surface area is available for stacking interactions, van der Waals interactions and desolvation contributions are also very important in the stability of the Á Á Á interactions (Meyer et al., 2003). The red and green regions in the isosurfaces of the Á Á Á interaction (Fig. 4a) indicate that the contribution of stabilizing interactions surpasses those of destabilizing interactions in the stacking. This stacking interaction is expected to improve charge transport properties along the stacked aromatic layers as confirmed by good OFET characteristics observed in the direction of stacking layers. The pivotal role of aromatic Á Á Á interactions in the formation of a molecular bridge between adjacent molecules has been experimentally demonstrated by molecular conductance measurements in oligo-phenylene ethynylene-monothiol molecules (Wu et al., 2008), conjugated polymer poly(3-hexylthiophene) (Sirringhaus et al., 1999) and phenylene vinylene derivatives (Seferos et al., 2005). The strongcoupling through intermolecular Á Á Á interactions between molecular junctions is responsible for the efficient charge transport across the molecular junction and it was successfully demonstrated by the measurement of charge transport properties across the molecular junction (Wu et al., 2008;Sirringhaus et al., 1999;Seferos et al., 2005). Furthermore, the confirmation of Á Á Á interactions from the NCI analysis correlate well with the topological properties obtained by the QTAIM analysis. Additionally, it supports earlier theoretical predictions (Wen et al., 2009;Kobayashi et al., 2013;Stehr et al., 2011) and single-crystal FET characteristics (Podzorov et al., 2004), where an anisotropic charge transfer was observed for rubrene with the highest charge mobility along the crystallographic b axis due to the stacking interactions between the tetracene backbones. RDG isosurfaces for the C4Á Á ÁH5 interaction are highlighted in Fig. 4(b). The green RDG isosurface is localized between the C and H atoms demonstrate the presence of CÁ Á ÁH interactions in the crystal packing. A green oblate RDG isosurface in Fig. 4(c) is found for the homopolar C8-H8Á Á ÁH8-C8 interaction (perpendicular to the C Á Á ÁC stacking interactions). The two smaller isosurface discs seen close to the larger RDG isosurface of the H8-H8 bonding correspond to subtle C9-H9Á Á ÁH8-C8 interactions [2.745 (2) A ˚] in Fig. 4(c). This distance is much longer than the sum of the van der Waals radii of H-H bonding (2.4 A ˚) and has much smaller contri-bution to the crystal packing. These observations are also supported by interaction energy calculations using the PIXEL and multipole methods (see above). The NCI analysis complements the QTAIM analysis. In the latter approach, only a limited number of bcps corresponding to CÁ Á ÁC interactions was found, while in the NCI, the low values of the RDG indicate significant intermolecular interactions (Fig. 4a).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f81527245"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T14:19+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>An alignment was made of the amino acid sequence of MtLPMO9A and the amino acid sequence of PMO1 from Thielavia terrestris, which scored highest in a <rs id="a12967888" type="software">Blast</rs> search using the MtLPMO9A sequence against the Protein Data Bank (75% amino acid identity). Using this alignment and the available structure of TtPMO1 (PDBid: 3eii) as template, structural models were obtained for
MtLPMO9A using the <rs id="a12967889" type="software">Modeller</rs> program version <rs id="a12967890" type="version" corresp="#a12967889">9.14</rs> <rs id="a12967891" type="bibr">[43]</rs>. Thirty comparative models were generated, after which the model with the lowest corresponding DOPE score [44] was selected for image generation using <rs id="a12967892" type="software">Pymol</rs> ( <rs id="a12967903" type="software">Pymol, The
PyMOL Molecular Graphics System</rs>, Version <rs id="a12967895" type="version" corresp="#a12967903">1.5.0.4</rs> <rs id="a12967896" type="publisher" corresp="#a12967903">Schrödinger, LLC</rs>, New York, NY, USA).</p>
<p>For matrix-assisted laser desorption ionization-time of flight mass spectrometry (MALDI-TOF MS), an Ultraflex workstation using <rs id="a12967897" type="software">FlexControl</rs> <rs id="a12967898" type="version" corresp="#a12967897">3.3</rs> ( <rs id="a12967899" type="publisher" corresp="#a12967897">Bruker Daltonics</rs>) equipped with a nitrogen laser of 337 nm was used. The pulsed ion extraction was set on 80 ns. Ions were accelerated to a kinetic energy of 25 kV and detected in positive reflector mode with a set reflector voltage of 26 kV. The lowest laser energy required was used to obtain a good signal-to-noise ratio. A total of 200 spectra were collected for each measurement. The mass spectrometer was calibrated using a mixture of maltodextrins (Avebe, Veendam, The Netherlands) in a mass range (m/z) of 500-2,500. The peak spectra were processed by using <rs id="a12967900" type="software">FlexAnalysis</rs> software version <rs id="a12967901" type="version" corresp="#a12967900">3.3</rs> ( <rs id="a12967902" type="publisher" corresp="#a12967900">Bruker Daltonics</rs>). Prior to analysis, samples were desalted by adding AG 50 W-X8 Resin (Bio-Rad Laboratories). To obtain lithium (Li) adducts, the supernatant was dried under nitrogen and re-suspended in 20 mM LiCl [28]. Each lithium-enriched sample of a volume of 1 µL was mixed with 1 µL of matrix solution (12 mg mL -1 2,5-dihydroxy-benzoic acid (Bruker Daltonics) in 30% (v/v) acetonitrile in H 2 O), applied on an MTP 384 massive target plate (Bruker Daltonics) and dried under a stream of warm air.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f337890524"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:42+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The COVID-19 pandemic has challenged researchers and policy makers to identify public safety measures forpreventing the collapse of healthcare systems and reducingdeaths. This narrative review summarizes the available evidence on the impact of social distancing measures on the epidemic and discusses the implementation of these measures in Brazil. Articles on the effect of social distancing on COVID-19 were selected from the <rs id="a12953651" type="software">PubMed</rs>, <rs id="a12953652" type="software">medRXiv</rs> and <rs id="a12953655" type="software">bioRvix</rs> databases. Federal and state legislation was analyzed to summarize the strategies implemented in Brazil. Social distancing measures adopted by the population appear effective, particularly when implemented in conjunction with the isolation of cases and quarantining of contacts. Therefore, social distancing measures, and social protection policies to guarantee the sustainability of these measures, should be implemented. To control COVID-19 in Brazil, it is also crucial that epidemiological monitoring is strengthened at all three levels of the Brazilian National Health System (SUS). This includes evaluating and usingsupplementary indicators to monitor the progression of the pandemic and the effect of the control measures, increasing testing capacity, and making disaggregated notificationsand testing resultstransparentand broadly available.</p>
<p>A total of 2,771 articles on COVID-19, published up to April 6, 2020 and listed in the <rs id="a12953653" type="software">PubMed</rs> databases, were screened for inclusion in this narrative review. In addition, manuscripts in the prepublication phase and available in the <rs id="a12953654" type="software">medRXiv</rs> and <rs id="a12953656" type="software">bioRvix</rs> databases or in the grey literature were also reviewed. Due to the speed of publication at the present time, articles published after the cut-off date but of the utmost relevance for Brazil were included in this review a posteriori.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f201073686"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:00+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>There are now many bibliographic management packages available and many factors to consider when choosing the product that best meets the needs of the individual user or institution. Popular tools include <rs id="a12951399" type="software">RefWorks</rs>, <rs id="a12951400" type="software">End-Note</rs>, <rs id="a12951401" type="software">Zotero</rs>, <rs id="a12951402" type="software">Mendeley</rs>, and <rs id="a12951403" type="software">F1000 Workspace</rs>. This review will cover the first four; <rs id="a12951404" type="software">F1000 Workspace</rs> was reviewed in the Journal of the Medical Library Association (JMLA) in 2017 [2].</p>
<p>First released in 1988 <rs id="a12952229" type="bibr">[3]</rs>, <rs id="a12951405" type="software">End-Note</rs> is a commercial product that is primarily marketed via sales of its desktop application (currently version <rs id="a12951406" type="version" corresp="#a12951405">X8</rs>). A basic online version is free, but it has limited features and functionality. <rs id="a12951407" type="software">RefWorks</rs>, first released in 2001 <rs id="a12952228" type="bibr">[4]</rs>, is an entirely web-based application marketed to libraries as an institution-wide tool, though a vendor representative indicated that individual accounts used to be available and will be offered again [5]. The product is currently transitioning to a new interface, referred to by the vendor as "new <rs id="a12951408" type="software">RefWorks</rs>." <rs id="a12951522" type="software">Zotero</rs>'s free, open source citation manager was initially introduced in 2006 as an extension for the <rs id="a12951495" type="software">Firefox</rs> web browser. It is now available as a standalone application [6]. First released in 2008 [7], <rs id="a12951409" type="software">Mendeley</rs> is a free cloud-based citation manager with desktop and online versions. It also serves as an academic research network, offering a variety of social networking features.</p>
<p>The remainder of this review focuses on how these products differ with respect to the most commonly used features of citation managers and the advantages and disadvantages of each product. Table 1 summarizes key differences between the products. This review is based primarily on current desktop versions (if applicable) of the products as of February 2018, though online versions are discussed as needed to provide a complete picture of a tool's functionality. For <rs id="a12951410" type="software">Mendeley</rs>, this review covers the free version only. For <rs id="a12951496" type="software">RefWorks</rs>, this review covers the new <rs id="a12951411" type="software">RefWorks</rs> only; it does not address the older version, known as <rs id="a12951497" type="software">Legacy RefWorks</rs>.</p>
<p>Users can search within databases, mark references to save or export, and select from a variety of options to add references to their preferred citation manager tools. Choosing a direct export option opens any of these tools that are installed on users' computers, and references can be added with one mouse-click. Each of the products has direct export options for at least one of the following databases: PubMed, Web of Science, Science Direct, EBSCO (CINAHL), and ProQuest (PsycINFO). All four systems allow direct export of records from EB-SCO (CINAHL), while <rs id="a12951413" type="software">EndNote</rs> is the only tool that has a direct export option for PubMed.</p>
<p>Users can also use the browser add-ons to automatically import references into their reference collections. The add-ons for <rs id="a12951414" type="software">Mendeley</rs>, <rs id="a12951415" type="software">RefWorks</rs>, and <rs id="a12951416" type="software">Zotero</rs> allow users to import references to their reference collections from multiple databases. Depending on the database, users can select individual references or batches, and the references and associated PDFs are imported. <rs id="a12951417" type="software">Mendeley</rs> <rs id="a12951505" type="software" subtype="environment">EndNote</rs>'s <rs id="a12951506" type="software" subtype="component" corresp="#a12951505">Capture Reference</rs> bookmarklet has more limited functionality than the browser addons for the other three products. When displaying a list of PubMed search results, <rs id="a12951507" type="software">Capture Reference</rs> only imported all references on the page; it did not allow us to select specific references to import. <rs id="a12951508" type="software">Capture Reference</rs> did not work at all for us with a list of results from Google Scholar. The only way to import these results was to open each one and then capture it. It also did not directly capture bibliographic information about web pages as easily as the other add-ons did. When we attempted to import information about a web page using <rs id="a12951510" type="software" subtype="component" corresp="#a12951511">Capture Reference</rs>, it created an RIS file that we then had to import into <rs id="a12951511" type="software" subtype="environment">EndNote</rs>, whereas the other three add-ons added information about web pages directly.</p>
<p>The tools also offer several other ways to add references. <rs id="a12951419" type="software">Mendeley</rs> users can add references by entering a PubMed ID (PMID), digital object (DOI), or ArXivID. Similarly, Zotero users can add references using the international standard book number (ISBN), DOI, or PMID. In the online version of <rs id="a12951420" type="software">Mendeley</rs>, users can search and import references from <rs id="a12951421" type="software">Mendeley</rs>'s web catalog, a collection of all the references that have been added to the personal libraries of <rs id="a12951422" type="software">Mendeley</rs> users [8]. <rs id="a12951423" type="software">EndNote</rs> and <rs id="a12951424" type="software">RefWorks</rs> also allow users to search databases and library catalogs from within the application and import selected search results. <rs id="a12951425" type="software">EndNote</rs> offers an extensive list of free and commercial databases for searching. As of this writing, the new <rs id="a12951426" type="software">RefWorks</rs> only offers PubMed and the Library of Congress as search options, and, when tested, neither search option was functional. According to the <rs id="a12951427" type="software">RefWorks</rs> lead product manager, institutional account administrators can allow users to search any database that is accessible via the Z39.50 search standard. He also indicated that <rs id="a12951520" type="software">ProQuest</rs> is building application programming interfaces (APIs) to integrate <rs id="a12951428" type="software">RefWorks</rs> with other <rs id="a12951519" type="software" subtype="environment">ProQuest</rs> tools such as <rs id="a12951518" type="software" subtype="component" corresp="#a12951519">Summon</rs> and <rs id="a12951517" type="software" subtype="component" corresp="#a12951519">Primo</rs>, which should increase in-app search options [5].</p>
<p>All four applications allow users to create standalone bibliographies in virtually any word processor, including Google Docs. With <rs id="a12951430" type="software">End-Note</rs>, users can create a standalone bibliography by selecting citations and an output style, and copying and pasting into a word processor document. <rs id="a12951431" type="software">EndNote</rs> also allows users to create a subject bibliography that is based on one or more keywords in users' citations. Both <rs id="a12951432" type="software">Mendeley</rs> and <rs id="a12951433" type="software">Zotero</rs> allow users to drag references from the desktop client into a word processor, where they will be formatted according to the style that users have selected, the quickest and most user-friendly method of bibliography creation. <rs id="a12951434" type="software">RefWorks</rs> includes a feature that allows users to generate a bibliography from a batch of references in a folder, but that feature did not work when we tested it, leaving no way to generate standalone bibliographies from citations.</p>
<p>More commonly, users create bibliographies from in-text citations in a manuscript. All four tools offer <rs id="a12951435" type="publisher" corresp="#a12951436">Microsoft</rs> <rs id="a12951436" type="software">Word</rs> plug-ins to support this functionality. Table 1 provides details about which tools work with other word processors. In <rs id="a12951437" type="software">EndNote</rs>, the bibliography is automatically generated as the citations are inserted into the document. In <rs id="a12951438" type="software">Mendeley</rs>, <rs id="a12951439" type="software">RefWorks</rs>, and <rs id="a12951440" type="software">Zotero</rs>, inserting a citation and creating a bibliography are separate steps, and at least one citation must be added to the document in order to create a bibliography. All four products made occasional small errors in citations, especially when we cited web pages, but <rs id="a12951441" type="software">Mendeley</rs> performed especially poorly, omitting key information from web page citations, such as date accessed.</p>
<p>Each tool offers different options for adding PDF documents. All four systems allow users to add PDF documents by dragging and dropping them into their reference collections and by attaching them to existing citations. <rs id="a12951442" type="software">EndNote</rs> and <rs id="a12951443" type="software">Mendeley</rs> users can drag and drop PDFs both individually and in folders. <rs id="a12951444" type="software">RefWorks</rs> users can only add PDFs one at a time, while <rs id="a12951445" type="software">Zotero</rs> users can add multiple PDFs at once. <rs id="a12951446" type="software">Mendeley</rs> users can also add PDFs by putting them in a designated folder called a Watch Folder. <rs id="a12951447" type="software">Mendeley</rs> monitors the contents of these folders and automatically adds any PDFs to reference collections.</p>
<p>All four products can generate metadata from PDFs to create a citation record, but they use somewhat different methods to do so. When we tested articles from three different journals, all four products extracted metadata inconsistently and occasionally inaccurately. For example, one product extracted metadata completely for a given article, while another failed to extract key information (e.g. author name, page numbers) from the same PDF, and a third failed to import any metadata from the PDF. All products exhibited these failures, though <rs id="a12951498" type="software">RefWorks</rs> appeared to be the least accurate, with at least one significant error with each of the three PDFs that we tested.</p>
<p>All of the products, other than <rs id="a12952223" type="software">Zotero</rs>, support PDF annotation in the application. <rs id="a12952224" type="software">Zotero</rs> users can open PDFs in the application of their choice, annotate them, and save them back to the <rs id="a12952225" type="software">Zotero</rs> database. An add-on called Zotfile [9] allows users to extract annotations and perform other PDF management tasks.</p>
<p><rs id="a12951512" type="software">EndNote</rs> and <rs id="a12951513" type="software">Zotero</rs> can use an openURL link resolver to help users retrieve full text from a library's electronic collections. Users can specify the baseURL of their libraries' link resolver in the product settings, and the products will use metadata from a citation in their libraries to attempt to locate full text for that item. In <rs id="a12951514" type="software">Zotero</rs>, this feature is called Library Lookup. Users click on a reference in their collections, and if full text is found, the PDF file can be easily dragged and dropped into their reference collections. <rs id="a12951515" type="software">EndNote</rs> users can access full-text through their institutions by using the Find Full Text feature. <rs id="a12951448" type="software">Mendeley</rs> used to allow integration with a library's link resolver but no longer offers this feature [8]. For <rs id="a12951499" type="software">RefWorks</rs>, institutional administrators can configure a link resolver for all users at that institution.</p>
<p>According to <rs id="a12951449" type="software">RefWorks</rs> documentation, <rs id="a12951450" type="software">RefWorks</rs> users can only share collections with users at their own institutions [10]. The <rs id="a12951451" type="software">Ref-Works</rs> senior product manager indicated, however, that as of fall 2017, <rs id="a12951452" type="software">RefWorks</rs> users can share folders with other <rs id="a12951453" type="software">RefWorks</rs> users across institutions [5]. <rs id="a12951454" type="software">EndNote</rs> <rs id="a12951455" type="version" corresp="#a12951454">X7</rs> and <rs id="a12951456" type="version" corresp="#a12951454">X8</rs> users can share with each other in groups of up to 100 members [11]. <rs id="a12951457" type="software">Mendeley</rs> and <rs id="a12951458" type="software">Zotero</rs> users can create both public and private groups [12,13], though <rs id="a12951459" type="software">Mendeley</rs> users with a free account can create and own only one private group, and private groups created by free accounts are limited to three members [14]. <rs id="a12951460" type="software">Mendeley</rs> offers additional social networking features in the online version that the other products do not provide. <rs id="a12951461" type="software">Mendeley</rs> users can search for and follow other researchers with similar interests and receive updates on actions and events of researchers they are following via the <rs id="a12951462" type="software">Mendeley</rs> Newsfeed [12].</p>
<p><rs id="a12951463" type="software">EndNote</rs>, <rs id="a12951464" type="software">Mendeley</rs>, and <rs id="a12951465" type="software">Zotero</rs> collections and documents are stored locally and, therefore, available offline. <rs id="a12951466" type="software">RefWorks</rs> is a purely cloud-based system, so access to the application itself is not available offline. Users can, however, link a DropBox account to <rs id="a12951468" type="software">RefWorks</rs> to provide offline access to full-text documents in <rs id="a12951469" type="software">RefWorks</rs> <rs id="a12952227" type="bibr">[10]</rs>.</p>
<p>Of the four products, EndNote is the only one that offers a journal matching feature, known as Manuscript Matcher, to help users find the right journal for their manuscripts. Users of the online version can provide their article titles, abstracts, and references, and <rs id="a12951471" type="software">End-Note</rs> will provide a list of journal recommendations based on its analysis of Web of Science citation data <rs id="a12952226" type="bibr">[15]</rs>. <rs id="a12951474" type="software">RefWorks</rs> is the only product to offer a plug-in for
Google Docs, an especially useful feature at universities where Google tools are used heavily by students. It is also the only fully cloud-based product. While both <rs id="a12951478" type="software">Mendeley</rs> and <rs id="a12951479" type="software">Zotero</rs> are free, <rs id="a12951480" type="software">Zotero</rs> is the only open-source product among the four. Its source code is hosted on GitHub and freely available under an AGPLv3 license [16].</p>
<p>All four of the tools reviewed here are usable for standard reference manager functions, and each has strengths and weaknesses. For example, in our testing, <rs id="a12951481" type="software">Zotero</rs>'s browser add-on was the easiest to use and captured data more accu-rately than the other add-ons did. <rs id="a12951482" type="software">EndNote</rs> offered the most choices for searching databases within the tool, and <rs id="a12951483" type="software">Zotero</rs> generated the most accurate bibliographies. Each also offers unique features that may be especially valuable to certain populations (e.g., <rs id="a12951484" type="software">RefWorks</rs>' integration with <rs id="a12951485" type="software">Google Docs</rs>, <rs id="a12951486" type="software">Mendeley</rs>'s social networking functions). Often, though, the best choice for a given purpose may be determined by factors other than the functionality of the applications themselves. These factors include cost, support provided by institutions, research needs, familiarity with a product from previous experience, and accessibility for the research team members. For example, if users are working on a systematic review with authors at several institutions, they will need to choose a tool that is accessible to everyone on the team. Since users are not limited to the citation managers supported by their institutions, information professionals need to be familiar with all popular choices in order to guide and support their users effectively.</p>
<p>All four products offer plug-ins for <rs id="a12951487" type="publisher" corresp="#a12951488">Microsoft</rs> <rs id="a12951488" type="software">Word</rs>. <rs id="a12951489" type="software">EndNote</rs>, <rs id="a12951490" type="software">Mendeley</rs>, and <rs id="a12951491" type="software">Zotero</rs> offer desktop clients, while <rs id="a12951492" type="software">RefWorks</rs> is entirely web-based. Table</p>
<p>shows platforms and browser compatibility. All four products offer a web-based version that works with recent versions of popular browsers. Some tools offer plug-ins for other browsers as well, and all offer browser add-ons (bookmarklets, extensions, etc.) for importing bibliographic information from web pages. The <rs id="a12951493" type="software">Mendeley</rs> browser add-on functions only with the online version of <rs id="a12951494" type="software">Mendeley</rs>; the <rs id="a12951521" type="software">Zotero</rs> add-on requires the desktop version for full functionality; and the <rs id="a12951516" type="software">EndNote</rs> add-on can be used in the desktop and online versions.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f492594832"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:22+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Random forests take a 'divide and conquer' approach by combining outcomes from a sequence of regression decision trees, and are particularly popular as they have few parameters to tune, are robust to over-fitting and are robust to small and large datasets [43,44]. Accuracy is measured by 'out of bag error' which estimates error computed on reference data set aside prior to building the random forest model. In this study, 25% of the reference data are set aside for validation. We utilise a GPU accelerated <rs id="a12973011" type="software" corresp="#a12973004">random forest</rs> implementation in <rs id="a12973004" type="software" subtype="environment">python</rs> which shows enhanced computational efficiency compared to CPU equivalents [45].</p>
<p>Additional predictor variables for the machine learning model were created by applying several image processing <rs id="a12973012" type="software" subtype="implicit" corresp="#a12973005">tools</rs> implemented in <rs id="a12973005" type="software" subtype="environment">Whitebox-Tools</rs> <rs id="a12973006" type="bibr">[57]</rs>. These filters were applied to both the COPDEM30 elevations and the tree height data, to detect features such as edges, anomalies or variability that might be due to forests. For example, a sobel edge detector filter with a 3 × 3 window was applied, and an unsharp filter was applied to emphasize edges, while reducing noise. Filters computing the difference between gaussian filters of varying sizes were also computed. Lastly, a bilateral edge-preserving smoothing filter and gaussian filters to emphasize long-range variability were applied to the forest height variable only.</p>
<p>Data on building footprints and density from <rs id="a12973007" type="software">OpenStreetMap</rs> were not used due to the lack of global consistency <rs id="a12973008" type="bibr">[63]</rs> and the low importance of the variable(s) in other location specific random forest based DEM correction studies [35,39]. Log transformations were applied to population, GDP per area and travel times. As for the forest correction, filters (difference of gaussians, sobel and unsharp) were applied, but in this case to detect edges of urban areas or taller buildings in COPDEM30. The number of samples per LiDAR country for building removal are detailed in table S2.</p>
<p>Additional steps were applied to correct areas that had been corrected too much, or areas where pixels were noisy. Firstly, to remove artifacts outside building and forest corrected areas, pits up to 4 pixels in size were filled. In building and forest removed areas, large depressions were filled, but not higher than the original COPDEM30 (after filling small pits in the COPDEM30 of up to 100 pixels in size). Only pixels that had been adjusted (i.e. Building and Forested) were filled. Noise in the DEM was subsequently reduced by running an adaptive filter twice, and then a bilateral filter. An adaptive filter is effective at removing speckle noise [64], and was used in MERIT DEM [15]. A bilateral filter is an edge preserving smoothing filter [65] that preserves edges of features but reduces short-scale variation. These filters were implemented in <rs id="a12973009" type="software">WhiteBoxTools</rs> <rs id="a12973010" type="bibr">[57]</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f326253321"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:46+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>For model building, the Training data were transformed into a time series with frequency seven. Trend, daily effects and weather effects were removed by applying separate linear regressions for each effect, and the coefficients used in the models. A piecewise linear model approach was adopted (Chatfield 2016) which allowed trends to be analyzed sequentially in order to mitigate breaks in data, changes to reporting, and other changes in trend. Hence, while seasonality and weather effects relate to the entire reporting period, trend information was used in a more targeted manner so that expected/forecast values contained the appropriate trends. Potential weather effects were represented by the maximum daily temperature and amount of rainfall (mm) (Historical Weather data 2020). Interaction affects within weather and day of the week were not statistically significant, which meant that the simpler models described below were adopted. The resulting residuals from the detrended and deseasoned models were analysed with ARIMA time series models, appropriate models then selected using an <rs id="a12893757" type="software" subtype="implicit" corresp="#a12893758">automated function</rs> in <rs id="a12893758" type="software" subtype="environment">R</rs> <rs id="a12953649" type="bibr">(Hyndman et al. 2020)</rs>. The <rs id="a12893760" type="software" subtype="implicit">automated function</rs> used a <rs id="a12893761" type="software" subtype="implicit" corresp="#a12893760">step-wise selection algorithm</rs> <rs id="a12953650" type="bibr">(Hyndman and Khandakar 2008)</rs> based upon Akaike information criterion (AIC) to iteratively determine the coefficients for the ARIMA model which best explained the deseasoned and detrended residuals. The resulting ARIMA model was used, along with the daily effect, the weather effect and trend, to produce a forecast value for each crime type with an associated 95% confidence interval. 6 Hence the general equation for the crime rate was: where X t is the Crime rate, µ(t) is the trend, s(t) is the daily effect, w(t) is the weather effect and ǫ t are the ran- dom errors. All at time t.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f543782712"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:48+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Of course, other fields have also tackled the issue of causal inference with longitudinal data. For example, the sociologists Elwert and Pfeffer (2019) developed an approach that uses future values of the independent variable to detect and reduce omitted variable bias. In epidemiology, the particularly promising approach of marginal-structural models (Cole &amp; Hernán, 2008;Robins et al., 2000) has been developed. These models implement a multistep estimation procedure to control for time-varying confounding variables (Williamson &amp; Ravani, 2017). The promise of such models for causal inference in psychology, however, has not yet been well recognized (Lüdtke &amp; Robitzsch, 2020;Usami, 2020). Thoemmes and Ong (2016) provided an introduction to marginal-structural models in combination with inverse probability weighting as a means for third-variable adjustment in longitudinal data, including annotated <rs id="a12894672" type="software">SPSS</rs> and <rs id="a12894673" type="software" subtype="environment">R</rs> <rs id="a12894674" type="software" subtype="implicit" corresp="#a12894673">code</rs> for psychologists. The tutorial by Bray et al. (2006) showcased an implementation in <rs id="a12894675" type="software">SAS</rs> and highlighted how this method can, unlike other common methods, successfully adjust for time-varying confounders. Finally, VanderWeele et al. ( 2020) developed a comprehensive template for so-called outcome-wide longitudinal designs in which the goal is to identify the The fixed-effects approach (or alternatively, withinpersons mean centering) can control for unobserved time-invariant confounders whose effects do not change over time. For example, when considering the effects of talkativeness on happiness, extraversion (a stable personality trait) may be such a confounder: Extraverted individuals are habitually more talkative, but extraverted individuals may also simply be dispositionally happier. Figure 1, which has been adapted from (Hamaker &amp; Muthén, 2020, p. 367), shows the causal model underlying the standard fixedeffects model. Note that this model focuses only on the (contemporaneous) effects of X on Y, and X is treated as exogenous (i.e., the model does not impose constraints on the causal relationship between X variables and U).</p>
<p>The general framework for using models to make predictions about the effects of various interventions are so-called marginal effects. Marginal effects have received comparatively little attention in psychology outside of methods journals; they are more common in, for example, sociology (e.g., Mize et al., 2019), possibly because the statistical software <rs id="a12894676" type="software">Stata</rs>, popular in that field, makes calculating them quite easy <rs id="a12953671" type="bibr">(Williams, 2012)</rs>. However, there are now <rs id="a12894678" type="software" subtype="implicit" corresp="#a12894679">packages</rs> available that allow researchers to calculate marginal effects in <rs id="a12894679" type="software" subtype="environment">R</rs> using structural equation models (<rs id="a12894680" type="bibr">Mayer, 2019</rs>;<rs id="a12894681" type="bibr">Mayer et al., 2016</rs>) and a vast number of other model classes, including multilevel models (Arel-Bundock, 2022;Lenth, 2022). As far as we know, no comprehensive primer to marginal effects for psychologists has been published to date, but recent blog posts have tried to provide a gentle introduction (Heiss, 2022;Rohrer, 2022).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f333903624"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:47+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Weibull distributions were fitted to the summary data from each study, using Nelder-Mead optimisation (implemented in the <rs id="a12894640" type="software" subtype="component" corresp="#a12894641">stats</rs> package in <rs id="a12894641" type="software" subtype="environment">R</rs> [<rs id="a12894642" type="bibr">24</rs>]) for those reporting medians and IQRs. Specifically, the shape and scale parameters were varied in order to minimise the squared distance between the distribution and study quantiles. Where estimates were presented as a mean, x, and standard deviation, s, the distribution was fitted by momentmatching using the <rs id="a12894643" type="software">mixdist</rs> package [<rs id="a12894644" type="bibr">25</rs>]. The same approach was also tested using gamma distributions, but Weibull was marginally preferred with respect to total squared error in the fitted quantiles. These distributions were then discretised using the <rs id="a12894645" type="software" subtype="component" corresp="#a12894646">distcrete</rs> package in <rs id="a12894646" type="software" subtype="environment">R</rs> [<rs id="a12894647" type="bibr">26</rs>]. A total of 100,000 samples were then drawn from each of these distributions, with weighting according to their sample size. Specifically, the study distributions were first sampled according to a multinomial distribution defined by the studies' relative sample sizes, and LoS was then sampled from each of these sampled distributions. Due to potential important differences in the characteristics of each study population, it may not be appropriate to weight entirely on sample size without considering how representative the cohort is of the general population. Therefore, as a sensitivity analysis, we performed the same analysis without weighting in order to understand how much this influences the distribution. In some cases, studies reported LoS according to some stratification and not over the whole study population. Here, we applied the same method to summarise across the strata and obtain an estimated median and IQR across the whole population, validating the approach using examples where the overall summary statistics were also provided.</p>
<p>All analyses were performed using <rs id="a12894648" type="software">R</rs> version <rs id="a12894649" type="version" corresp="#a12894648">3.6.3</rs> (29 February 2020).</p>
<p>Having been the first country to observe this novel coronavirus, published data on COVID-19 patient outcomes in China is more widely available than from countries to which the epidemic spread later on. The set of studies found in this review reflects this bias towards evidence obtained from China, particularly Wuhan. The small number of studies identified from outside of China means it is difficult to interpret comparisons across settings. Several studies have been published after our search dates which provide additional LoS estimates from outside of China. A study of 5700 patients from hospitals in the New York area reported comparable estimates for total LoS (median 4.5; IQR 2.4-8.1) [51]; however, studies from Northern Italy [52], Japan [53], and California and Washington [50] reported longer estimates of LoS. Therefore, the total LoS outside of China may in fact be longer than what we concluded. <rs id="a12894650" type="software" subtype="implicit">Our code</rs> is freely available on <rs id="a12894651" type="software">github</rs>, and additional studies may easily be added. As more studies emerge from a broader range of settings, it would be important to re-evaluate LoS estimates, as there are likely to be between-country differences that we have not captured here. Furthermore, a number of studies include patients from the same hospital over the same period, for example, Yang et al. [54] and Wu et al. [55] who both reported patients from Jin Yin-Tan hospital in Wuhan, and it is possible that these studies had overlapping study populations. Furthermore, Guan et al. [36] was a national study conducted in China and ISARIC [27] included 25 countries worldwide; therefore, these studies may also include patients previously described. The effect of this double-counting would be to bias the overall summary statistics towards the LoS from these settings, and potentially reduce the total variation. Although this is acknowledged as an issue, it was not considered as a basis for exclusion since any criteria for selecting one study from the overlapping group would have been arbitrary and potentially induce another source of bias in itself. Therefore, we instead chose to conduct a sensitivity analysis based on a straightforward selection criteria of largest sample size from those studies with any potential for overlap, and found little difference between the estimates of LoS whether overlapping studies were included or excluded. The overall benefit of inclusion, particularly as many of these studies reported LoS for different subgroups, was deemed to outweigh the potential bias which may arise as a result of overlapping patient populations.</p>
<p>As far as the authors are aware, the approach demonstrated here to summarise median and IQRs across multiple studies has not been proposed before, although there are similarities with the approach taken by others in the CMMID Working Group to pool R 0 estimates [59]. We present an intuitive method which exploits two optimisation methods to fit parametric distributions based on reported summary statistics rather than individual data, then samples across them. In this way, we capture the central tendency and overall variation between a set of quantiles from different study populations. This allows multiple sources of evidence to be consolidated into a single distribution which can be used in bed forecasting going forward. By providing both the <rs id="a12894652" type="software" subtype="implicit">code</rs> for this analysis and our summary distributions, better bed occupancy predictions can be made in the future.</p>
<p>The data and <rs id="a12894653" type="software" subtype="implicit">code</rs> used in this work can be accessed at <rs id="a12894654" type="url" corresp="#a12894653">https://github.com/ esnightingale/los_review</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f566501142"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:44+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The <rs id="a12894895" type="software">TSA</rs> program (<rs id="a12894896" type="publisher" corresp="#a12894895">Copenhagen Trial Unit</rs>, Centre for Clinical Intervention Research, Denmark) provides a simple and useful way to calculate the RIS. The <rs id="a12894897" type="software">TSA</rs> program uses the heterogeneity-adjustment factor (AF) to adjust for heterogeneity among the included trials. AF is calculated as the total variance in a random-effects model divided by the total variance in a fixed-effect model as follows: AF = AF: heterogeneity-adjustment factor V R : total variance in a random-effects model V F : total variance in a fixed-effect model Because the total variance in a random-effects model is greater than or equal to the total variance in a fixed-effect model (V R ≥ V F ), AF is always greater than or equal to 1.</p>
<p>In the <rs id="a12894898" type="software">TSA</rs> program, RIS is automatically calculated by defining the statistical hypotheses, namely information size, Type I error, power, relative risk reduction, incidence in the intervention and control arms, and heterogeneity correction in the Alpha-spending boundaries setting window is activated in TSA tab and displayed in the TSA diagram (Fig. 1). This will be discussed later in TSA tab section.</p>
<p>The <rs id="a12894899" type="software">TSA</rs> program provided statistical monitoring boundaries that show a sensible threshold for statistical significance (alpha spending functions) based on methods developed by Lan and DeMets [21][22][23]. In the <rs id="a12894900" type="software">TSA</rs> program, alpha spending functions are automatically calculated, and statistical monitoring boundaries are displayed in the TSA diagram (Fig. 1). The monitoring boundaries presented in <rs id="a12894901" type="software">TSA</rs> are dependent on the RIS fraction, which was included in the meta-analysis [15]. The lower the number of patients reached compared with RIS, the higher the intervention uncertainty. In contrast, when the closer the number of patients that reach the RIS, the uncertainty decreases. As uncertainty increases, the statistical significance level decreases, and the significance interval widens. Thus, when the fraction of RIS is small, the interval between the monitoring boundaries becomes wider.</p>
<p>Fig. 3A shows that the last point in the z-curve is outside of the conventional test boundary but within the monitoring boundaries. Therefore, we can conclude that there is a statistical difference in the conventional meta-analysis, but we cannot conclude a statistical difference in TSA. When adding a new trial and updated TSA (with adding 79 patients, number of patients included in the TSA increased from 244 [Fig. 3A] to 323 [Fig. 3B or C]), the last point in the z-curve may remain within the monitoring boundaries ('Not Statistically Significant Zone') (Fig. 3B) or outside the monitoring boundaries to reach 'Area of Benefit' (Fig. 3C). Thus, the pooled estimates in TSA may become statistically nonsignificant (Fig. 3B) or significant (Fig. 3C) after the addition of the new trial. In that case, we either conclude that intervention has an effect (Fig. 3C) or further studies are needed as a conclusion could not be derived (Fig. 3B). The construction of the monitoring boundaries in the <rs id="a12894902" type="software">TSA</rs> program will be discussed in TSA tab section.</p>
<p>The possibility of inflating Type II errors also exists for multiple and sequential analyses in meta-analysis. Similar to the alpha spending function, the methods proposed by Lan and DeMets [21][22][23] can be extended to control Type II errors. In the <rs id="a12894903" type="software">TSA</rs> program, futility boundaries are provided using the methodology proposed by Lan and DeMets and reflect the uncertainty of obtaining a chance negative finding in relation to the strength of the available evidence (e.g., the accumulated number of patients).</p>
<p>The <rs id="a12894904" type="software">TSA</rs> program uses the Z-statistic or the Z-value, which is calculated by dividing the log of the pooled intervention effect by its standard error (Fig. 1). Z-statistics are assumed to follow a standard normal distribution, with a mean of 0 and a standard deviation of 1. The larger the absolute value of the Z-value, the larger the probabilities that the two interventions are different, and these differences cannot be explained by chance. As P value is the probability of finding the difference between the observed difference or if the null hypothesis is true, P and Z-values are interchangeable and can be inferred from Z-value (for example, a two-sided P value of 5% represents Z-value of 1.96). Whenever a meta-analysis is updated, the <rs id="a12894905" type="software">TSA</rs> program calculates the corresponding Z-value and then provides a Z-curve that plots the series of consecutive cumulative Z-statistics.</p>
<p>Another approach to adjust the issues of repeated significance testing is to penalize the Z-values by the strength of the available evidence and number of statistical tests. The <rs id="a12894906" type="software">TSA</rs> program uses the law of iterated logarithms for this purpose. The law of the iterated logarithms states that if data are normally distributed, data divided by the logarithm of the logarithm of the number of observations will exist between -2 and 2. This law is utilized to adjust the inflation of Type I errors due to repeated significance testing.</p>
<p>For dichotomous data, the <rs id="a12894907" type="software">TSA</rs> program uses relative risk (RR), risk difference (RD), odds ratio (OR), and Peto's odds ratio as the effect measure for meta-analysis. When events are rare, Peto's odds ratio is the preferred effect measure for meta-analysis.</p>
<p>For continuous data, the <rs id="a12894908" type="software">TSA</rs> program uses mean difference as the effect measure to perform a meta-analysis. However, the <rs id="a12894909" type="software">TSA</rs> program does not support meta-analysis using the standardized mean difference.</p>
<p>The <rs id="a12894910" type="software">TSA</rs> program provides four models to integrate effective sizes: 1) fixed effect model, and random effect models using the 2) DerSimonian-Laird (DL) method, 3) Sidik-Jonkman (SJ) method, and 4) Biggerstaff-Tweedie (BT) method.</p>
<p>The fixed effect model is applied based on the assumption that the treatment effect is the same, and the variance between studies is only due to random errors. Thus, the fixed effect model can be used when the studies are considered homogeneous; namely, the same design, intervention, and methodology are used in the combined studies, and the number of included studies is very small. In contrast, the random effect model assumes that the combined studies are heterogeneous, and the variance between studies is due to random error and between-study variability [5]. The random effect model may be used when the design, intervention, and methodology used in the included studies are different. <rs id="a12894911" type="software">TSA</rs> program provides three different methods to integrate the effect estimate. The DL method is the most commonly used and simplest random effect model and is the only option for <rs id="a12894912" type="software">Review Manager</rs> software ( <rs id="a12894913" type="publisher" corresp="#a12894912">Nordic Cochrane Centre</rs>, Denmark). However, DL method tends to underestimate the between-trial variance. This can be overcome by the SJ method that applies a noniterative estimate of the variance based on re-parametrization [25]. SJ method reduces the risk of Type I error compared with DL method. In a meta-analysis with moderate or substantial heterogeneity, the false positive rate based on the SJ method was estimated to be close to the desired level (conventionally 5%), but the false positive rate based on the DL method increased from 8% to 20% [25]. However, the SJ method has the risk of creating too wide a confidence interval by overestimating the between-trial variance, especially in meta-analyses with mild heterogeneity. BT method incorporates the uncertainty of estimating the between-trial variance and minimizes the effect of the bias via appropriate weighting in large trials, especially when the size of the trials varied and small trials were biased [26].</p>
<p>The <rs id="a12894914" type="software">TSA</rs> program provides three methods for handling zero-event trials. Some studies with dichotomous data have zero events in the intervention or control groups. In this case, the estimate measures (RR and OR) of the intervention effect are not meaningful [27]. To address this problem, continuity correction, where we add some constant to the number of events and nonevents in the compared groups, can be the statistical solution.</p>
<p>In constant continuity correction, a constant is added to the number of events and nonevents in all groups. This method is simple and the most commonly used. The continuity correction factor commonly used in <rs id="a12894915" type="software">Review Manager</rs> software is
0.5. This method yields some problems, such as inaccurate estimation of intervention when the randomization ratio to groups are not equal or too narrow confidence interval is induced [27].</p>
<p>The <rs id="a12894916" type="software">TSA</rs> shows the menu bars at the start of the program: File, Batch, and
Review Manager. Under these menu bars, another row, namely Meta-analysis, Trials, TSA, Graphs, and Diversity, are located. We can start a new meta-analy-sis project by clicking the New Meta-analysis sub-menu under the File menu bar. Then, a New Meta-analysis window will be created with a drop-box named Data Type, blanks named Name, Label for Group 1, Label for Group 2, and Comments, and check-box named Outcome type. By entering or selecting appropriate information in the New Meta-analysis window, we can create a new meta-analysis. Here, we can choose dichotomous or continuous Data Type drop-box and negative or positive in the Outcome type check-box.</p>
<p><rs id="a12894917" type="software">TSA</rs> programs provide the option to import meta-analysis data saved in the <rs id="a12894918" type="software">Review Manager</rs> v. <rs id="a12894919" type="version" corresp="#a12894918">5</rs> file (*.rm5) through <rs id="a12894920" type="software" subtype="component" corresp="#a12894921">RM5 Converter</rs>, shown in the menu bar of the <rs id="a12894921" type="software" subtype="environment">TSA</rs> program. We can also add, edit, and delete trials using the Trials tab. When clicking on the Trials tab, Add Dichotomous Trial or Add Continuous Trial according to the type of data, Edit/Delete Trial, and Ignore Trials area will appear on the left side of the window.</p>
<p>When the TSA tab is activated, the Add area appears on the left upper side of the window. There are three buttons within the Add area: Conventional Test Boundary, Alpha-spending Boundaries, and Law of the Iterated Logarithm, where we can apply the type of significance test. Clicking on the Conventional Test Boundary button activates the Add Conventional Test window, in which the name of test (Name) and Type I error (Type I error) can be specified and the Boundary type (one-sided upper, one-sided lower, and two-sided) can be selected. The <rs id="a12894922" type="software">TSA</rs> program provides a linear conventional test boundary according to the boundary type and Type I error applied in the TSA graph.</p>
<p>In the Boundary Identifier area, we can name the test applied (Name). In the Hypothesis Testing area, there are Boundary Type and Information Axis check-boxes and αspending Function drop-boxes. The Boundary Type enables choosing the type of boundary (One-sided Upper, Ones-sided Lower, and Two-sided), and Information Axis allows choosing the type of information as the number of patients included (Sample Size), number of events (Event Size), or Statistical Information. We can also set the Type I error value and choose whether to apply the inner wedge using the Apply Inner Wedge check-box. The Apply Inner Wedge enables testing for futility by choosing the level of Type II error (Power) and β-spending Function. For both the α-and β-spending Function, only the O'Brien-Fleming function is available in the <rs id="a12894923" type="software">TSA</rs> program.</p>
<p>To perform the analysis using <rs id="a12894924" type="software">TSA</rs> program, we use the Perform calculations button in the Calculations area.</p>
<p>The <rs id="a12894925" type="software">TSA</rs> program provides diversity estimates for the random effect models using the DL method (Random DL), SJ method (Random SJ), and BT method (Random BT).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f491355917"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:30+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Web-based data collection is increasingly popular in both experimental and survey-based research because it is flexible, efficient, and location-independent. While dedicated software for laboratory-based experimentation and online surveys is commonplace, researchers looking to implement experiments in the browser have, heretofore, often had to manually construct their studies' content and logic using code. We introduce <rs id="a12898889" type="software">lab.js</rs>, a free, open-source experiment builder that makes it easy to build studies for both online and in-laboratory data collection. Through its visual interface, stimuli can be designed and combined into a study without programming, though studies' appearance and behavior can be fully customized using <rs id="a1" type="language" corresp="#a12898893">HTML</rs>, <rs id="a2" type="language" corresp="#a12898893">CSS</rs>, and <rs id="a43" type="language" corresp="#a12898893">JavaScript</rs> <rs id="a12898893" type="software" subtype="implicit">code</rs> if required. Presentation and response times are kept and measured with high accuracy and precision heretofore unmatched in browser-based studies. Experiments constructed with <rs id="a12898894" type="software">lab.js</rs> can be run directly on a local computer and published online with ease, with direct deployment to cloud hosting, export to web servers, and integration with popular data collection platforms. Studies can also be shared in an editable format, archived, re-used and adapted, enabling effortless, transparent replications, and thus facilitating open, cumulative science. The software is provided free of charge under an open-source license; further information, <rs id="a12898895" type="software" subtype="implicit">code</rs>, and extensive documentation are available from <rs id="a12898896" type="url" corresp="#a12898895">https://lab.js.org</rs>.</p>
<p>Our goal herein is to make available this untapped potential, and to increase the accessibility and usefulness of browser-based and online research by providing an open, general-purpose study builder-<rs id="a12898897" type="software">lab.js</rs>-in the spirit of widely used laboratory-based software (Mathôt et al., 2012;Peirce, 2007). It is designed to be easy to use without prior technical knowledge, but fully customizable and extensible by advanced users. It is built to integrate with the existing ecosystem of tools for online research, and to make use of the full capabilities of modern browsers for interactivity. It also provides excellent timing performance across systems, addressing a major concern in online experimental research (Hilbig, 2016;Semmelmann &amp; Weigelt, 2017a;de Leeuw &amp; Motz, 2015). Our software is available free of charge, enabling the open archival, sharing, and replication of studies. Indeed, it is designed to facilitate and encourage the exchange and re-use of tasks and paradigms. Thus, we have endeavored to meet the practical needs of researchers as well as the requirements of a modern, open, transparent, and reproducible scientific practice.</p>
<p>In doing so, we extend previous work that has identified and partially addressed these issues. In particular, libraries such as <rs id="a12898898" type="software">jsPsych</rs> <rs id="a12899030" type="bibr">(de Leeuw, 2014)</rs> and <rs id="a12898900" type="software">QRTEngine</rs> <rs id="a12899031" type="bibr" corresp="#a12898900">(Barnhoorn et al., 2015)</rs> have greatly facilitated manual coding of studies by providing programming frameworks that automate stimulus display and response collection for many common paradigms. The <rs id="a12898902" type="software">QRTEngine</rs> in particular pioneered new methods for more accurate experimental timing, and <rs id="a12898903" type="software">jsPsych</rs> was the first to provide a set of templates for common research scenarios, making in-browser experimentation much more accessible. All of these previous libraries, however, require programming skills. <rs id="a12898904" type="software">JATOS</rs> <rs id="a12899032" type="bibr">(Lange et al., 2015)</rs> and <rs id="a12898906" type="software">Tatool Web</rs> <rs id="a12899033" type="bibr">(von Bastian et al., 2013)</rs> offer easy-to-use interfaces that vastly simplify the assembly and deployment of task batteries, though the paradigms themselves are still constructed manually. <rs id="a12898907" type="software">PsyToolkit</rs> <rs id="a12899034" type="bibr">(Stoet, 2017)</rs> presents yet another perspective, using a custom textbased syntax to generate questionnaires and experiments.</p>
<p>The software is the result of our own requirements and experience, and in constructing it, we have endeavored to meet the demands of modern experimental research: <rs id="a12898909" type="software">lab.js</rs> is freely available, and thus accessible regardless of resources. It is open as code, thereby amenable to inspection, customization, and extension. Because of its open nature, studies created with it can likewise be shared freely, re-used, and adapted, facilitating cumulative, open science. It is flexible, in that it accommodates the full range of behavioral research, from psychophysical studies to questionnaires. Finally, it can be applied across contexts, in a laboratory setting as well as directly on participants' devices; there need only be a browser present.</p>
<p>Studies in <rs id="a12898910" type="software">lab.js</rs> are assembled from components, building blocks that, together, make up a study. These can be screens that present stimuli, sequences thereof, or more complex blocks that combine and repeat parts of a study. The <rs id="a12898911" type="software">lab.js</rs> study builder provides a visual interface for designing individual components and combining them into a larger experiment. The builder can be used in any recent browser without installation. It, too, is linked from the project homepage, or can be accessed directly by navigating to <rs type="url" corresp="#a12898911" id="a3">https://lab.js.org/builder</rs>.</p>
<p>To mark insertion points for varying content, <rs id="a12898912" type="software">lab.js</rs> uses placeholders, temporary stand-ins that are replaced by other data during the study. Placeholders are defined Fig. 5 Minimal settings for a loop that only counts repetitions using a dollar sign and curly braces, ${}, where an expression between the brackets represents the data that is substituted as the component is prepared. To make use of the previously defined parameters, for example, we can insert ${ this.parameters.word } in place of the fixed screen content, as a placeholder for the currently static word shown during every trial (Fig. 7). Similarly, by replacing the color code in the toolbar's fill option with ${ this.parameters.color }, we can vary the word's color. As a result, color and content now change dynamically across trials. At this point, we have constructed a complete within-subjects experimental paradigm, entirely without code. Many like it can be assembled with similar ease. Placeholders are central to studies in <rs id="a12898913" type="software">lab.js</rs>, and can be used in most options throughout the builder interface.</p>
<p>The topmost, shaded row in the data preview represents the latest entry in each column, or the study's current state. Through the state, data from previous components is accessible until overwritten by a new entry in the same column. For example, in our task, the last observed response persists until the next stimulus is shown, because the intervening components do not accept or store responses. This is often useful, particularly when providing feedback regarding previous answers. 1The study's state can be used within placeholders, in a manner analogous to the parameters introduced above. For example, we might include ${ this.state.correct } on the inter-trial screen to show the accuracy of the last response and provide feedback to participants. However, if we were to try this, the screen would remain empty. This is because <rs id="a12898914" type="software">lab.js</rs>, in order to maximize performance, attempts to prepare and render all screen content as early as possible, ideally as the page is loading.foot_1 Thus, by default, screen content is fixed entirely before participants start interacting with the study, and data generated later is not available for inclusion in components. To remedy this, individual components can be set to tardy mode by checking the corresponding box on the behavior tab. Activating this option on a component means that it is prepared only just prior to its presentation, allowing it to reflect the latest collected data, though at the expense of some further processing (and potentially a minuscule delay) during the study. Activating tardy mode on the inter-trial screen makes the feedback visible-the screen now indicates the veracity of the response through the values true and false. Admittedly, this Boolean value is not the most friendly feedback, but thankfully, it is also not difficult to replace.</p>
<p>Placeholders can contain any <rs id="a12898915" type="language">JavaScript</rs> expression, so that it is possible to include small programs directly in the screen content, or in any other option that supports placeholders. So far, we have retrieved values from variables and included them verbatim, but expressions give us the opportunity to perform further computations based on state and parameters. For our example, we might want to translate the binary values into more helpful feedback by replacing the Boolean values with friendlier messages. A ternary expression helps us achieve this, by switching between two outcomes based on a binary variable. It consists of three parts, a binary criterion, and two values that are substituted depending on whether the condition is met or not. For example, ${ this.state.correct ? 'Well done!' : 'Please try again.' } evaluates into the message 'Well done!' after correct responses, whereas the message 'Please try again.' is shown following incorrect answers.</p>
<p>Although the visual interface is undoubtedly convenient, the browser offers more options for defining content. Indeed, most information on the Web is not defined as a fixed visual layout, but using the Hypertext Markup Language, HTML. This technology allows studies built with <rs id="a12898917" type="software">lab.js</rs> to draw upon the manifold options and resources for content and interaction design available to any web page, which extend far beyond the capabilities of many classical experimental tools. <rs id="a12898918" type="software">lab.js</rs> supports HTML-based screens through a dedicated component type (second from the left in Fig. 2), and studies can combine both as required.</p>
<p>Screen design presents another common hurdle for beginning online experimenters, since it requires formatting instructions defined in the CSS language. In this regard, too, <rs id="a12898924" type="software">lab.js</rs> assists researchers by providing built-in defaults for commonly used layouts. For example, a three-tiered vertical structure can be designed quickly by adding , and tags to a screen and placing the respective content within them; the built-in styles will automatically provide an appropriate screen layout (e.g., Fig. 9). To facilitate placement and alignment of content, the library also provides a range of convenience CSS classes. 3 As an example, the content-vertical-center class centers content on the vertical axis (alternatively, content can be moved to the top and bottom vertically, and horizontally to the left, right and center).</p>
<p>An additional advantage of HTML is its ability to represent forms and questionnaires, making <rs id="a12898931" type="software">lab.js</rs> useful beyond purely experimental research. This is supported through form components, which capture and process data collected in forms. 4 Their content is also defined using HTML, 5 so that a minimal questionnaire might be represented by the following snippet:</p>
<p>With the construction of the study, of course, the scientific work has only just begun. Data collection and the archival of a study's materials are further, central steps in the scientific process that <rs id="a12898935" type="software">lab.js</rs> makes easier and more efficient.</p>
<p>As with most other experimental software, studies constructed in <rs id="a12898936" type="software">lab.js</rs> can be saved to a single file that contains all content, settings, stimuli and auxiliary files. This file can be downloaded using the corresponding button in the toolbar (Fig. 1), and is best suited for development and public archival. Using the dropdown menu next to the save button, an experiment file can be re-opened later for inspection and further modification.</p>
<p>For a study to be truly useful, it must run beyond the confines of the builder and the experimenter's device, and be made accessible within a laboratory or publicly on the Internet. This, too, previously demanded specialized technical knowledge, and therefore presented a challenge for researchers considering online data collection. Depending on the project goals and the available infrastructure, <rs id="a12898937" type="software">lab.js</rs> offers a wide and growing range of options for data collection, all designed to vastly simplify the previously complex task of hosting studies. All deployment are likewise available from the dropdown menu next to the save button.</p>
<p>Where full control over the data collection process is required, the PHP backend bundle can be used to install a survey on most, if not all, common web servers. This option produces a zip archive which, extracted on a PHP- enabled webspace, fully automates data collection: Data are continuously sent from the client and gathered in a database as participants complete the study. 7Studies created with <rs id="a12898938" type="software">lab.js</rs> also integrate with external tools as part of a larger data collection project. Another export option creates the code required for integration in survey tools such as the proprietary services <rs id="a12898939" type="software">Qualtrics</rs> (<rs id="a12898940" type="publisher" corresp="#a12898939">Qualtrics</rs>, 2016) or <rs id="a12898941" type="software">SoSci Survey</rs> <rs id="a12899026" type="bibr">(Leiner, 2014)</rs>, and open-source alternatives like the powerful survey frameworks <rs id="a12898943" type="software">Formr</rs> <rs id="a12899025" type="bibr">(Arslan et al., 2020)</rs> or <rs id="a12898945" type="software">LimeSurvey</rs> (<rs id="a12898946" type="publisher" corresp="#a12898945">Limesurvey
GmbH</rs>, 2018). Beyond that, the builder provides a direct export to <rs id="a12898947" type="software">The Experiment Factory</rs> <rs id="a12899027" type="bibr">(Sochat et al., 2016;Sochat, 2018)</rs>, which is an open-source framework for assembling and hosting batteries of tasks in fully reproducible containers, as well as <rs id="a12898948" type="software">JATOS</rs> <rs id="a12899028" type="bibr">(Lange et al., 2015)</rs>, <rs id="a12898950" type="software">Open Lab</rs> <rs id="a12899029" type="bibr">(Shevchenko &amp; Henninger, 2019)</rs>, and <rs id="a12898951" type="software">Pavlovia</rs> (<rs id="a12898952" type="url" corresp="#a12898951">https://pavlovia.org</rs>). These easy-to-use, comprehensive, open-source study hosting platforms not only make study hosting, recruitment and data collection easy, but provide further features such as (in the case of <rs id="a12898953" type="software">JATOS</rs>) real-time interaction between participants and coordination with crowdsourcing services such as <rs id="a12898954" type="publisher" corresp="#a12898955">Amazon</rs>'s <rs id="a12898955" type="software">Mechanical Turk</rs>.</p>
<p>Through all of these deployment options, we aim to support a wide range of data collection scenarios, so that <rs id="a12898956" type="software">lab.js</rs> can be used by researchers regardless of their technical experience and the infrastructure at their disposal. Across all alternatives, we automatically implement best practices for online research wherever possible, with the least amount of effort on part of the user. For example, cloud and server deployment options are configured to support the multiple site entry technique, through which participant populations can be distinguished by the URL through which they access the study (Reips, 2002). Likewise, the software automatically captures information provided by external recruitment services, such as worker and task IDs generated by <rs id="a12898957" type="publisher" corresp="#a12898958">Amazon</rs> <rs id="a12898958" type="software">Mechanical Turk</rs> (cf. Stewart, Chandler, &amp; Paolacci, 2017). Where external files are used, their paths are obfuscated so as not to reveal the experiment's structure.</p>
<p>Beyond the publication and re-use of entire studies, <rs id="a12898959" type="software">lab.js</rs> is built to facilitate the recombination, extension, and exchange of individual components or larger parts of an experiment. Screens, forms, or entire tasks are designed to be self-contained and easily transferable between studies.</p>
<p>Through the template mechanism, <rs id="a12898960" type="software">lab.js</rs> bridges the gap between manual programming, which offers control over every aspect of a study, and entirely template-focused tools that limit researchers to a set of predefined tasks or stimuli. Using templates, more advanced users can package technically complex paradigms into easy-to-use units that can be reapplied without the expertise and effort that were necessary to create them. This, however, does not hinder customization and adaptation-by exposing the relevant settings, a task can be adjusted to match the needs of a research project without requiring detailed knowledge of its innermost workings. Because the template setting is reversible, the accessibility of a task bundled as a template does not preclude in-depth inspection and modification: Paradigms can be handled and modified at multiple levels of abstraction or technical detail, suiting the needs of the individual researcher.</p>
<p>A common concern of researchers considering online data collection has been the accuracy and precision of presentation and response timing, especially for fast-paced experimental paradigms. Empirical validation studies have found that browser-based stimulus display and response collection incurred lags and variability both within a given browser and across different combinations of browser and operating system (e.g., Reimers &amp; Stewart, 2014). Though many phenomena are demonstrably robust to any measurement inaccuracy introduced both by moving from dedicated experimental software to browser-based data collection and gathering data outside of the controlled laboratory setting (Semmelmann &amp; Weigelt, 2017a;de Leeuw &amp; Motz, 2015;Hilbig, 2016;Crump et al., 2013;Simcox &amp; Fiez, 2014), considerable room for improvement has remained with regard to both accuracy (denoting freedom from bias or lag) and precision (freedom from measurement noise, Plant &amp; Turner, 2009). With <rs id="a12898961" type="software">lab.js</rs>, we build and improve upon previous approaches to browser-based experimentation, reducing both lags and measurement noise, and further approaching the performance of native experimental software (see also Henninger, Schuckart, &amp; Arslan, 2019).</p>
<p>The first prerequisite for precise time measurement is exact timekeeping. Our framework consistently uses highresolution timers that provide sub-millisecond precision for all measurements, following Barnhoorn et al. (2015). This is a simple, but effective improvement over previous inbrowser timing methods that truncate timestamps at the millisecond level by default. 8A second imperative for precise timing is that measurements are synchronized to the display refresh. Failing to do so results in added noise because time measurement might start before or even after a stimulus has been presented. Therefore, frame synchronization is commonplace in dedicated, native experimental software (cf. Mathôt et al., 2012). In <rs id="a12898962" type="software">lab.js</rs>, all timer onsets are aligned to the browser's animation frame cycle, which closely tracks the underlying graphics hardware (Barnhoorn et al., 2015). Presentation times are likewise synchronized to browser's screen update rate: An adaptive algorithm monitors the current rendering performance and presents new content with the frame that most closely matches the intended display duration. This provides a considerable improvement over the typically used setTimeout function, which is prone to overshooting any specified duration and thereby adding lag.</p>
<p>The final element to high-performance timing is an optimized rendering engine that minimizes delays in stimulus presentation. Here again, <rs id="a12898963" type="software">lab.js</rs> improves upon previously available tools, adopting strategies formerly found only in native experimental software: It reduces computation during the study as much as possible, preloading and preparing stimuli prior to their presentation (a prepare-run-strategy, cf. Mathôt et al., 2012). For screens constructed using the visual editor, the canvasbased rendering engine provides flexible, high-performance graphics capabilities by removing the computationally expensive layout calculations required for HTML content. Users can further minimize the amount of content that the browser needs to re-render during the study through Frame components, which provide a constant frame of HTML content around a changing stimulus, thereby avoiding large changes to the document and the corresponding costly layout recalculations. For example, in our Stroop task, we might extract those parts of the screen that remain constant during the task into a frame and place the stimulus loop inside, varying only the main content between screens while leaving its surroundings (e.g., instructions) in place. Thereby, only the actual stimulus is exchanged between screens instead of re-drawing the entire display with every change, however minor. Using the canvas.Frame component to enclose a set of canvas-based screens provides a common element across all screens. This eliminates changes to the HTML document entirely, further increasing rendering performance.</p>
<p>Figures 10 and 11 summarize our validation results. In a nutshell, presentation intervals were consistently met across browsers and operating systems. <rs id="a12898967" type="software">Chrome</rs> and <rs id="a12898968" type="software">Safari</rs> always matched the intended stimulus duration exactly. <rs id="a12898969" type="software">Firefox</rs> met this criterion in more than 98% of our measurements on <rs id="a12898970" type="software">Windows</rs> and <rs id="a12898971" type="software">MAC OS</rs>, with somewhat reduced performance on <rs id="a12898972" type="software">Linux</rs>. However, <rs id="a12898973" type="software">Firefox</rs> never deviated more than a single frame from the target interval. <rs id="a12898974" type="software">Internet Explorer
Edge</rs> showed a considerable increase in single-frame deviations, and 0.3% of measurements two or more frames off the preset duration. However, the excellent performance across all other browsers demonstrates that this is specific to IE, and one might expect this browser to catch up with its competitors as it matures. 9 The overall result thus demonstrates that <rs id="a12898975" type="software">lab.js</rs> offers extremely precise stimulus timing capabilities, with the best-performing browsers approaching the level of popular Fig. 10 Timing validation results for stimulus presentation, in percent of target frames hit across simulated durations, browsers, and systems. The green areas represent the proportion of exact matches, orange areas are one frame to early or to late, and red areas two frames or more (only the case for <rs id="a12898976" type="software">Internet Explorer Edge</rs>, in less than 1% of the two longest presentation intervals). See also https://lab.js.org/performance for the most recent timing results native experimental software (Mathôt et al., 2012;Garaizar et al., 2014;Garaizar &amp; Vadillo, 2014).</p>
<p>Regarding response times, our results show somewhat greater variability across browsers. Most consistently overestimate response latencies by between one and two frames (16.7 to 33.4 ms), with fairly little noise (the maximum SD we observed was 7.4 ms, in <rs id="a12898977" type="software">Internet Explorer Edge</rs> at 1000-ms response latency). <rs id="a12898978" type="software">Chrome</rs> stands out not only for its small measurement variability across operating systems, but also for its consistent lag of almost exactly a frame on <rs id="a12898979" type="software">Linux</rs> and <rs id="a12898980" type="software">MAC OS</rs>, and around 1.5 ms on <rs id="a12898981" type="software">Windows</rs>. We fully anticipate that this result will improve further with browsers' future development, and provide more detailed and up-to-date information at https://lab.js.org/performance. All this being said, we would like to emphasize that the standard of absolute timing accuracy and precision applied above, while well worth pursuing, is a very high one. In practice, the measurement noise for response times we report above is negligible in many common paradigms: Even for between-subject experimental comparisons, Reimers and Stewart (2014) show through simulations that a small increase in the number of participants makes up for any loss of power due to between-browser variations and a timing noise larger than the one we observed (see also Ulrich &amp; Giray, 1989;Damian, 2010;Brand &amp; Bradley, 2012). Similarly, within-subjects designs that focus on differences in response times between conditions (which we intuit are already common in paradigms that rely on response times) are insensitive to any consistent lag introduced in timing (see also Reimers &amp; Stewart 2014, for an in-depth discussion). Our sole caution is that correlations between individual differences and absolute response times might be mediated by participants' choice of browser (Buchanan &amp; Reips, 2001), but again, compared to common variation in response times, we observe only very small differences between browsers. Should such concerns, however, become pressing, studies built in <rs id="a12898982" type="software">lab.js</rs> also translate naturally to a laboratory setting, which provides the opportunity to run the study in a browser with the desired timing characteristics, and on consistent hardware.</p>
<p>In sum, reviewing this pattern of results and particularly the timing performance that <rs id="a12898983" type="software">lab.js</rs> offers in combination with the most powerful browsers, we cautiously predict that further improvements are unlikely to stem from browserbased experimental software itself, but will result from browser and operating system advancements. Finally, we would like to note that all of these measurements are exclusively concerned with, and therefore purposefully isolate, our software's performance: Beyond any timing inaccuracy introduced by the software, common peripheral hardware such as off-the-shelf keyboards and displays is likely to introduce further lags and measurement noise (e.g., Garaizar et al., 2014;Plant &amp; Turner, 2009;Lincoln &amp; Lane, 1980). These, however, apply not only to online data collection but also most laboratory settings, unless specialized response hardware is used.foot_8 Though the variability of peripherals outside of the laboratory is likely to introduce a slight amount of additional variance, this is unlikely to affect qualitative findings except for the smallest of effects (Hilbig, 2016;Semmelmann &amp; Weigelt, 2017a;Brand &amp; Bradley, 2012).</p>
<p>All components in a study can be customized through <rs id="a12898984" type="language" corresp="#a12898985">JavaScript</rs> <rs id="a12898985" type="software" subtype="implicit">code</rs> to fit the requirements of the scientific endeavor at hand. In the builder's scripts tab, custom instructions can be attached to every component, to be run at specific points during its lifecycle. For example, when it is prepared, a component's options might be adjusted depending on participants' previous performance, enabling adaptive experimentation. When a component is run, the presentation can be extended beyond the default behavior, enabling, for example, interaction patterns within a single screen that go far beyond the standard stimulusresponse pattern. In addition, code can be executed when the presentation comes to an end, for example to compute indices based on the collected responses. Similarly, because the library exposes the stimulus canvas and the HTML document directly via standardized browser interfaces, any content or library that can be included on a regular web page can also be added to an experiment built with <rs id="a12898987" type="software">lab.js</rs>. This ability to add custom logic during the study, and to draw from the rich ecosystem of the web, greatly increases the flexibility of our tool, allowing it to cater to applications yet to be envisioned, and to complete tasks we have not foreseen or implemented directly in the interface.</p>
<p>All of these components provide a consistent <rs id="a12898988" type="language">JavaScript</rs> interface. For example, a developer might write stroop Screen.run() to trigger the preparation and display of the screen defined above, which would show the stimulus and wait for one of the predefined responses. The exact same method can be applied to the stroopTrial to present a slightly more involved sequence of events. To include custom logic, instructions can be added to any component for execution at a later point in time through the command .foot_9 On a technical level, all different components are linked through <rs id="a12898989" type="language">JavaScript</rs>'s inheritance mechanism, and adopt the vast majority of their behavior from the general-purpose lab.core.Component, extending it only as by their specific use. For the lab.html.Screen component only inserts its HTML content into the page when it is run; most other logic, including the substitution of placeholders, is provided by the library core. In a similar way, and with very little effort, new functionality can be added to the library itself by creating custom components that perform specific tasks. In addition, a plugin mechanism exists to attach logic to any component in a study, regardless of its type. This is, for example, used to provide the data preview for all parts of the study.</p>
<p>We have taken great care to follow best practices for scientific software (Wilson et al., 2014) while developing <rs id="a12898991" type="software">lab.js</rs>: The <rs id="a12898992" type="software" subtype="implicit">code</rs> available under an open-source license which allows for free extension and modification. All changes are tracked in the project repository, and a set of automated tests run across multiple browsers are applied to every one, ensuring continued stability and compatibility. We are confident that this infrastructure will greatly facilitate sustained development. Experiment files are automatically upgraded to the latest library version, incorporating changes and updates, while exported studies include all necessary files to support continued use and long-term archival.</p>
<p> <rs id="a12898993" type="software">lab.js</rs> provides an easy-to-use, visual interface for building browser-based studies, enabling efficient data collection both in the laboratory and online. The graphical builder makes the design of studies easy, while HTML, CSS, and <rs id="a12898996" type="language">JavaScript</rs> integration give researchers full control over their studies' presentation and behavior. Its present focus is on experimental paradigms, for which it provides powerful, high-performance stimulus and response timing methods, but our library supports the full gamut of behavioral research, and can make full use of the powerful capabilities of modern browsers.</p>
<p>We believe that the advantages of a platform like <rs id="a12898997" type="software">lab.js</rs> are not limited to more efficient data collection: Because the web is an almost ubiquitous medium, and <rs id="a12898998" type="software">lab.js</rs> freely available, studies can easily be shared with colleagues before they are used in the field, facilitating collaboration within a project. Following publication, studies can be publicly archived, viewed, modified and adapted by interested researchers, who can build upon previous efforts and customize or extend existing studies without having to re-implement a paradigm in its entirety. Our software makes it easy to export parts of studies in an editable format for sharing and re-use, facilitating collaboration and cumulative science (Nielsen, 2011;Ince et al., 2012); completed studies can be similarly shared in archivable form, so that paradigms can be viewed and potentially replicated directly without additional software.</p>
<p>Through <rs id="a12898999" type="software">lab.js</rs>, we have aimed to make available the power and flexibility offered by dedicated laboratorybased data collection software in the browser and on the web. We hope that it will enable a more efficient (and, keeping with the spirit of Mathôt et al., 2012, perhaps even fun) realization of behavioral science, and we would be proud to support the ingenuity and creativity of our fellow researchers. The internet has been the medium with the fastest growth in the history of humanity, and it continues to evolve rapidly. We hope to track these developments and incorporate future best practices in the library, benefiting all users. As an open and freely available project, we would be thrilled to serve as a foundation for future browser-based research.</p>
<p>For many experimental paradigms, the accuracy of stimulus' presentation durations and the measurement of response times is paramount. To rigorously assess the performance of <rs id="a12899000" type="software">lab.js</rs>, we therefore captured the generated stimulus durations and simulated response times of different lengths precisely using external hardware.</p>
<p>A simple experiment alternated screens of dark and light stimuli. To test display durations, 100 stimuli of either brightness were shown in succession, set to a timeout of either 50, 100, 250, 500, or 1000 ms. For response time validation, we conducted a second experiment with the same setup that waited for keyboard input on every screen, and we simulated a keypress after each of the aforementioned intervals. We ran this experiment through all presentation and response durations across the most recent versions of all major browsers ( <rs id="a12899001" type="software">Firefox</rs>, <rs id="a12899002" type="software">Chrome</rs>, <rs id="a12899003" type="software">Safari</rs>, <rs id="a12899004" type="software">Internet Explorer</rs> <rs id="a12899005" type="version" corresp="#a12899004">1</rs>).</p>
<p>A <rs id="a12899008" type="software" subtype="implicit">visual questionnaire builder</rs> for <rs id="a12899009" type="software">lab.js</rs> is currently under development</p>
<p>At the time of writing, <rs id="a12899010" type="software">lab.js</rs> provides direct export to the <rs id="a12899011" type="software">Netlify</rs> hosting service (<rs id="a12899012" type="url" corresp="#a12899011">https://netlify.com</rs>).</p>
<p>At the time of writing, some browsers add a small amount of artificial jitter-2 ms in the case of <rs id="a12899013" type="software">Firefox</rs>-to all time measurements for security reasons, limiting the potential timing precision of any browser-based data collection project. Having resolved the underlying security issues, the <rs id="a12899014" type="software">Chrome</rs> developers have proceeded to remove this artificial source of noise, and we expect the other browsers to follow suit.</p>
<p>At the time of writing, <rs id="a12899015" type="software">Internet Explorer</rs> is slated for replacement by a new browser with the same technical foundation as <rs id="a12899016" type="software">Chrome</rs></p>
<p>The workhorse of stimulus display and data collection in <rs id="a12899017" type="software">lab.js</rs> is a custom <rs id="a12899018" type="language">JavaScript</rs> framework that governs all interaction with participants. In the workflow demonstrated above, the builder interface generates a JavaScript representation of the study, which is read and executed by the framework within participants' browsers. The entire study logic thus runs on the client side, reacting immediately to participant input and eliminating network latency which would be introduced by loading new page content from the server between screens.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f612953881"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:17+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Apparatus. The stimuli were presented using <rs id="a12951885" type="software">PsyScope X</rs> (<rs id="a12951886" type="url" corresp="#a12951885">http://psy.ck.sissa.it</rs>) running on a MacPro 4, 1. Videos were displayed on a widescreen (24″) Tobii T-60 XL eye tracker system (Stockholm, Sweden). Looking behavior was recorded both by a video recorder for offline analysis and by <rs id="a12951887" type="software">Tobii Studio</rs> <rs id="a12951888" type="version" corresp="#a12951887">3.0</rs> running on a Dell Precision T5400.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f223021646"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T16:31+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>In the training version of the task, the n level of the dual n-back task increased adaptively when participants achieved 80% correct responses in the trial, and the n level decreased when participants made &gt;50% errors in the trial. After each trial, the n level achieved by a participant was recorded, and the mean n level during each of 18 training session was used later to calculate the total training progress (Supplementary Figs. 1 and2). Participants from the control group performed a single 1-back with auditory or visuospatial stimuli variants. To minimize boredom of participants, the order of the 1-back variants was randomly selected at the beginning of each training session. Therefore, each participant from the control group had the same number of training trials on single auditory and visuospatial nback tasks. Participants completed a total of 18 sessions (30 min each) under the supervision of the experimenter. Each participant completed 20 blocks (each consisting of 20 + n trials, depending on the n level achieved by the participant) of the n-back task during each training session. The study was double-blind; the experimenter performing the fMRI examination was not aware of the group assignment of the participants, and participants were not aware that the study was designed in a way that there were two groups (experimental and control). The apparatus used in the study consisted of two 17" Dell Inspiron Laptops, and two pairs of Sennheiser headphones. Stimulus delivery was controlled by a <rs id="a12885853" type="software" subtype="environment">Python</rs> <rs id="a12885854" type="software" subtype="implicit" corresp="#a12885853">adaptation of the dual n-back task</rs> used by <rs id="a12885855" type="bibr">Jaeggi et al. 14</rs> (<rs id="a12885856" type="url" corresp="#a12885854">http://brainworkshop. sourceforge.net/</rs>).</p>
<p>In the fMRI scanning version of the task, participants performed the dual nback task with two levels of difficulty: 1-back and 2-back. Each session of the task consisted of 20 blocks (30 s per block; 12 trials with 25% of targets) of alternating 1and 2-back conditions. To enable dynamic network comparison across blocks, we did not add any systematic variation to block length and block order. The instruction screen was displayed for 4000 ms before each block, informing the participant of the upcoming condition. Both visual and auditory stimuli were presented in a pseudorandom order. Participants were asked to push the button with their right thumb if the currently presented square was in the same location as the previous square (1-back) or two squares back in the sequence (2-back) and, at the same time, push the button with their left thumb when the currently played consonant was the same as the previous consonant (1-back) or two consonants back (2-back). The participants had 2000 ms to respond, and were instructed to respond as quickly and accurately as possible. The experimental protocol execution and control (stimulus delivery and response registration) employed version <rs id="a12885857" type="version" corresp="#a12885858">17.2.</rs> of <rs id="a12885858" type="software">Presentation</rs> software (<rs id="a12885859" type="publisher" corresp="#a12885858">Neurobehavioral Systems</rs>, Albany, NY), as well as MRI compatible goggles (visual stimulation), headphones (auditory stimulation), and response grips (response registration) (NordicNeuroLab, Bergen, Norway). Before each scanning session, participants performed a short dual n-back training session outside the fMRI scanner to (re-)familiarize them with the rules of the task.</p>
<p>Data processing. After converting from DICOM to NifTI format, functional and anatomical data were structured according to the BIDS (Brain Imaging Data Structure) standard 46 and validated with <rs id="a12885860" type="software">BIDS Validator</rs> (<rs id="a12885861" type="url" corresp="#a12885860">https://bids-standard. github.io/bids-validator/</rs>). Neuroimaging data was preprocessed using <rs id="a12885862" type="software" subtype="component" corresp="#a12885865">fMRIPrep</rs> <rs id="a12885863" type="version" corresp="#a12885862">1.1.1</rs> <rs corresp="#a12885862" id="a12885864" type="bibr">47</rs> a <rs id="a12885865" type="software" subtype="environment">Nipype</rs> <rs id="a12885866" type="bibr" corresp="#a12885865">48</rs> -based tool. See Supplementary Methods for details on anatomical data processing. Functional data were slice time corrected using <rs id="a12885867" type="software" subtype="component" corresp="#a12885868">3dTshift</rs> from <rs id="a12885868" type="software" subtype="environment">AFNI</rs> v <rs id="a12885869" type="version" corresp="#a12885868">16.2.07</rs> <rs id="a12885870" type="bibr" corresp="#a12885868">49</rs> and motion corrected using <rs id="a12885871" type="software" subtype="component" corresp="#a12885872">MCFLIRT</rs> (<rs id="a12885872" type="software" subtype="environment">FSL</rs> v <rs id="a12885873" type="version" corresp="#a12885872">5.0.9</rs> <rs id="a12885874" type="bibr" corresp="#a12885872">50</rs> ). This process was followed by co-registration to the corresponding T1w using boundarybased registration 51 with 9 degrees of freedom, using <rs id="a12885890" type="software" subtype="component" corresp="#a12885875">bbregister</rs> ( <rs id="a12885875" type="software" subtype="environment">FreeSurfer</rs> v <rs id="a12885876" type="version" corresp="#a12885875">6.0.1</rs>). Motion correcting transformations, BOLD-to-T1w transformation and T1w-totemplate (MNI) warp were concatenated and applied in a single step using <rs id="a12885877" type="software" subtype="component" corresp="#a12885878">antsApplyTransforms</rs> (<rs id="a12885878" type="software" subtype="environment">ANTs</rs> v <rs id="a12885879" type="version" corresp="#a12885878">2.1.0</rs>) employing Lanczos interpolation. Physiological noise regressors were extracted by applying CompCor 52 . Principal components were estimated for the two CompCor variants: temporal (tCompCor) and anatomical (aCompCor). A mask to exclude signal with cortical origin was obtained by eroding the brain mask,ensuring that it only contained subcortical structures. Six tCompCor components were then calculated including only the top 5% variable voxels within that subcortical mask. For aCompCor, six components were calculated within the intersection of the subcortical mask and the union of the CSF and WM masks calculated in T1w space, after their projection to the native space of each functional run. Frame-wise displacement 53 (FD) was calculated for each functional run using the implementation of <rs id="a12885880" type="software">Nipype</rs>. The internal operations of <rs id="a12885881" type="software" corresp="#a12885891">fMRIPrep</rs> use <rs id="a12885891" type="software" subtype="environment">Nilearn</rs> <rs id="a12885882" type="bibr">54</rs> , principally within the BOLD-processing workflow. For more details of the pipeline see <rs id="a12885883" type="url" corresp="#a12885881">https://fmriprep.readthedocs.io/en/latest/ workflows.html</rs>.</p>
<p>Non-smoothed functional images were denoised using <rs id="a12885884" type="software">Nilearn</rs> <rs id="a12885885" type="bibr">54</rs> and <rs id="a12885886" type="software">Nistats</rs>. We implemented voxel-wise confound regression by regressing out (1) signals from six aCompCor components, (2) 24 motion parameters representing 3 translation and 3 rotation timecourses, their temporal derivatives,and quadratic terms of both, (3) outlier frames with FD &gt; 0.5 mm and DVARS (Derivative of rms VARiance over voxelS) 55 with a threshold of ±3 SD, together with their temporal derivatives, (4) task effects and their temporal derivatives 56 , and (5) any general linear trend. Time series were filtered using a 0.008-0.25-Hz band-pass filter. We excluded four high motion participants (two from the control group, and two from the experimental group) with a mean FD larger than 0.2 mm and more than 10% of outlier volumes in any scanning session (Supplementary Fig. 5).</p>
<p>Due to the nested nature of the study data, we used two-level (trials nested within participants) and three-level (trials nested within sessions nested within participants) multilevel models 27 (MLM) at four points during our analysis of the data. In all cases, random intercepts were estimated. The significance of models was estimated with chi-square tests, where models with increasing complexity were compared and the resulting value of likelihood ratio test (χ 2 ) and corresponding p-value were reported 61 . The MLM analysis was performed using <rs id="a12885887" type="software" subtype="component" corresp="#a12885889">nlme</rs> <rs id="a12885888" type="bibr">62</rs> <rs id="a12885889" type="software" subtype="environment">R</rs> package.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f30781653"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T07:02+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Periodic boundary condition calculations were performed using the linear-scaling pseudopotential DFT software <rs id="a12971790" type="software">ONETEP</rs> version <rs id="a12971791" type="version" corresp="#a12971790">3.5.9.8</rs> [<rs id="a12971792" type="bibr">56</rs>,<rs id="a12971793" type="bibr">57</rs>]. PBE OPIUM [58] norm conserving pseudopotential (NC-PPs) bundled with <rs id="a12971794" type="publisher" corresp="#a12971795">Accelerys</rs> <rs id="a12971795" type="software">Material Studio</rs> <rs id="a12971796" type="version" corresp="#a12971795">6.0.0</rs> were utilised in all <rs id="a12971797" type="software">ONETEP</rs> calculations. An effective kinetic energy cutoff of approximately 800 eV was used for the psinc basis set [59], which is equivalent to the energy cutoff used in conventional plane-wave DFT codes. DFT in <rs id="a12971798" type="software">ONETEP</rs> was performed using self-consistent field convergence criteria whereby the RMS gradient of the NGWFs must be equal to or less than 1.8375 Â 10 À6 E h a À3=2 0 . Geometry optimisations proceeded using the BFGS algorithm until the difference in energy between iterations was equal to or less than 1 Â 10 À5 eV, 0.03 eV Å À1 and 0.001 Å for the energies, forces and maximum atomic displacement respectively. Unless otherwise stated, all calculations were performed using these settings. <rs id="a12971799" type="software">NWChem</rs> software version <rs id="a12971800" type="version" corresp="#a12971799">6.3</rs> <rs id="a12971801" type="bibr">[60]</rs> was used to perform all-electron calculations. Unless otherwise specified, these calculations were performed using the driver module, DFT and the PBE-GGA functional. All calculations used a total energy SCF tolerance of 10 Â 10 À8 E h and the aug-cc-pvtz basis set. For the data presented in Fig. 5, <rs id="a12971802" type="software">NWChem</rs> geometry optimisations were performed using the stepper module and a 0.05 Å maximum displacement per iteration. Example input files used and unit conversions can be found within the Supplementary Information Section 1.</p>
<p>In order to validate the pseudopotential used, the geometry, deprotonation energy and adsorption energy of monomeric silanol-water and silanolate-water systems were compared with allelectron calculations, the results of which are presented in Tables 1 and2 respectively. It can be seen that there is good agreement (1-4% difference) in the calculated <rs id="a12971803" type="software">ONETEP</rs> NC-PPs and all-electron energies. Optimised geometries (not shown) demonstrated excellent agreement, with bond lengths within 0.01 Å and angles within 0.1°.</p>
<p>Born-Oppenheimer AIMD simulations were performed using <rs id="a12971804" type="software">ONETEP</rs> to investigate the proton transfer dynamics of three water clusters (</p>
<p>Implicit solvation calculations were performed using <rs id="a12971805" type="software">ONETEP</rs>, using a self-consistent cavity and a fine grid scale of 3.0 in a 47.5 Å cubic simulation cell with open boundary conditions [62].</p>
<p>Similarly to the work of Leung et al. [13], the calculations reported herein treat the nuclei classically and it is assumed that the effects of zero point motion and tunnelling do not affect the qualitative nature of proton transfer mechanisms. This has been shown to be the case for electron transfer and pure water proton transport [63,64]. Visualisation was performed using the <rs id="a12971806" type="software">Visual Molecular Dynamics</rs> software <rs id="a12971807" type="bibr">[65]</rs> with O-H bonds and Si-O bonds drawn of internuclear separations of less than 1.1 Å and 1.7 Å respectively. Bond distances are given in Å. In some figures, hydrogen bonds have been drawn as unlabelled dotted lines using a cutoff of 3 Å and 20°angles between hydrogen bond acceptors and donors.</p>
<p>The initial silica structure of a-cristobalite was obtained from the structures bundled with <rs id="a12971808" type="publisher" corresp="#a12971809">Accelrys</rs> <rs id="a12971809" type="software">Material Studio</rs> <rs id="a12971810" type="version" corresp="#a12971809">6.0.0</rs> which was itself generated based on a paper by Dollase [71] (primitive tetragonal P4 1 2 1 2 space group, a = b = 4.978 Å, c = 6.948 Å). A variable-cell geometry optimisation using the <rs id="a12971811" type="software">CASTEP</rs> software <rs id="a12971812" type="bibr">[72]</rs> was performed on the primitive cell in order to obtain relaxed unit cell-parameters for use in future calculations (a = b = 5.075 Å, c = 7.085 Å). For this calculation, a 900 eV kinetic energy cutoff was utilised with a 4 Â 4 Â 4 k-point grid and the aforementioned NC-PPs.</p>
<p>Using the CASTEP relaxed crystal geometry, a supercell was created from these coordinates with doubled lattice parameters, and this was optimised using the <rs id="a12971813" type="software">ONETEP</rs> software, which is a fixedcell dimension calculation. This produced no significant change in molecular geometry of the crystal. A (1 0 1)-plane slab of 14 Å thickness was cleaved from this crystal and passivated with a layer of hydrogen on both top and bottom, resulting in a system of 168 atoms of isolated silanol groups. The resulting lattice parameters were a = 17.431 Å, b = 10.150 Å, c = 105.929 with approximately 90 Å of this being vacuum padding. The slab was relaxed using <rs id="a12971814" type="software">ONETEP</rs>, with no significant rearrangement of the bulk. The optimisation resulted in a contraction of approximately 0.1 Å slab thickness. The final coordinates are shown in Fig. 1.</p>
<p>The optimised silica slab (Fig. 1) demonstrated isolated silanols with an OÁ Á ÁO distance of 4-5 Å and the closest OÁ Á ÁH approach distance of 4.8 Å. This result deviates from that reported by Musso et al. [53], who reported a zig-zag pattern of hydrogen bonds. However, Musso et al. comment that these hydrogen bonds are weak and disrupted at room temperature and entirely broken in the presence of water [53]. This was investigated by repeating the geometry optimisation using <rs id="a12971815" type="software">CASTEP</rs>, (1000 eV kinetic energy cutoff, C-point sampling of the Brillouin zone and ultrasoft pseudopotentials of Civalleri and Harrison [73]). This resulted in the same geometry as the previous <rs id="a12971816" type="software">ONETEP</rs> optimisation. This indicates that the deviation in structure between this work and that of Musso et al. is a result of the latter being in a different local minimum. The local minimum obtained herein provides an idealised model of a silica surface composed of isolated silanols.</p>
<p>Taking the neutral slab, a proton was removed from a surface silanol (indicated with a circle in Fig. 1)) to create a negatively charged silanolate group and the system was geometry optimised in <rs id="a12971817" type="software">ONETEP</rs>. In the protonated system the in-plane Si-O bond length was 1.638 Å and the out-of-plane (Si)-(OH) bond length was 1.643 Å, in the deprotonated system the in-plane Si-O bond was slighlty stretched (1.692 Å) and the out-of-plane Si-O À bond was shortened (1.547 Å). The geometry of the bulk and the other surface silanols were not significantly affected by the deprotonation, demonstrating that the silanols are truly isolated even in the deprotonated system. The surface charge density used in this work ($0.05 Si-O À per nm 2 ) is similar to that calculated by Behrens and Grier for a silica plate in deionised water [74], however the surface charge density of silica is highly variable depending on surface preparation, ionic strength and pH.</p>
<p>Unless otherwise specified, calculations were performed using 3D periodic boundary conditions using <rs id="a12971818" type="software">ONETEP</rs> and a vacuum gap with a neutralising background charge to minimise periodic interactions. For systems with a net charge which are also orthorhombic, it has been shown that there will be some uncompensated neutralising background charge [75] that leads to a divergent system energy, though the forces remain convergent. The <rs id="a12971819" type="software">ONETEP</rs> implementation of DFT has the advantage that there is little computational cost to using a large vacuum gap, therefore allowing the forces within this work to be well converged with respect to the simulation cell size to within $0.005 eV/Å.</p>
<p>Proton transfer during a geometry optimisation indicates that the initial encounter-pair is energetically unstable and that there is no activation energy to the proton transfer process. Fig. 5 shows the energy profile for a geometry optimisation performed upon a cluster system at both the PBE-GGA and MP2 level of theory, and on the periodic slab model of the silica surface (PBE). It can be seen that the total energy of the system decreases smoothly and monotonically. Consistent with this observation, using both the <rs id="a12971820" type="software">ONETEP</rs> and <rs id="a12971821" type="software">NWChem</rs> transition state search functionality, no transition state could be identified for these proton transfer coordinates.</p>
<p>Thermal fluctuations might be expected to reduce the basin of attraction by breaking the hydrogen-bonded water-wires required for Grotthuss mechanism-like proton transport, however, the thermal energy would also allow activated proton transfer, thereby increasing the size of the basin of attraction. It is interesting to compare these results with the results of the AIMD simulations 1) and ( 3) and the respective system energy versus geometry optimisation step is shown in figure (a) and figure (b) respectively. A minimal cluster silanol model was used, and the chemical system is drawn as insets within each figure. The initial energy (y-axis) is normalised to zero. As each optimisation took a different number of steps, for comparison the optimisation progress (x-axis) is presented, in which the optimisation has been scaled to range from the initial structure (left of x-axis) to the fully optimised structure (far right of the x-axis). It can be seen that in both optimisations there was a smooth, monotonic decrease in energy upon optimisation, indicating an activationless proton transfer. Images of each geometry optimisation can be found in Fig. 2 within the main text for the <rs id="a12971822" type="software">ONETEP</rs> optimisations, and in Section 3 of the Supplementary Information for the <rs id="a12971823" type="software">NWChem</rs> optimisations. Fig. 6. Geometry optimisation of a Zundel cation (H5O þ</p>
<p>In order to quantify the energetics of this proton transfer, allelectron calculations using <rs id="a12971824" type="software">NWChem</rs> were performed upon a silica cluster model of an isolated silanol molecule (SiH 3 OH) and an orthosilicic acid molecule (SiðOHÞ 4 ). Three different reaction schemes were considered for each reaction in which a single additional water molecule stabilised the reactants, the full details of these calculations can be found within the Supplementary Information Section 5. Each different reaction scheme considers a different combination of hydrogen bonding between the products, which can lead to large differences in the reaction energies. Reaction energies of between À637 and À682 kJ/mol for orthosilicic acid, and of between À655 to À693 kJ/mol for silanol were calculated for Reaction (1). Reaction energies of between À43.5 and À105 kJ/mol were calculated for orthosilicic acid, and of between À25.9 and À142 kJ/mol for silanol were calculated for Reaction (3). For Reaction (3), using a ðHOÞ 3 Si-O-SiðOHÞ 2 ðOHÞ 3 model of the surface, Xiao and Lasaga calculated a reaction energy of À232.6 kJ/mol at the MP2/6-31G ⁄ level, The resulting reaction energy is likely more exothermic than the silicic acid and silanol due to the formation of multiple hydrogen bonds in the resulting complex [43].</p>
<p>a <rs id="a12971825" type="software">NWChem</rs> calculation. b Basis Set Superposition Error (BSSE) corrected. c <rs id="a12971826" type="software">ONETEP</rs> calculation.</p>
<p>AE = All-electron. NC-PP = norm-conserving pseudopotential. Water adsorption energy DE ads;gas (kJ/mol) Monomer (AE) a Monomer (NC-PP) d SiH 3 OHÁ Á ÁH 2 O À24.26 b (À23.04 c,b ) À25.18 SiH 3 O À Á Á Á H 2 O À80.78 À81.06 a <rs id="a12971827" type="software">NWChem</rs> calculation. b BSSE corrected. c MP2 level, DZ(p,d) basis set calculation from Ref. [61]. d <rs id="a12971828" type="software">ONETEP</rs> calculation.</p>
<p>I AIMD was additionally performed for those systems. II MP2 level geometry optimisation. III Geometry optimisations performed using the implicit solvation model of <rs id="a12971829" type="software">ONETEP</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f483686135"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:47+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Recent methodological researches produced permutation methods to test parameters in presence of nuisance variables in linear models or repeated measures ANOVA. Permutation tests are also particularly useful to overcome the multiple comparisons problem as they are used to test the effect of factors or variables on signals while controlling the family-wise error rate (FWER). This article introduces the <rs id="a12895085" type="software">permuco</rs> package which implements several permutation methods. They can all be used jointly with multiple comparisons procedures like the cluster-mass tests or threshold-free cluster enhancement (TFCE). The <rs id="a12895086" type="software">permuco</rs> package is designed, first, for univariate permutation tests with nuisance variables, like regression and ANOVA; and secondly, for comparing signals as required, for example, for the analysis of event-related potential (ERP) of experiments using electroencephalography (EEG). This article describes the permutation methods and the multiple comparisons procedures implemented. A tutorial for each of theses cases is provided.</p>
<p>The aim of the present article is to provide an overview of the use of permutation methods and multiple comparisons procedures using permutation tests and to explain how it can be used in <rs id="a12895087" type="software" subtype="environment">R</rs> ( <rs id="a12895088" type="publisher" corresp="#a12895087">R Core Team</rs>
2021) with the package <rs id="a12895089" type="software" subtype="component" corresp="#a12895087">permuco</rs> <rs id="a12954038" type="bibr">(Frossard and Renaud 2019)</rs>. The package is available from the <rs id="a12895091" type="software">Comprehensive
R
Archive Network (CRAN)</rs> at <rs id="a12895092" type="url" corresp="#a12895089">https://CRAN.R-project.org/package=permuco</rs>. Note that the presentation and discussion of the available packages that handle permutation tests in related settings is deferred to Section 5.1, where all the notions are introduced. Appendix A shows a comparison of the relevant code and outputs. But first, Section 2 focuses on fixed effect models. It explains the model used for ANOVA and regression and the various permutation methods proposed in the literature. Section 3 introduces the methods for repeated measures ANOVA. Section 4 explains the multiple comparisons procedures used for comparing signals between experimental conditions and how permutation tests are applied in this setting. Section 5 describes additional programming details and some of the choices for the default settings in the <rs id="a12895093" type="software">permuco</rs> package. Section 6 treats two real data analyses, one from a control trial in psychology and the second from an experiment in neurosciences using EEG.</p>
<p>The permutation test is exact under the null hypothesis for finite samples if the data are exchangeable under the null hypothesis. This assumption is not fulfilled in model in Equation 1as we cannot control the influence of the nuisance term Dη when permuting. In fact, under the null hypothesis in Equation 2, the responses follow a distribution (Dη, σ 2 I n ) which are not exchangeable due to the presence of unequal first moments. Pauly et al. (2015) show however that permuting the responses and using a Wald-type statistic is an asymptotically exact procedure in factorial designs. Another approach, which is the focus of this paper, is to transform the data prior to the permutation. Those transformation procedures are what will be called permutation methods. They are described in Section 2.2 and are implemented in <rs id="a12895094" type="software">permuco</rs>.</p>
<p>The discussed permutation methods are functions that transform the data in order to reduce the effect of the nuisance variables. They can be computed for all permutations P ∈ P where P is the set of all n P distinct permutation matrices of the same size. For any permutation matrix P , a given permutation method will transform the observed data {y, D, X} into the permuted data {y * , D * , X * }. The <rs id="a12895095" type="software">permuco</rs> package provides several permutation methods that are summarized in Table 1 using a notation inspired by Winkler et al. (2014).</p>
<p>The default method of <rs id="a12895096" type="software">permuco</rs> is the freedman_lane method that works as follows: we first fit the "small" model which only uses the nuisance variables D as predictors. Then, we permute its residuals and add them to the fitted values. Theses steps produce the permuted response variable y * which constitutes the "new sample". It is fitted using the unchanged design D and X. In this procedure, only the residuals are permuted and they are supposed (Freedman and Lane 1983) ( to share the same expectation (of zero) under the null hypothesis. For each permutation, the effect of nuisance variables is hence reduced. Using the above notation, the fitted values of the "small" model can be written as H D y and its residuals R D y. Its permuted version is pre-multiplied by a permutation matrix, e.g., P R D y. The permuted response variable is therefore simply written as y * = H D y + P R D y = (H D + P R D )y, as displayed in Table 1. The permuted statistics (e.g., t or F statistics) are then computed using y * and the unchanged design matrices D * = D and X * = X.</p>
<p>For each of the methods presented in Table 1, permutation tests can be computed using different statistics. For univariate or multivariate β parameters, the <rs id="a12895097" type="software">permuco</rs> package implemented a F statistic that constitutes a marginal test (or "type III" sum of square) (Searle 2006, pp. 53-54). For a univariate β 1×1</p>
<p>Similarly to the fixed effects model, we can test hypotheses using permutation methods (Kherad-Pajouh and Renaud 2015). The ones that are implemented in the <rs id="a12895098" type="software">permuco</rs> package are given in Table 2. The two methods are based on a similar idea. By pre-multiplying the design and response variables by R D or R D,E , we orthogonalize the model to the nuisance variables. This procedure can be viewed as an extension of the kennedy procedure (see Table 1) to repeated measures ANOVA.</p>
<p>All these approaches use permutations and are compatible with the methods displayed in Tables 1 and2, as shown next. In addition to multiple comparisons procedures that use permutation, the well-known Bonferroni and Holm (Holm 1979) corrections and the control of the false positive rate by Benjamini and Hochberg (1995) are also implemented in <rs id="a12895099" type="software">permuco</rs>.</p>
<p>Several <rs id="a12895100" type="software" subtype="implicit" corresp="#a12895101">packages for permutation tests</rs> are available for <rs id="a12895101" type="software" subtype="environment">R</rs> in <rs id="a12895102" type="software">CRAN</rs>. Since permutation tests have such a variety of applications, we only review packages (or the part of packages) that handle regression, ANOVA or comparison of signals.</p>
<p>For testing one factor, the <rs id="a12895103" type="software">perm</rs> <rs id="a12954047" type="bibr">(Fay and Shaw 2010)</rs>, <rs id="a12895105" type="software">wPerm</rs> <rs id="a12954039" type="bibr">(Weiss 2015)</rs> and <rs id="a12895107" type="software">coin</rs> <rs id="a12954040" type="bibr">(Hothorn, Hornik, Van De Wiel, and Zeileis 2008)</rs> packages produce permutation tests of differences of locations between two or several groups. The latter can also test the difference within groups or block, corresponding to a one within factor ANOVA.</p>
<p>The package <rs id="a12895109" type="software">lmPerm</rs> <rs id="a12954041" type="bibr">(Wheeler and Torchiano 2016)</rs> produces tests for multifactorial ANOVA and repeated measures ANOVA. It computes sequential (or Type I) and marginal (or Type III) tests for factorial ANOVA and ANCOVA but only the sequential is implemented for repeated measures, even when setting the parameter seqs = FALSE. The order of the factors will therefore matter in this case. The permutation method consists in permuting the raw data even in the presence of nuisance variables, which correspond to the manly method, see Table 1. For repeated measures designs, data are first projected into the "Error()" strata and then permuted, a method that has not been validated (to our knowledge) in any peer-reviewed journal. Additionally, <rs id="a12895111" type="software">lmPerm</rs> by default uses a stopping rule based on current p value to define the number of permutations. By default, the permutations are not randomly sampled but modified sequentially merely on a single pair of observations. This speeds up the code but the quality of the obtained p value is not well documented.</p>
<p>The <rs id="a12895112" type="software">flip</rs> package <rs id="a12954042" type="bibr">(Finos 2018)</rs> produces permutation and rotation tests (Langsrud 2005) for fixed effects and handles nuisance variables based on methods similar to the huh_juhn method of Table 1. It performs tests in designs with random effects only for singular models (e.g. repetition of measures by subjects in each condition) with method based on Basso and Finos (2012) and Finos and Basso (2014) to handle nuisance variables.</p>
<p>The <rs id="a12895114" type="software">GFD</rs> package <rs id="a12954043" type="bibr">(Friedrich, Konietschke, and Pauly 2017b)</rs> produces marginal permutation tests for pure factorial design (without covariates) with a Wald-type statistic. The permutation method is manly. This method has been shown to be asymptotically exact even under heteroscedastic conditions (Pauly et al. 2015). Moreover, Friedrich, Konietschke, and Pauly (2021) generalize these tests to multivariate data like MANOVA models.</p>
<p>To our knowledge, only the <rs id="a12895116" type="software">permuco</rs> package provides tests for comparison of signals.</p>
<p>The codes and outputs for packages that perform ANOVA/ANCOVA are given in Appendix A.1 and in Appendix A.2 for repeated measures. For fixed effects, this illustrates that <rs id="a12895117" type="software">permuco</rs>, <rs id="a12895118" type="software">flip</rs> and <rs id="a12895119" type="software">lmPerm</rs> handle covariates and are based on the same statistic (F ) whereas <rs id="a12895120" type="software">GFD</rs> uses the Wald-type statistic. It also shows that <rs id="a12895121" type="software">flip</rs> is testing one factor at a time (main effect of sex in this case) whereas the other packages produce directly tests for all the effects. Also, the nuisance variables in <rs id="a12895122" type="software">flip</rs> must be carefully implemented using the appropriate coding variables in case of factors. Note that <rs id="a12895123" type="software">lmPerm</rs> centers the covariates using the default setting and that it provides both marginal (Type III) or sequential (Type I) tests.</p>
<p>Concerning permutation methods, only the manly method is used for both <rs id="a12895124" type="software">lmPerm</rs> and <rs id="a12895125" type="software">GFD</rs>, the <rs id="a12895126" type="software">flip</rs> package uses the huh_jhun method, whereas multiple methods can be set by users using the <rs id="a12895127" type="software">permuco</rs> package. Note also that different default choices for the V matrix as implemented in <rs id="a12895128" type="software">flip</rs> (based on eigendecomposition) and <rs id="a12895129" type="software">permuco</rs> (based on QR decomposition) packages lead to slightly different results (see Table 1 for more information on the permutation methods).</p>
<p>Finally, concerning repeated measures designs, <rs id="a12895130" type="software">flip</rs> cannot handle cases where measures are not repeated in each condition for each subject, and therefore cannot be compared in Appendix A.2. As already said, <rs id="a12895131" type="software">lmPerm</rs> produces sequential tests in repeated measures designs and <rs id="a12895132" type="software">permuco</rs> produces marginal tests. This explains why, with unbalanced data, only the last interaction term in each strata produces the same statistic.</p>
<p>For the fixed effects model, simulations (Kherad-Pajouh and Renaud 2010; Winkler et al. 2014) show that the method freedman_lane, dekker, huh_jhun and terBraak perform well, whereas manly, draper_stoneman and kennedy can be either liberal or conservative. Moreover Kherad-Pajouh and Renaud (2010) provide a proof for an exact test of the huh_jhun method under sphericity. Note that huh_jhun will reduce the dimensionality of the data and if n -(p -q) ≤ 7 the number of permutations may be too low. Based on all the above literature the default method for the <rs id="a12895133" type="software">permuco</rs> package is set to freedman_lane.</p>
<p>The multcomp argument can be set to "bonferroni" for the Bonferroni correction (Dunn 1958), to "holm" for the Holm correction (Holm 1979), "benjamini_hochberg" for the Benjamini-Hochberg method (Benjamini and Hochberg 1995), to "troendle", see Section 4.2, to "clustermass", see Section 4.3 and to "tfce", see Section 4.4. Note that in the <rs id="a12895134" type="software">permuco</rs> package, these 6 methods are available in conjunction with permutation, although the first 3 methods are general procedures that could also be used in a parametric setting.</p>
<p>The algorithm and formula presented in the previous sections may not be efficient for very large size of data. When available, they are implemented in a more efficient way in <rs id="a12895135" type="software">permuco</rs>.</p>
<p>To load the <rs id="a12895136" type="software">permuco</rs> package:</p>
<p>The permutation tests are obtained with the <rs id="a12895137" type="software">aovperm</rs> function. The np argument sets the number of permutations. We choose to set a high number of permutations (np = 100000) to reduce the variablity of the permutation p values so that they can safely be compared to the parametric ones. The <rs id="a12895138" type="software">aovperm</rs> function automatically converts the coding of factors with the contr.sum which allows us to test the main effects of factors and their interactions. The interaction LOSc:insurance is significant both using the parametric p value 0.0116 and the permutation one 0.0233 using a 5% level. However, the difference between these 2 p values is 0.0117 which is high enough to lead to different conclusions e.g., in case of correction for multiple tests or a smaller α level.</p>
<p>If the researcher has an a priori oriented alternative hypothesis H A : β sex=M &gt; β sex=F , the <rs id="a12895139" type="software">lmperm</rs> function produces one-sided t tests. To run the same models as previously, we first need to set the coding of the factors with the <rs id="a12895140" type="software">contr.sum</rs> function before running the permutation tests.</p>
<p>To test the effect of the sex within the public insured persons (called simple effect), we change the coding of the factors inside the data.frame using the <rs id="a12895141" type="software">contr.treatment</rs> function and disable the automatic recoding using the argument coding_sum = FALSE.</p>
<p>We perform the permutation tests by running the <rs id="a12895142" type="software">aovperm</rs> function. The within subject factors should be written using + Error(...) similarly to the <rs id="a12895143" type="software" subtype="component" corresp="#a12895144">aov</rs> function from the <rs id="a12895144" type="software" subtype="environment">stats</rs> package: R&gt; mod_jpah2016 &lt;-aovperm(iapa ~bmic * condition * time + Error(id/(time)), + data = jpah2016, method = "Rd_kheradPajouh_renaud")</p>
<p>Resampling test using Rd_kheradPajouh_renaud to handle nuisance variables and 5000 permutations. This analysis reveals a significant p value for the effect of the interaction bmic:condition with a statistic F = 5.4269 , which lead to a permutation p value p = 0.0224 not far from the parametric one. For this example, the permutation tests backs the parametric analysis. The permutation distributions can be viewed using the <rs id="a12895145" type="software">plot</rs> function like in Figure 3.</p>
<p>attentionshifting_signal and attentionshifting_design are data provided in the <rs id="a12895146" type="software">permuco</rs> package. They come from an EEG recording of 15 participants watching images of either neutral or angry faces (Tipura, Renaud, and Pegna 2019). Those faces were shown at a different visibility: subliminal (16ms) and supraliminal (166ms) and were displayed to the left or to the right of a screen. The recording is at 1024 Hz for 800 ms. Time 0 is when the image appears (event-related potential or ERP). dataset contains the ERP of the electrode O1. The design of experiment is given in the attentionshifting_design dataset along with the laterality, sex, age, and 2 measures of anxiety of each subjects, see Table 3.</p>
<p>As almost any ERP experiment, the data is designed for a repeated measures ANOVA. Using the <rs id="a12895147" type="software">permuco</rs> package, we test each time points of the ERP for the main effects and the interactions of the variables visibility, emotion and direction while controlling for the FWER. We perform F tests using a threshold at the 95% quantile, the sum as a clustermass statistics and 5000 permutations. We handle nuisance variables with the method Rd_kheradPajouh_renaud:</p>
<p>This article presents recent methodological advances in permutations tests and their implementation in the <rs id="a12895148" type="software">permuco</rs> package. Hypotheses in linear models framework or repeated measures ANOVA are tested using several methods to handle nuisance variables. Moreover permutations tests can solve the multiple comparisons problem and control the FWER trough cluster-mass tests or TFCE, and the <rs id="a12895149" type="software">clusterlm</rs> function implements those procedures for the analysis of signals, like EEG data. Section 6 illustrates some real data example of tests that can be performed for regression, repeated measures ANCOVA and ERP signals comparison.</p>
<p>We hope that further developments of <rs id="a12895150" type="software">permuco</rs> expand cluster-mass tests to multidimensional adjacency (space and time) to handle full scalp ERP tests that control the FWER over all electrodes. An early version of the <rs id="a12895151" type="software" subtype="implicit">functions</rs> are already available in the the following repository: <rs id="a12895152" type="url" corresp="#a12895151">https://github.com/jaromilfrossard/permuco4brain</rs>. Another evolution will concern permutation procedures for mixed effects models to allows researchers to perform tests in models containing participants and stimuli specific random effects. Indeed, we plan to include in <rs id="a12895153" type="software">permuco</rs> the re-sampling test presented by Bürki, Frossard, and Renaud (2018) as they show that, first, using F statistic (by averaging over the stimuli) in combination with cluster-mass procedure increases the FWER and, secondely, that a re-sampling method based on the quasi-F statistic (Clark 1973;Raaijmakers, Schrijnemakers, and Gremmen 1999)</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f305078099"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:10+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>We provide detailed descriptions of data sources and methods in SI Appendix. For material services, we relied primarily on published data on production and prices (SI Appendix, section S1.1, Material Services). For regulating services, we relied on biophysical data from government sources and use the <rs id="a12972806" type="software">InVEST</rs> suite of models <rs id="a12972807" type="bibr">(73)</rs> to calculate the provision of services (SI Appendix, sections S1. 2-S1.8). We then applied a variety of market and nonmarket valuation methods to convert provision of services into monetary estimates of value. For nonmaterial services, in this case ecotourism, we applied travel cost methods using a survey on visitation and trip expenditures (SI Appendix, section S1.8 Nonmaterial Services). We also accounted for the monetary value of the ecosystem services generated in Qinghai to different beneficiaries (in Qinghai Province, other provinces in China, and globally).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f201640863"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:17+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>During coding it was observed that certain additions and changes frequently occurred and were common across the stories. As such a qualitative analysis was conducted using <rs id="a12951893" type="software">NVivo</rs> <rs id="a12951894" type="version" corresp="#a12951893">10</rs> (<rs id="a12951895" type="publisher" corresp="#a12951893">QSR International</rs>, 2012). See Table 3 for results. The common changes or additions considered were:</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f81949093"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T08:15+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The review was registered with <rs id="a12952011" type="software">PROSPERO</rs> (number CRD42012002193), and the study protocol was published (Piotrowska, Stride, &amp; Rowe, 2012). As such the methodology is briefly described here with a focus on protocol adjustments. To identify studies with relevant data, we searched PsycInfo, Web of Knowledge, Scopus, Medline, CINAHL, Applied Social Sciences Index and Abstracts (ASSIA), Sociological Abstracts, Worldwide Political Science Abstracts, National Criminal Justice Reference Service, EconLit, System for Information on Grey Literature in Europe, UK National Statistics, and Education Resources Information Center (ERIC) databases, seeking articles published between 1960 and 2012 that investigated socioeconomic status and child and adolescent antisocial behaviour. The full search criteria and key words are available in the protocol (Piotrowska et al., 2012).</p>
<p>Due to the expected heterogeneity in constructs and methodologies, both between and within studies, a random-effects model was used. Correlational effect sizes were converted to Fisher's z scale and the weighted average of transformed scores was calculated (Field &amp; Gillett, 2010;Hedges &amp; Vevea, 1998). Analyses were performed in <rs id="a12951994" type="software">SPSS</rs> v <rs id="a12951995" type="version" corresp="#a12951994">20</rs> and <rs id="a12951996" type="software">R</rs> <rs id="a12951997" type="version" corresp="#a12951996">2.12.1</rs> <rs id="a12952012" type="bibr">(Field &amp; Gillett, 2009)</rs>. A forest plot created in <rs id="a12951999" type="software">MS Excel</rs> <rs id="a12952013" type="bibr">(Neyeloff, Fuchs, &amp; Moreira, 2012)</rs>, and the heterogeneity between studies was assessed using the Q statistic and I 2 index.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f395000403"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T09:17+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Tonsillar immune cells (n=7) or FACS-sorted MBCs (n=6) single-cell libraries were generated with the 10x Genomics Chromium Single Cell 3' (v2; n=1) or 5' and V(D)J (v1; n = 6) assays (TableS1) prior to sequencing on the Illumina NextSeq500 with 26/8/134 bp (scRNA) or 155/8/155 bp (scVDJ) read configurations. scRNA-seq basecall files were processed with <rs id="a12883059" type="software">CellRanger</rs> (v <rs id="a12883060" type="version" corresp="#a12883059">3.0.0</rs>) to provide FASTQ files for mapping to GRCh38 (release 92) to produce gene-by-cell expression matrices. scVDJ datasets were processed with <rs id="a12883061" type="software" subtype="environment">CellRanger</rs> <rs id="a128830610" type="software" subtype="component" corresp="#a12883061">vdj</rs> using the refdata-cellranger-vdj-GRCh38-alts-ensembl-2.0.0 reference. Incomplete or low quality IgH contigs, or those with insufficient coverage of constant regions to ensure accurate isotype assignment between closely related subclasses, were filtered from subsequent analyses.</p>
<p>Bulk VDJ sequencing reads were processed with <rs id="a12883062" type="software">pRESTO</rs> (v <rs id="a12883063" type="version" corresp="#a12883062">0.5.10</rs>) (<rs id="a12883064" type="bibr">59</rs>) and combined with scVDJderived IgH sequences (Supplementary Methods). Briefly, reads with mean Phred scores &gt;25 underwent UMI alignment using <rs id="a12883065" type="software">MUSCLE</rs> (v <rs id="a12883066" type="version" corresp="#a12883065">3.8.31</rs>) (<rs id="a12883067" type="bibr">60</rs>) and UMIs with &gt;3 unique reads were used to assemble consensus VDJ sequences. Duplicate sequences within each biological sample were collapsed before integrating with filtered scVDJ contigs from <rs id="a12883068" type="software">cellranger</rs>. All IgH sequences were annotated with <rs id="a12883069" type="software" subtype="component" corresp="#a12883070">AssignGenes.py</rs> (<rs id="a12883070" type="software" subtype="environment">ChangeO</rs> v <rs id="a12883071" type="version" corresp="#a12883070">0.4.5</rs>) (<rs id="a12883072" type="bibr">61</rs>) and <rs id="a12883073" type="software">IgBLAST</rs> (v <rs id="a12883074" type="version" corresp="#a12883073">1.12.0</rs>) (<rs id="a12883075" type="bibr">62</rs>) before correction of ambiguous V gene assignments using <rs id="a12883076" type="software">TIgGER</rs> (v <rs id="a12883077" type="version" corresp="#a12883076">0.3.1</rs>) (<rs id="a12883078" type="bibr">63</rs>). Clonally-related sequences were identified using <rs id="a12883079" type="software">DefineClones.py</rs> with nearest neighbour distance threshold determined by <rs id="a12883080" type="software" subtype="component" corresp="#a12883081">distToNearest</rs> (<rs id="a12883081" type="software" subtype="environment">Shazam</rs> v<rs id="a12883082" type="version" corresp="#a12883081">0
.1.11</rs>) (<rs id="a12883083" type="bibr">61</rs>). Germline sequences were inferred using <rs id="a12883084" type="software">CreateGermlines.py</rs> and SHM frequencies calculated with <rs id="a12883085" type="software">observedMutations</rs>. SHM frequencies &gt;0.02 were annotated as "High", 0-0.02 as "Low" and 0 as "None". For bulk BCR repertoire analysis, scVDJ sequences were excluded, providing ~1.5 million high-confidence unique IgH sequences. Lineage trees of expanded clones were constructed via maximum parsimony using <rs id="a12883086" type="software" subtype="component" corresp="#a12883087">buildPhylipLineage</rs> (<rs id="a12883087" type="software" subtype="environment">Alakazam</rs> v<rs id="a12883088" type="version" corresp="#a12883087">0.2.11</rs>) (<rs id="a12883089" type="bibr">61</rs>).</p>
<p>To quantify class switch hierarchies, the observed frequency of direct edges between unique sequences of different isotypes were counted and normalised to the frequency of such transitions expected by chance (calculated by iterative (k=100) sampling of isotype frequencies). scVDJ sequences were integrated with scRNA-seq datasets as described previously (64). Diversity analyses were performed with <rs id="a12883090" type="software">Alakazam</rs>. Mean clonal diversity scores, SHM and isotype frequencies for each donor were compared using Student's t test.</p>
<p><rs id="a12883091" type="software">Cellranger</rs> expression matrices were used to quantify mitochondrial percentages and generate summed Ig and TCR VDJ counts before these genes were removed before processing with <rs id="a12883092" type="software">Seurat</rs> (v <rs id="a12883093" type="version" corresp="#a12883092">3.0.3</rs>) (<rs id="a12883094" type="bibr">65</rs>) with log transformation, normalisation, cell cycle prediction and variable gene identification (Supplementary Methods). A preliminary integration of unsorted immune samples or sorted MBC samples was performed using <rs id="a12883095" type="software">FindIntegrationAnchors</rs> and <rs id="a12883096" type="software">IntegrateData</rs> (3000 genes) regressing out cell cycle scores and mitochondrial gene expression, principle component analysis (PCA) and preliminary clustering. One preliminary cluster enriched with high frequency of predicted doublets from <rs id="a12883097" type="software">DoubletFinder</rs> (v <rs id="a12883098" type="version" corresp="#a12883097">2.0.1</rs>) (<rs id="a12883099" type="bibr">66</rs>) was removed. Following this quality control, all samples were integrated using the unsorted immune samples as reference with 4000 highly variable genes before regressing cell cycle and mitochondrial gene expression, PCA and identifying broad cell type lineages (B cell, T cell and non-lymphoid cells) which were then reclustered separately. Clusters were manually annotated using differential gene markers from <rs id="a12883100" type="software">FindAllMarkers</rs> and scVDJ antibody features where relevant.</p>
<p><rs id="a12883101" type="software">Uniform Manifold Approximation and Projection (UMAP)</rs> was used for visualisation.</p>
<p><rs id="a12883102" type="software">Cellranger</rs> gene expression matrices from human lymph nodes (n=7) and spleen (n=1) (64,67) were processed as above, before integration with the unsorted tonsillar immune cell gene expression objects using <rs id="a12883103" type="software" subtype="component" corresp="#a12883105">FindIntegrationAnchors</rs> and <rs id="a12883104" type="software" subtype="component" corresp="#a12883105">IntegrateData</rs> (<rs id="a12883105" type="software" subtype="environment">Seurat</rs>) with the unsorted tonsillar immune samples as reference with 4000 highly variable genes before regressing cell cycle and mitochondrial gene expression, PCA and unbiased clustering. Clusters were annotated based on a consensus of previous cell type annotations and confirmed by differential gene expression analysis. Processed scVDJ metadata from mesenteric lymph nodes (n=2) were obtained from (64). Lymph node and spleen memory B cells (B_MBC and B_MBC_FCRL4) were annotated with high resolution tonsillar memory B cell subsets using <rs id="a12883106" type="software" subtype="component" corresp="#a12883108">FindTransferAnchors</rs> and <rs id="a12883107" type="software" subtype="component" corresp="#a12883108">TransferData</rs> with 3000 variable features (<rs id="a12883108" type="software" subtype="environment">Seurat</rs>).</p>
<p>Differential gene expression for antibody class-specific or somatic hypermutation frequency for GC B cells or class-switched MBCs was performed using <rs id="a12883109" type="software">FindAllMarkers</rs> with Benjamini-Hochberg false discovery rate (FDR) correction. Genes were deemed significantly different if FDR &lt;0.05, average log fold change &gt;0.1 and the gene was detected in &gt;20% of cells in that group. <rs id="a128831090" type="software">Ingenuity Pathway Analysis</rs> (<rs id="a128831091" type="publisher" corresp="#a128831090">Qiagen</rs>) was performed using avg_logFC values of all genes significantly enriched in at least one class.</p>
<p>Gene ontology enrichment analyses were otherwise performed with <rs id="a12883110" type="software">Metascape</rs> (<rs id="a12883111" type="bibr">68</rs>). Single-cell gene signature scoring was performed with <rs id="a12883112" type="software">AUCell</rs> (v <rs id="a12883113" type="version" corresp="#a12883112">1.5.5</rs>) (<rs id="a12883114" type="bibr">69</rs>), including a manually curated shortlist of CSR-related genes (4), high and low affinity GC B cells (GSE73729) (11) and anti-IgM stimulation of B cells (GSE41176) (19). Affinity signatures were derived by quantifying RNA-seq transcript counts using <rs id="a12883115" type="software">Salmon</rs> (v <rs id="a12883116" type="version" corresp="#a12883115">1.0.0</rs>) (<rs id="a12883117" type="bibr">70</rs>), collapsing protein-coding transcripts with <rs id="a12883118" type="software">tximport</rs> (v <rs id="a12883119" type="version" corresp="#a12883118">1.10.1</rs>) (<rs id="a12883120" type="bibr">71</rs>), identifying significant gene expression differences using <rs id="a12883121" type="software">DESeq2</rs> (v <rs id="a12883122" type="version" corresp="#a12883121">1.22.2</rs>) (<rs id="a12883123" type="bibr">72</rs>) with a threshold of fold change &gt; 1.5 and FDR &lt; 0.05 and converting mouse gene IDs to human with <rs id="a12883124" type="software">bioMart</rs> (v <rs id="a12883125" type="version" corresp="#a12883124">2.38.0</rs>) (<rs id="a12883126" type="bibr">73</rs>). Genes upregulated following α-IgM treatment were identified from microarray data with Geo2R (74). preGC and FCRL3 high signatures used the top 50 most significantly enriched genes per cluster. Unless indicated otherwise, Wilcoxon Ranked Signed Sum test was used to test for significant differences.</p>
<p>To examine the expression of ligand-receptor pairs between different scRNA-seq clusters raw count matrices were analysed with "<rs id="a12883127" type="software" subtype="component" corresp="#a12883128">statistical_analysis</rs>" option of <rs id="a12883128" type="software" subtype="environment">CellPhoneDB</rs> (v<rs id="a12883129" type="version" corresp="#a12883128">2.0.6</rs>) (<rs id="a12883130" type="bibr">75</rs>). The number of unique significant ligand-receptor co-expression pairs between each cell type were determined and interactions of interest visualised by the means of average expression of gene 1 in cell type 1 and gene 2 in cell type 2 indicated by colour and p values indicated by circle size.</p>
<p>Spliced and unspliced transcripts were quantified for tonsil immune samples with <rs id="a12883131" type="software">velocyto</rs> (v<rs id="a12883132" type="version" corresp="#a12883131">0.17.10</rs>) (<rs id="a12883133" type="bibr">17</rs>) and integrated with raw counts of annotated B cell scRNA-seq in <rs id="a12883134" type="software">Scanpy</rs> (v <rs id="a12883135" type="version" corresp="#a12883134">1.4</rs>) (<rs id="a12883136" type="bibr">76</rs>) using <rs id="a12883137" type="software">scVelo</rs> (v <rs id="a12883138" type="version" corresp="#a12883137">0.1
.23</rs>) (<rs id="a12883139" type="bibr">18</rs>). <rs id="a12883140" type="software">Velocyto</rs>-derived counts were processed, filtered and normalised prior to velocity estimation using a dynamical model with <rs id="a12883141" type="software">scVelo</rs>. Velocities were projected and visualised onto <rs id="a12883142" type="software">UMAP</rs> embedding. Velocity-based pseudotime reconstruction was performed using latent time recovery of single-cell velocities with <rs id="a12883212" type="software">tl.recover_latent_time</rs> and <rs id="a12883213" type="software">tl.velocity_pseudotime</rs> of the Naïve, Activated, preGC and LZ GC B cell clusters. Dynamic gene expression changes were examined using <rs id="a12883214" type="software">tl.rank_velocity_genes</rs> for velocity-based sub-clustering (res=1) and top 200 genes per sub-cluster (ribosomal genes removed) were collapsed to unique genes for heatmap visualisation with smoothed scores. Gene expression or signature scores were quantified across pseudotime as smoothed normalised counts with geom_smooth() ±95% CI. For pseudotemporal analysis of the continuum of GC B cell states (Fig4), all GC B cell clusters were analysed with diffusion-based pseudotime (<rs id="a12883215" type="software" subtype="component" corresp="#a12883143">tl.dpt</rs>, <rs id="a12883143" type="software" subtype="environment">Scanpy</rs>) independent of RNA velocity using default settings. Partition-based graph abstraction (PAGA) analysis was performed with <rs id="a12883144" type="software">Scanpy</rs> using default settings (threshold = 0.05) with all clusters except for MBCs due to their close transcriptional similarity with naïve B cells.</p>
<p>All reads mapped to the IgH locus (chr14:105540180-105879151) were quantified with <rs id="a12883145" type="software">dropEst</rs> (v <rs id="a12883146" type="version" corresp="#a12883145">0.8.6</rs>) (<rs id="a12883147" type="bibr">77</rs>) against a custom GTF file containing I promoter germline transcript coordinates. Counts were log10-normalised and scaled in <rs id="a12883148" type="software">Seurat</rs> before class-specific counts were summed.</p>
<p>Cryopreserved cells were thawed, washed and blocked with human FcR Blocking Reagent (Miltenyi Biotec) then stained with a panel of fluorophore conjugated antibodies (Table S10) and DAPI (Sigma) to discriminate live and dead cells. Samples were run on a Cytek Aurora spectral flow cytometer using SpectroFlo (Cytek) and unmixed before analysis. Data were analysed using <rs id="a12883149" type="software">FlowJo</rs> v <rs id="a12883150" type="version" corresp="#a12883149">10</rs> ( <rs id="a12883151" type="publisher" corresp="#a12883149">Treestar</rs>) and gates were set using fluorescence minus one (FMO) controls.</p>
<p>Formalin-fixed paraffin-embedded tonsil samples were deparaffinized in xylene and rehydrated through a series of ethanols to water. Endogenous peroxidase was blocked with 3% hydrogen peroxide before heat-mediated antigen retrieval with a citrate-based unmasking buffer (Vector Labs) at 120°C. 3 µm sections were incubated for 40 min at RT with anti-APE1 (1:1000; HPA002564; Sigma) or anti-FCRL3 (1:100; HPA048022; Sigma) before using the Super-sensitive-Polymer HRP system (Biogenex) with purple chromogen VIP (Vector Labs) and hematoxylin as a nuclear counterstain. Slides were scanned (Pannoramic250 Flash) before soaking in xylene to de-coverslip before rehydration through ethanol to water. De-staining and stripping of primary antibodies and heat-labile chromogen was achieved by a subsequent round of heat-mediated antigen retrieval. Anti-CD20 (1:500; M0755; Dako) was incubated for 40 min at RT, followed by detection, visualization and scanning as before. Negative controls were performed by treating sequential sections as above but without anti-CD20 staining to confirm complete stripping. Images were prepared using <rs id="a12883152" type="software">CaseViewer</rs> (<rs id="a12883153" type="publisher" corresp="#a12883152">3DHistTech</rs>). Table S1. Sample metadata for scRNA-seq libraries and frequencies of immune cell clusters in scRNAseq datasets.</p>
<p>945 946 metadata for that cell was ascribed as "Multi". IgH diversity analyses were performed using the <rs id="a12883154" type="software" subtype="component" corresp="#a12883156">rarefyDiversity</rs> and <rs id="a12883155" type="software" subtype="component" corresp="#a12883156">testDiversity</rs> of <rs id="a12883156" type="software" subtype="environment">Alakazam</rs>. To assess clonal relationships between cell types, cooccurrence of expanded clone members between cell types was reported as a binary event for each clone that contained a member within two different cell types in either single-cell or bulk repertoires.</p>
<p>Counts of individual IgH constant region genes were also summed together (IgG1-4, and IgA1-A2) and removed. Modified gene-by-cell matrices were then used to create Seurat objects for each sample using <rs id="a12883158" type="software">Seurat</rs> (v <rs id="a12883159" type="version" corresp="#a12883158">3.0.3</rs>) (<rs id="a12883160" type="bibr">65</rs>), removing genes expressed in fewer than 3 cells. Cell barcodes with &lt;1000 or &gt;60000 UMIs and &lt;500 or &gt;7000 genes detected were removed, as were cell barcodes with &gt;30% mitochondrial reads. Individual matrices were then log transformed, normalised by a factor of 10000 prior to predicting cell cycle phases using the <rs id="a12883161" type="software">CellCycleScoring</rs> command and identifying the 3000 most variable genes within each sample using the "vst" method. A preliminary integration of all unsorted immune cells or all sorted memory B cell datasets together was performed using <rs id="a12883162" type="software">FindIntegrationAnchors</rs> and <rs id="a12883163" type="software">IntegrateData</rs> (3000 genes) before regressing out cell cycle scores and mitochondrial gene expression, performing principle component analysis (PCA) and preliminary clustering and cell type annotation. One preliminary cluster was identified to be enriched with predicted doublets based on the results from <rs id="a12883164" type="software">DoubletFinder</rs> (v <rs id="a12883165" type="version" corresp="#a12883164">2.0.1</rs>) (<rs id="a12883166" type="bibr">66</rs>), and a small number of cell barcodes with co-expression of B/T/non-lymphoid markers were removed. Following the removal of poor quality cell barcodes based on these preliminary analyses, all normalised count matrices were integrated together using the unsorted immune samples as a reference with 4000 highly variable genes before scaling the integrated data and regressing cell cycle and mitochondrial gene expression, running PCA and identifying broad cell type lineages (B cell, T cell and non-lymphoid cells) using a broad resolution for clustering. These lineages were then separated for more detailed annotation by recomputing the</p>
<p>Raw sequencing reads from bulk VDJ libraries were processed to generate UMI-collapsed consensus VDJ sequences using <rs id="a12883167" type="software">pRESTO</rs> (v <rs id="a12883168" type="version" corresp="#a12883167">0.5.10</rs>) (<rs id="a12883169" type="bibr">59</rs>). Paired-end sequencing reads with mean Phred quality scores less than 25 were removed, and remaining sequences were annotated and trimmed for PCR primer and UMI sequences. UMI barcodes were then filtered by length and the presence of ambiguous nucleotides, prior to UMI alignment using <rs id="a12883170" type="software">MUSCLE</rs> (v <rs id="a12883171" type="version" corresp="#a12883170">3.8.31</rs>) (<rs id="a12883172" type="bibr">60</rs>). To correct for sequencing or other errors, we generated consensus sequences from UMIs with at least 3 unique sequencing reads required, prior to assembly of paired-end UMI consensus sequences into a single VDJ contig and annotation of constant region isotype using <rs id="a12883173" type="software">MaskPrimers.
py</rs> align to correct for primer misalignment.</p>
<p>Duplicate VDJ sequences within each subset were collapsed using <rs id="a12883174" type="software" subtype="component" corresp="#a12883176">CollapseSeq.py</rs> before VDJ gene assignment and functional annotation with <rs id="a12883175" type="software" subtype="component" corresp="#a12883176">AssignGenes.py</rs> (<rs id="a12883176" type="software" subtype="environment">ChangeO</rs> v <rs id="a12883177" type="version" corresp="#a12883176">0.4.5</rs>) (<rs id="a12883178" type="bibr">61</rs>) and <rs id="a12883179" type="software">IgBLAST</rs> (v <rs id="a12883180" type="version" corresp="#a12883179">1.12.0</rs>) (<rs id="a12883181" type="bibr">62</rs>).</p>
<p>Following initial quality control, all single-cell VDJ sequences were combined together with bulk BCR repertoire sequences from the same donor for subsequent processing. IgH sequences were annotated using <rs id="a12883182" type="software">AssignGenes.py</rs> and <rs id="a12883183" type="software">IgBLAST</rs> before isotype class assignment prior to correction of ambiguous V gene assignments using <rs id="a12883184" type="software">TIgGER</rs> (v <rs id="a12883185" type="version" corresp="#a12883184">0.3.1</rs>) (<rs id="a12883186" type="bibr">61</rs>,<rs id="a12883187" type="bibr">63</rs>). Clonally-related IgH sequences were identified using <rs id="a12883188" type="software" subtype="component" corresp="#a12883189">DefineClones.py</rs> ( <rs id="a12883189" type="software" subtype="environment">ChangeO</rs>) with a nearest neighbour distance threshold of 0.0818, as determined by the mean 99% confidence interval of all 8 donors with <rs id="a12883190" type="software" subtype="component" corresp="#a12883191">distToNearest</rs> (<rs id="a12883191" type="software" subtype="environment">Shazam</rs> v<rs id="a12883192" type="version" corresp="#a12883191">0.1.11</rs>) (<rs id="a12883193" type="bibr">61</rs>). <rs id="a12883194" type="software">CreateGermlines.py</rs> was then used to infer germline sequences for each clonal family and <rs id="a12883195" type="software">observedMutations</rs> was used to calculate somatic hypermutation frequencies for each IgH sequence.</p>
<p>Sequences with somatic hypermutation frequencies greater than 0.02 were annotated as "High" mutation levels, those between 0 and 0.02 as "Low" mutation levels and 0 as "None". For bulk BCR repertoire analysis in Figure 1, single-cell VDJ sequences were excluded, providing ~1.5 million highconfidence and unique IgH sequences, with a median of 14 UMIs per sequence, a median of 28,918 unique sequences per donor per subset and approximately 96-99% of these sequences annotated as functional by <rs id="a12883196" type="software">IgBlast</rs>. To quantify antibody class switch hierarchies, lineage trees for expanded clonotypes were constructed via maximum parsimony using <rs id="a12883197" type="software" subtype="component" corresp="#a12883198">buildPhylipLineage</rs> (<rs id="a12883198" type="software" subtype="environment">Alakazam</rs> v <rs id="a12883199" type="version" corresp="#a12883198">0.2.11</rs>) (<rs id="a12883200" type="bibr">61</rs>). The observed frequency of direct edges between unique sequences of different isotypes were counted and expressed relative to the frequency of such transitions expected by chance (calculated by iterative (k=100) random sampling of isotype frequencies) within each B cell subsets.</p>
<p>Single-cell VDJ analysis was performed broadly as described previously (64). Briefly, the number of quality filtered and annotated IgH, IgK or IgL were determined per unique cell barcode prior to integration with single-cell gene expression objects. If more than one contig per chain was identified, PCA (RunPCA), nearest neighbour graph ( <rs id="a12883201" type="software">FindNeighbors</rs>) and unbiased clustering ( <rs id="a12883202" type="software">FindClusters</rs>).</p>
<p><rs id="a12883203" type="software">Uniform Manifold Approximation and Projection (UMAP)</rs> was used to visualise integrated and lineagespecific datasets. B cells were annotated with scVDJ metadata from the integrated repertoire analysis detailed above and features such as isotype frequency, SHM levels and clonal properties were used to improve confidence of cell type annotation (such as between naïve and MBC clusters).</p>
<p>To evaluate potential cell-cell communication, we used <rs id="a12883204" type="software">CellPhoneDB</rs> (v<rs id="a12883205" type="version" corresp="#a12883204">2.0.6</rs>) (<rs id="a12883206" type="bibr">75</rs>) to examine the expression of ligand-receptor pairs between different scRNA-seq clusters. Briefly, we exported raw gene count matrices from <rs id="a12883207" type="software">Seurat</rs>, converted gene IDs to <rs id="a12883208" type="software">Ensembl</rs> IDs using <rs id="a12883209" type="software">bioMart</rs>. We re-annotated all non-lymphoid cell type clusters as antigen-presenting cells (APCs), naïve and effector T cell groups by CD4 or CD8 expression, Treg and Tfr as "Treg" and rare GC subsets (prePB and FCRL3 high ) as "GC" and exported cell type metadata for use with raw count data using the "<rs id="a12883210" type="software" subtype="component" corresp="#a12883211">statistical_analysis</rs>" command of <rs id="a12883211" type="software" subtype="environment">CellPhoneDB</rs> with database v2.0.0. The number of unique significant ligand-receptor co-expression pairs (putative interactions; p value&lt;0.05) between each cell type was then counted and visualised as a heatmap, while exemplar interacting pairs were visualised by calculating mean average expression level of gene 1 in cell type 1 and gene 2 in cell type 2 are indicated by colour and p values indicated by circle size.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f478783916"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:46+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>In the synthetic data, R 0 was set to 2.0 initially, then to 0.8 and 1.15, to simulate the adoption and later the partial lifting of public health interventions. To mimic estimation in real-time, we truncated the time series at t = 150, before the end of the epidemic. Estimates from the methods of Wallinga and Teunis and Cori et al. were obtained using the <rs id="a12894682" type="software" subtype="environment">R</rs> package <rs id="a12894683" type="software" subtype="component" corresp="#a12894682">EpiEstim</rs> <rs id="a12953672" type="bibr">[20]</rs>. Estimates based on the method of Bettencourt and Ribeiro were obtained by translating <rs id="a12894685" type="software" subtype="implicit">code</rs> from [<rs id="a12894686" type="bibr">6</rs>,<rs id="a12894687" type="bibr">23</rs>] to the <rs id="a12894688" type="language">Stan</rs> language <rs id="a12953673" type="bibr">[24]</rs>. We initially assumed all infections were observed. Unless otherwise noted, the smoothing window was set to 1 day (effectively, estimates were not smoothed). To August 27, 2020 3/25 mimic the timescale of observations, we used daily time steps when generating synthetic data and performing analyses.</p>
<p> <rs id="a12894690" type="software">EpiEstim</rs> <rs id="a12953674" type="bibr">[20]</rs> allows users to account for uncertainty in the mean and standard deviation of the generation interval by resampling over a range of plausible values [14,20]. Similarly, Bayesian methods such as <rs id="a12894692" type="software">EpiNow2</rs> <rs id="a12953675" type="bibr">[41]</rs> and the <rs id="a12894694" type="software">rt.live</rs> adaptation <rs id="a12953676" type="bibr">[6]</rs> of the Bettencourt and Ribeiro method allow users to specify the prior variance of the mean and standard deviation. Uncertainty around an incorrect value can August 27, 2020 9/25 widen the resulting 95% interval but will not shift the assumed central value toward the truth, and will not correct bias in the central R t estimates.</p>
<p>Joint estimation of both R t and the serial interval is possible, depending on data quality and magnitude of R t [12,42,43], and the <rs id="a12894696" type="software">EpiEstim</rs> [<rs id="a12894697" type="bibr">15</rs>,<rs id="a12894698" type="bibr">20</rs>] package provides an off-the-shelf option for joint estimation. However, these off-the-shelf methods should be used with caution, as they estimate the observed serial interval, not the intrinsic generation interval, and do not account for changes over time in behavior or susceptibility.</p>
<p>Obtaining temporally accurate R t estimates thus requires assumptions about lags from infection to observation. If the distribution of delays can be estimated, then R t can be estimated in two steps: first by inferring the incidence time series from observations and then by inputting the inferred time series into an R t estimation method. Alternatively, the unobserved time series could be inferred simultaneously with R t or treated as a latent state. Such methods are now under development and available in a development version of the <rs id="a12894699" type="software" subtype="environment">R</rs> package <rs id="a12894700" type="software" subtype="component" corresp="#a12894699">EpiNow2</rs> [<rs id="a12894701" type="bibr">41</rs>,<rs id="a12894702" type="bibr">44</rs>].</p>
<p>Shifting inputs or R t estimates by a fixed amount also fails to account for realistic uncertainty in the true mean delay, which will not be known exactly and might change over time. development. Given a known delay distribution, one potential solution is to infer the unlagged signal using maximum-likelihood deconvolution. This method was applied to AIDS cases, which feature long delays from infection to observation [47], and in the reconstruction of incidence from mortality times series for the 2009 H1N1 pandemic [45]. It is now being applied to
COVID-19 [8,48]. Fig. 5 shows an example of deconvolution applied following the methods in [45]. The method of [47] is implemented in the <rs id="a12894703" type="software" subtype="component" corresp="#a12894704">backprojNP</rs> function within the <rs id="a12894704" type="software" subtype="component" corresp="#a12894705">surveillance</rs> <rs id="a12894705" type="software" subtype="environment">R</rs> package [<rs id="a12894706" type="bibr">49</rs>,<rs id="a12894707" type="bibr">50</rs>]. In principle, deconvolution can more accurately estimate the latent time series than temporal shifting or backward convolution, but the method is sensitive to misspecification of the mean, variance or form of the delay distribution, and the stringency of the stopping condition of the deconvolution algorithm. It can also be difficult to quantify uncertainty in the deconvolved time series [41] and to implement deconvolution while adjusting for right truncation.</p>
<p>A potential alternative to deconvolution is R t estimation models that include forward delays to observation in the inference process or that treat the time series of infections as latent states. Such methods are in development within the <rs id="a12894708" type="software" subtype="environment">R</rs> package <rs id="a12894709" type="software" subtype="component" corresp="#a12894708">EpiNow2</rs> <rs id="a12953677" type="bibr">[41]</rs>. An additional advantage of inferring the time series of infections jointly with R t is seamless integration of various sources of uncertainty, e.g., in R t and reporting. By comparison, the two-step approach of first transforming the observed time series and then calculating R t requires users to propagate uncertainty from the back-calculation step into the R t estimation step. A final advantage of latent state methods is that they could in theory facilitate inference from multiple data streams simultaneously. For example, by assuming that cases, hospitalizations, and deaths all arise from a common infection process, these methods might be able to infer the incident time series of infections more accurately and precisely, potentially while also estimating delays and changes in ascertainment for specific data sources (e.g., outpatient cases).</p>
<p>We tested the accuracy of several methods for R t estimation in near real-time and recommend the methods of Cori et al. [14], which are currently implemented in the <rs id="a12894711" type="software" subtype="environment">R</rs> package <rs id="a12894712" type="software" subtype="component" corresp="#a12894711">EpiEstim</rs> <rs id="a12953678" type="bibr">[20]</rs>. The Cori et al. method estimates the instantaneous rather than the case reproductive number and is conceptually appropriate for near real-time estimation. The method uses minimal parametric assumptions about the underlying epidemic process and can accurately estimate abrupt changes in the instantaneous reproductive number using ideal, synthetic data.</p>
<p>Most epidemiological data are not ideal, and statistical adjustments are needed to obtain accurate and timely R t estimates. First, to obtain timely and temporally accurate R t estimates, considerable pre-processing is needed to infer the underlying time series of infections (i.e., transmission events) from delayed observations and to adjust for right truncation. Best practices for this inference are still under investigation, especially if the delay distribution is uncertain. The smoothing window must also be chosen carefully, potentially adaptively, and daily counts must be sufficiently high for changes in R t to be resolved on short timescales. To avoid biases in R t estimates, the generation interval distribution must be estimated and specified accurately. Finally, to avoid false precision in R t , uncertainty arising from delays to observation, from adjustment for right truncation, and from imperfect observation must be propagated. The functions provided in the <rs id="a12894714" type="software">EpiEstim</rs> package quantify uncertainty arising from the R t estimation model but currently not from uncertainty arising from imperfect observation or delays.</p>
<p>Work is ongoing to determine how best to infer infections from observations and to account for all relevant forms of uncertainty when estimating R t . Some useful extensions of the methods provided in <rs id="a12894715" type="software" subtype="component" corresp="#a12894716">EpiEstim</rs> have already been implemented in the <rs id="a12894716" type="software" subtype="environment">R</rs> package <rs id="a12894717" type="software" subtype="component" corresp="#a12894716">EpiNow2</rs> [<rs id="a12894718" type="bibr">41</rs>,<rs id="a12894719" type="bibr">44</rs>], and further updates to this package are planned as new best practices become established.</p>
<p>All <rs id="a12894720" type="software" subtype="implicit">code</rs> for analysis and figure generation is available at <rs id="a12894721" type="url" corresp="#a12894720">https://github.com/cobeylab/Rt estimation</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f396674309"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T12:58+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Masonry structures represent the highest proportion of building stock worldwide. Currently, the structural condition of such structures is predominantly manually inspected which is a laborious, costly and subjective process. With developments in computer vision, there is an opportunity to use digital images to automate the visual inspection process. The aim of this study is to examine deep learning techniques for crack detection on images from masonry walls. A dataset with photos from masonry structures is produced containing complex backgrounds and various crack types and sizes. Different deep learning networks are considered and by leveraging the effect of transfer learning crack detection on masonry surfaces is performed on patch level with 95.3% accuracy and on pixel level with 79.6% F1 score. This is the first implementation of deep learning for pixel-level crack segmentation on masonry surfaces. <rs id="a12953692" type="software" subtype="implicit">Codes</rs>, data and networks relevant to the herein study are available in: <rs id="a12953693" type="url" corresp="#a12953692">github.com/dimitrisdais/crack_detection_CNN_masonry</rs>.</p>
<p>The aim of this paper is to examine different DL techniques for crack detection on images from masonry walls. Recent developments in DL for crack detection and successful techniques are highlighted in Section 2.1 while studies for vision-based assessment on masonry surfaces found in the literature are presented in Section 2.2. In order to address the lack of data in the literature, a dataset with photos from masonry structures is produced containing complex backgrounds and various crack types and sizes (Section 3). Since for masonry structures little work has been done for crack detection it is deemed beneficial to train networks both for patch classification (Section 4) and pixel-level segmentation (Section 5) in order to examine the efficacy of different techniques and broadcast the feasibility of DL methods on crack detection for masonry surfaces. To the authors' best knowledge, this study is the first implementation of DL for pixel-level crack segmentation on masonry surfaces. The technique of transfer learning is also leveraged in order to improve the performance of the DL networks for crack detection on patch and pixel level. Finally, a comparative study is performed where a segmentation network trained on masonry images is tested on photos with cracks taken from concrete surfaces in order to evaluate the ability of CNNs to generalize over different materials (section 6). <rs id="a12953694" type="software" subtype="implicit">Codes</rs>, data and networks relevant to the herein study can be found in the GitHub repository: <rs id="a12953695" type="url" corresp="#a12953694">gi
thub.com/dimitrisdais/crack_detection_CNN_masonry</rs>.</p>
<p>Different state of the art CNNs pretrained on ImageNet (1.2 million images with 1000 categories) were examined herein for their efficacy to classify images from masonry surfaces on patch level as crack or noncrack. The considered networks were: <rs id="a12897548" type="software">VGG16</rs> <rs id="a12953698" type="bibr">[59]</rs>, <rs id="a12897560" type="software">MobileNet</rs> <rs id="a12953699" type="bibr">[60]</rs>, <rs id="a12897562" type="software">MobileNetV2</rs> <rs id="a12953700" type="bibr">[61]</rs>, <rs id="a12897615" type="software">InceptionV3</rs> <rs id="a12953701" type="bibr">[62]</rs>, <rs id="a12897625" type="software">DenseNet121</rs> <rs id="a12953702" type="bibr">[63]</rs>, <rs id="a12897630" type="software">DenseNet169</rs> <rs id="a12953703" type="bibr">[63]</rs>, <rs id="a12897632" type="software">ResNet34</rs> <rs id="a12953704" type="bibr">[64]</rs>, <rs id="a12897641" type="software">ResNet50</rs> <rs id="a12953705" type="bibr">[64]</rs>. The configuration of <rs id="a12897633" type="software">ResNet34</rs> and the pre-trained weights were obtained from Yakubovskiy <rs id="a12953706" type="bibr">[65]</rs>, while for the rest of the networks the configuration and the weights were extracted from Keras <rs id="a12953707" type="bibr">[66]</rs>. The details of the different networks are shown in Table 1. All the <rs id="a12953696" type="software">models</rs> were deposited in the GitHub repository: <rs id="a12953697" type="url" corresp="#a12953696">github.com/dimitrisdais/crack_detection_CNN_masonry</rs>.</p>
<p>At this point the architecture of <rs id="a12897563" type="software">MobileNet</rs> is highlighted since it obtained the best results as will be shown below (Section 4.3). Mobile-Net is a lightweight network destined to run on computationally limited platforms; it achieved accuracy comparable to <rs id="a12897549" type="software">VGG16</rs> on ImageNet with e Epoch where the highest accuracy was obtained for the validation set.</p>
<p>only 1/30 of the computational cost and model size [67]. A standard convolution both filters and combines inputs into a new set of outputs in one step. <rs id="a12897565" type="software">MobileNet</rs> is based on depthwise separable convolutions which is a form of factorized convolutions (see Fig. 5); the depthwise convolution applies a single filter to each input channel and the pointwise convolution then applies a 1 × 1 convolution to combine the outputs of the depthwise convolution. This factorization (Fig. 5) has the effect of drastically reducing computation and model size. <rs id="a12897566" type="software">MobileNet</rs> comprises of multiple factorized layers with depthwise convolution, 1 × 1 pointwise convolution, batch normalization and ReLU activation (Fig. 6a) instead of layers of regular convolutions followed by batch normalization and ReLU activation (Fig. 6b). The MobileNet architecture has two hyper-parameters that is width and resolution multipliers in order to easily produce smaller versions of the network. Herein, for both hyperparameters the default value is selected, that is 1, which means than no shrinking is applied to the model [<rs id="a12953708" type="bibr">60</rs>,66].</p>
<p> <rs id="a12897568" type="software">MobileNet</rs> or networks that made use of depthwise separable convolution have been implemented in recent studies for crack detection. Single Shot MultiBox Detector [68], an object detection framework, was combined with <rs id="a12897570" type="software">MobileNet</rs> to detect different damage types on road surfaces [69]. <rs id="a12897571" type="software">MobilneNet</rs> performed as the encoder of a semantic segmentation network based on
DeepLab [18] for real-time tunnel crack analysis [70]. The depthwise separable convolution was used to reduce computational complexity and improve computational efficiency of image classification for crack detection [71]. Depthwise convolutions have been successfully used for pixel-level segmentation of cracks on concrete surfaces [19].</p>
<p>For the training of the image classification networks the 4057 crack and the 7434 non-crack patches of the masonry dataset are used. 60% and 40% of the patches are used for training and validation respectively. The networks are implemented on <rs id="a12897531" type="software">Keras</rs> <rs id="a12953709" type="bibr">[66]</rs>, a high-level neural network API, written in <rs id="a12897533" type="language" corresp="#a12897531">Python</rs> and by utilizing <rs id="a12897534" type="software">TensorFlow</rs> as back-end. The networks are run on a laptop with Intel i7 processor with 2.20 GHz, 16 GB RAM and
Nvidia GPU GeForce(R) RTX 2060 with 6 GB.</p>
<p>In this section the results from the trained networks for image classification are presented .The obtained metrics from the trained models on the validation set are enlisted in Table 1 for the epoch that the highest accuracy is reached for each case. While all the considered networks obtain high accuracy on the validation set, that is 88% or more,
Mobi-leNet outperforms the rest by scoring accuracy 95.3% (Table 1). In order to examine the benefit of transfer learning, <rs id="a12897572" type="software">MobileNet</rs> is also evaluated without pretraining with its weights randomly initialized [66]. Indeed, the accuracy of <rs id="a12897574" type="software">MobileNet</rs> drops from 95.3% to 89.0% which reveals that transfer learning offers a significant boost to the performance of the network. In more detail, when random initialization is considered, the ratio of TN remains high, that is 96.4%, however TP declines considerably from 89.8% to 75.8%. Consequently, without pretraining the network struggles to differentiate edges corresponding to the crack class and tends to label them as non-crack.</p>
<p>In Figs. 3 and4 representative images of the masonry dataset are presented. Based on the accuracy of the model it can concluded that the network learns rich features that allow for correct classifications on the dataset produced. A closer look to the performance of <rs id="a12897575" type="software">MobileNet</rs> is highlighted in the produced confusion matrix (Fig. 7). It is inferred that <rs id="a12897576" type="software">MobileNet</rs> excels in predicting correctly the non-crack case with only 1.6% error while the error in the crack class is higher, that is 10.2% of the crack images are classified as non-crack. Different cases of FP and FN predicted with <rs id="a12897577" type="software">MobileNet</rs> from the validation set are displayed in Figs. 8 and9 respectively. Part of a pipe (Fig. 8a), joints without mortar (Fig. 8b, g-h), edges around doors (Fig. 8e-f), and blurry or dark edges (Fig. 8c-d) are wrongly classified as cracks. Evidently, a further expansion of the masonry dataset should take into consideration a better representation of the cases that yielded FP so that the network will learn their features and correctly classify them. On the other hand, crack images taken with acute angle (Fig. 9a) or with great field of view capturing thin cracks (Fig. 9b-d) are misclassified. Moreover, there are cases of close-up images of thin (Fig. 9e-j) or well-shaped cracks (Fig. 9k-m), crack with missing mortar (Fig. 9n) and crack in dark background (Fig. 9o) that the network falsely negates them to the noncrack class.</p>
<p>FPN [32] is a typical model architecture to generate pyramidal feature representations for object detection. FPN is independent of the Fig. 7. Confusion matrix obtained with the <rs id="a12897578" type="software">MobileNet</rs> on the validation set. backbone network and its architecture makes it easily configurable to receive different CNNs as the backbone of the encoder. In particular, FPN adopts a convolutional architecture as its backbone, typically designed for image classification, and builds a feature pyramid with a bottom-up pathway, a top-down pathway and lateral connections. The high-level features, which are semantically strong but lower resolution, are upsampled and combined with higher resolution features to generate feature representations that are both high resolution and semantically strong. The upsampling layer repeats the rows and columns of the input features by 2 × 2 and fills in the new rows and columns by using the nearest neighbour algorithm [66]. The bottom-up pathway which is the feed-forward computation of the backbone CNN produces a feature hierarchy consisting of feature maps at several scales with a scaling step of 2. Layers producing output maps of the same size are considered in the same network stage and for each stage one pyramid level is defined. The top-down pathway obtains higher resolution features by upsampling by a factor of 2 spatially coarser, but semantically stronger, feature maps from higher pyramid levels. These features are then enhanced by element-wise addition with features from the bottom-up pathway which undergo a 1 × 1 convolutional layer to reduce channel dimensions. Further on, 3 × 3 convolutions are appended on each merged feature map and the produced maps from the different stages are concatenated. A schematic representation of FPN is displayed in Fig. 12.</p>
<p>The CNNs that were tested for image classification in Section 4 are utilized as the encoder for U-net and FPN in order to perform crack segmentation on pixel level this time. In particular, the considered networks are: <rs id="a12897550" type="software">VGG16</rs> <rs id="a12953710" type="bibr">[59]</rs>, <rs id="a12897580" type="software">MobileNet</rs> <rs id="a12953711" type="bibr">[60]</rs>, <rs id="a12897581" type="software">MobileNetV2</rs> <rs id="a12953712" type="bibr">[61]</rs>, <rs id="a12897623" type="software">Incep-tionV3</rs> <rs id="a12953713" type="bibr">[62]</rs>, <rs id="a12897626" type="software">DenseNet121</rs> <rs id="a12953714" type="bibr">[63]</rs>, <rs id="a12897631" type="software">DenseNet169</rs> <rs id="a12953715" type="bibr">[63]</rs>, <rs id="a12897634" type="software">ResNet34</rs> <rs id="a12953716" type="bibr">[64]</rs>, <rs id="a12897635" type="software">ResNet50</rs> <rs id="a12953717" type="bibr">[64]</rs>. It is noted that <rs id="a12897636" type="software">U-net</rs> is also considered as a standalone network configured as explained above (Fig. 11). For further reference, the models based on U-net and FPN will be called with the base-model followed by the backbone network, e.g. <rs id="a12897583" type="software">U-net-MobileNet</rs> uses <rs id="a12897584" type="software">U-Net</rs> as base-model with <rs id="a12897582" type="software">MobileNet</rs> as backbone. Moreover, apart from U-net, other networks found in the literature and performed well in crack segmentation are examined as well. In particular,
DeepLabv3+ [73],
DeepCrack [17], and FCN based on <rs id="a12897551" type="software">VGG16</rs> (will be referred to as <rs id="a12897552" type="software">FCN-VGG16</rs>) [22]. All the <rs id="a12953719" type="software" subtype="implicit">networks</rs> used in the herein study for segmentation are listed in Table 2 and can be found in the GitHub repository: <rs id="a12953718" type="url" corresp="#a12953719">github. com/dimitrisdais/crack_detection_CNN_masonry</rs>.</p>
<p>For the training of the segmentation networks the 4057 crack patches of the masonry dataset were used. In particular, 60% and 40% of the patches were used for training and validation respectively. The crack patches were fed to the networks along with pixel-level annotated labels. Similar to the classification networks, the segmentation models were implemented on <rs id="a12897536" type="software">Keras</rs> <rs id="a12953720" type="bibr">[66]</rs> by utilizing <rs id="a12897538" type="software">TensorFlow</rs> as back-end and were run on the same computing laptop (see Section 4.2 for details).</p>
<p>In this section the segmentation results from the trained networks are presented. The obtained metrics from the trained models on the validation set are shown in Table 2 for the epoch that the highest F1 score is reached for each case. From Table 2, a high value of recall does not necessarily mean high precision and vice versa. Thus, F1 score, the average between recall and precision, is deemed the most indicative metric to decide which networks perform better. Thus, U-net-<rs id="a12897585" type="software">MobileNet</rs> and <rs id="a12897616" type="software">FPN-InceptionV3</rs> attain the highest F1 score, that is 79.6%, and <rs id="a12897591" type="software">FPN-MobileNet</rs> follows with 79.5%.</p>
<p>Firstly, the effect of the loss function on the performance of the networks was evaluated. <rs id="a12897587" type="software">U-net-MobileNet</rs> was trained, apart from WCE, with CE, F1 score and focal loss as loss function. It is noted that similar results were extracted for the other networks but for brevity only results for <rs id="a12897589" type="software">U-net-MobileNet</rs> are presented. The performance of <rs id="a12897588" type="software">U-net-MobileNet</rs> for the different loss functions is displayed in Table 2 and the evolution of the metrics is shown in detail in Fig. 13. As shown in Table 2, the best performance is reached when WCE is utilized; the obtained F1 score is 79.6%, 76.6%, 78.2% and 71.2% for WCE, CE, F1 score and focal loss respectively. Precision is in the range of 90% for CE (Fig. 13b) and focal loss (Fig. 13d) while recall remains significantly lower, i.e. in the range of 60% to 70%. Thus, these two loss functions are not able to handle the class imbalance problem for crack segmentation since the network becomes overconfident in predicting background while neglecting the minority class, that is crack. When WCE (Fig. 13a) and F1 score (Fig. 13c) are used as loss function the discrepancy between precision and recall is less profound. Specifically for WCE, in the first epochs, the recall value ranks approximately 90% while further on converges to 80% and from the 80th epoch onwards decreases to 70%. On the other hand, precision follows an opposite path, starting from 50% and gradually increasing up to 85% in the final epochs. F1 score in the beginning of training is 60% and then converges to value close to slightly below 80%. The highest F1 score is attained in the 45th epoch. The performance of the three metrics indicates that in the beginning, the system is overconfident to predict cracks. In this process, it misclassifies background as cracks. Similar behaviour was reported by [30,56]. As Zhang et al. [75] pointed out, precision and recall frequently conflict with each other and a compromise between recall and precision is made to select the best model. In order to visualize the meaning of different values of recall and precision, predictions with <rs id="a12897592" type="software">U-net-MobileNet</rs> for different images are exhibited for the epochs 3 and 45 which correspond to the highest recall and F1 score respectively (Fig. 14). All the examples in Fig. 14 rank a recall value close to 100% (i.e. maximum value) at epoch 3. Nevertheless, precision and F1 score remain significantly lower. Taking a closer look at the predictions at epoch 3, large parts of the background have been misclassified as cracks (Fig. 14). Regarding the predictions on epoch 45, recall slightly drops while precision significantly increases since the network learns to negate greater parts of the background (Fig. 14).</p>
<p>Furthermore, the networks found in the literature, that is
DeepCrack,
DeepLabv3+, <rs id="a12897553" type="software">FCN-VGG16</rs> and U-net, have similar performance in terms of F1 score, i.e. from 74% to 75.7%. U-net outperforms the other networks obtained from the literature achieving F1 score 75.7% with <rs id="a12897554" type="software">FCN-VGG16</rs> following closely with F1 score 75.6%. Moreover, regarding the performance of the networks found in the literature except for <rs id="a12897555" type="software">FCN-VGG16</rs>, significant discrepancy is observed between the recall and precision values; the networks favour the recall which lead to lower values of precision. The models based on U-net and FPN with a pretrained CNN as backbone attain F1 score from 77.2% to 79.6% which means that they surpass the F1 score, that is 75.7%, of the models found in the literature and are implemented without pretraining. Furthermore, in Table 2 can be observed that <rs id="a12897593" type="software">U-net</rs> and <rs id="a12897594" type="software">U-net-MobileNet</rs> without pretraining reach similar F1 score, that is 75.7% and 75.4% respectively, while the pretrained <rs id="a12897595" type="software">U-net-MobileNet</rs> yields F1 score 79.6%. This observation highlights the effect of pretraining on the performance of the networks; F1 score is boosted by 4.2% when pretraining is considered for <rs id="a12897596" type="software">U-net-MobileNet</rs>. The <rs id="a12897597" type="software">U-net-MobileNet</rs> without pretraining in terms of F1 score records performance similar to <rs id="a12897556" type="software">FCN-VGG16</rs> and U-net and outperforms
DeepCrack and
DeepLabv3+. The models based on FPN in general score higher than the corresponding ones built on U-net while the highest F1 score is obtained with <rs id="a12897598" type="software">U-net-MobileNet</rs> and <rs id="a12897617" type="software">FPN-InceptionV3</rs> (Table 2). It is noted that the models based on FPN have almost half the size of the ones with U-net in terms of model parameters and memory size of the stored weights (Table 2). Thus, FPN models match the performance of the U-net counterparts while being significantly more lightweight networks.</p>
<p>In Fig. 15 different examples from the validation set are presented with predictions obtained with
DeepCrack,
DeepLabv3+,
U-net, <rs id="a12897599" type="software">U-net-MobileNet</rs> (with and without pretraining) and <rs id="a12897618" type="software">FPN-InceptionV3</rs>. In particular, images with edges around openings (Fig. 15a-e), crack-like mortar joints (Fig. 15f-i), shadows (Fig. 15k) and dark spots (Fig. 15l) are displayed. While the pretrained <rs id="a12897600" type="software">U-net-MobileNe</rs>t and <rs id="a12897619" type="software">FPN-InceptionV3</rs> are able to negate different types of noisy background, the rest of the networks (Fig. 15) score lower in terms precision.</p>
<p>Images from the validation set with predictions obtained using <rs id="a12897601" type="software">Unet-MobileNet</rs> have already been presented in Fig. 14 and Fig. 15 while extra examples are shown in Fig. 16. The network successfully segments cracks with different crack size, scale and background complexity; closeup photos (Fig. 16a-c), images with a larger field of view (Fig. 16d-f) and with unwanted objects (i.e. windows and colour-paints) (Fig. 16g-i). Apparently, there are cases that the network failed to perform crack segmentation accurately. For example, in Fig. 16j-k the network fails to detect parts of the cracks. Moreover, Fig. 16l-r displays examples where the model does not manage to negate noisy types of background.</p>
<p>In a previous study for crack detection on concrete surfaces it was concluded that when a DL network was trained on images of monotonous background and subsequently tested on a more complex dataset the performance drastically decreased [19]. In more detail, precision from 87.4% fell to 23.1%. Moreover, DL networks trained on concrete images found to perform poorly when tested on masonry images because they are rather complex [48]. This behaviour of CNN was explained by [79]; the transferability of features decreases as the distance between the base task (i.e. training dataset) and target task (i.e. testing dataset) increases. To build up on these findings, <rs id="a12897602" type="software">U-net-MobileNet</rs> trained on the masonry dataset is tested on images from concrete surfaces in order to evaluate the ability of CNNs to generalize over different materials.</p>
<p>When <rs id="a12897603" type="software">U-net-MobileNet</rs> is tested on the concrete dataset it ranks 74.7%, 70.9%, 91.2% for F1 score, recall and precision respectively. The network does not perform satisfactorily in terms of recall value while excels in terms of precision. These results can be explained by taking a closer look on the predictions on the concrete dataset (Fig. 17). In fact, the network performs exceptionally segmenting cracks with complicated shapes (Fig. 17a-d) obtaining 79% recall or above and a minimum of 94% in terms of precision. On the other hand, the network fails to detect cracks like in Fig. 17e-f but it is noted that these defects look like spalling and do not have a typical crack-like shape; similar defects do not exist in the masonry dataset. Additionally, precision is high which implies that the network can easily negate the background. This could be attributed to the fact that concrete surfaces are rather homogeneous and less complex than masonry surfaces. Consequently, the performance of <rs id="a12897604" type="software">U-net-MobileNet</rs> trained on the masonry dataset deteriorates, i.e. F1 scores declines from 79.6% to 74.7%, when tested on the concrete dataset but not as drastically as reported in the literature when networks trained on concrete images were consequently tested on masonry photos. As explained above (Section 2.2), this is attributed to the fact that masonry surfaces are more complex than concrete ones. It is noted that in the literature there are various datasets of concrete surfaces while only limited data for masonry exist. Thus, when crack segmentation on concrete surfaces is requested, it is recommended to train a model solely on concrete images instead of relying on models trained on masonry data. On the other hand, for cases where only few data exist, e.g. timber surfaces, a model trained on a dataset with complex backgrounds like the masonry dataset produced herein could be an alternative.</p>
<p>In this study the feasibility of DL techniques for crack detection on images from masonry walls is investigated. Even though masonry surfaces have been reported to be rather complex for CNN on crack detection, this study showcases that DL algorithms are able to accurately detect cracks from images of real masonry surfaces. In order to address the lack of data in the literature, a dataset with photos from masonry structures was produced containing complex backgrounds and various crack types and sizes. Different DL networks are considered and by leveraging the effect of transfer learning crack detection on masonry surfaces is performed both on patch and pixel level. To the authors' best knowledge, this is the first implementation of DL for pixel-level crack segmentation on masonry surfaces. State of the art CNNs pretrained on ImageNet are examined for their efficacy to classify images from masonry surfaces on patch level with <rs id="a12897605" type="software">MobileNet</rs> obtaining the highest accuracy, that is 95.3%. U-net, a deep FCN, and FPN, a generic pyramid representation, are combined with different pretrained CNNs performing as the backbone of the encoder part of the network to perform pixel level crack segmentation. <rs id="a12897606" type="software">U-net-MobileNet</rs> and <rs id="a12897620" type="software">FPN-InceptionV3</rs> attain the highest F1 score, that is 79.6%, and outperform other networks for crack segmentation from the literature. In particular, for <rs id="a12897608" type="software">U-net-Mobi-leNet</rs>, when the backbone CNN is considered without pretraining, F1 score declines from 79.6% to 75.4%, which demonstrates the beneficial effect of transfer learning. The ability of CNNs to generalize over different materials is evaluated. The performance of <rs id="a12897607" type="software">U-net-MobileNet</rs> trained on the masonry dataset deteriorates, i.e. F1 scores declines from 79.6% to 74.7%, when tested on concrete images but not as drastically as reported in the literature when networks trained on concrete images were consequently tested on masonry photos. <rs id="a12897539" type="software" subtype="implicit">Codes</rs>, data and networks relevant to the herein study can be found in the GitHub repository: <rs id="a12897541" type="url" corresp="#a12897539">gi
thub.
com/dimitrisdais/crack_detection_CNN_masonry</rs>.</p>
<p>Several photos obtained by inspectors for Helifix, UK, and were Fig. 17. Results obtained when <rs id="a12897609" type="software">U-net-MobileNet</rs> is trained on the masonry dataset and tested on photos from the concrete dataset [24]. For each image, the original image, the ground truth and the prediction are displayed. At the top of each prediction the calculated metrics (F1: F1 score, RE: recall, and PR: precision) are highlighted.</p>
</text>
</tei>
  <tei>
    <teiHeader>
        <fileDesc id="f334948889"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:56+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text lang="en">
        <p>Quantitative data were analysed using 
            <rs type="software" id="s1">SPSS</rs> version
            <rs type="version" corresp="s1">22</rs>. Descriptive statistics explored breastfeeding duration, feeding experiences and reasons for stopping. Participants were grouped into dichotomous variables for demographic measures university-level/not universitylevel education, White/Black and minority wthnic (BAME) ethnicity, primiparous/multiparous and live-in partner/not live-in partner. Feeding data were used to calculate current feeding methods (exclusive breastfeeding, mixed feeding and exclusive formula feeding) with any breastfeeding (exclusive or mixed) used to determine continued breastfeeding at the time of the survey (yes/no).
        </p>
        </text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f478144111"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:45+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>We performed all model computations using <rs id="a12965497" type="software">MATLAB</rs> <rs id="a12965498" type="version" corresp="#a12965497">R2021a</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f333870402"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:27+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>In this work, we contribute to filling this knowledge gap by presenting a new data set of mining extents derived by visual interpretation of satellite images. Our data set covers more than six thousand mining sites distributed across the entire globe. These mining sites have reported mineral extraction or activities between the years 2000 and 2017, according to the SNL Metals and Mining database 19 . Within these regions, we delineated the mining areas (i.e., drew polygons) by visual interpretation of several satellite data sources, including <rs id="a12967645" type="software">Google Satellite</rs>, <rs id="a12967589" type="publisher" corresp="#a12967590">Microsoft</rs> <rs id="a12967590" type="software">Bing Imagery</rs> and <rs id="a12967591" type="software">Sentinel-2 cloudless</rs> <rs type="bibr" corresp="#a12967591">33</rs> . As a result, we derived a set of 21,060 polygons globally, covering a total area of 57,277 km 2 . The overall accuracy, calculated from 1,000 stratified random points is 88.4% (for details see the section on Technical Validation).</p>
<p>The polygons were delineated by two trained experts using an <rs id="a12967592" type="software" subtype="implicit">open-source web application</rs> <rs id="a12967593" type="bibr">35</rs> developed for this specific purpose. The web interface systematically displays buffers and markers with information about the mines. As background, the app offers three options of satellite layers: <rs id="a12967594" type="software">Google Satellite</rs>, <rs id="a12967595" type="publisher" corresp="#a12967596">Microsoft</rs> <rs id="a12967596" type="software">Bing Imagery</rs>, and <rs id="a12967597" type="software">Sentinel-2 cloudless</rs> <rs id="a12967598" type="bibr">33</rs> . <rs id="a12967599" type="software">Google Satellite</rs> and <rs id="a12967600" type="publisher" corresp="#a12967601">Microsoft</rs> <rs id="a12967601" type="software">Bing</rs> provide images with a spatial resolution finer than 5 m for many regions of the world. These images allow identifying ground features related to mines with high confidence 9 . However, these data sources do not cover the whole globe with the same spatial resolution and contain out-of-date images for some regions 36 . To fill this gap, we used the <rs id="a1" type="software">Sentinel-2 cloudless</rs> data product with a 10 m spatial resolution provided by EOX 33 . The <rs id="a12967602" type="software">Sentinel-2 cloudless</rs> provides a mosaic built from Sentinel-2 images taken during the years 2017 and 2018. Combining these data layers, the experts identified and delineated the ground features related to mining. All three satellite data sources were visually inspected before delineating the polygons. The majority of the inspected locations had at least two sources of clear images (e.g., no cloud cover) and sufficient spatial resolution to identify mining features. Only very few locations lacked images with sufficient quality to draw the polygons, for example, due to cloud cover or low spatial resolution.</p>
<p>From the mining polygons we derived global grid data sets with the mining area at 30 arcsecond, 5 arcminute and 30 arcminute spatial resolution (approximately 1 × 1 km, 10 × 10 km and 50 × 50 km at the equator). This is useful because many modeling applications require standardized grid data 39 . The 30 arcsecond grid was derived from the percentage of area of the geometric intersection between each cell and the geometries of the mining polygons. These percentages were rounded to zero decimal digits to reduce the size of the data set. Therefore, the percentage of the cell covered by mine should be greater than 0.5% to be considered, i.e., approximately 0.5 ha at the equator. To obtain the gridded mining area, we estimated the area of each cell in square kilometers and multiplied with the percentage of mining cover per cell, resulting in a 30 arcsecond global grid indicating the mining area within each cell. The 5 arcminute and 30 arcminute grid resolutions were downsampled form the 30 arcsecond grid. All <rs id="a12967649" type="software" subtype="implicit">scripts</rs> used in the geoprocessing of data records are available with our <rs id="a12967650" type="software" subtype="implicit">open-source web application tool</rs> <rs id="a12967603" type="bibr">35</rs> .</p>
<p>Mining polygons. Figure 1 illustrates how the satellite images were used to delineate the mining extent. In this example, the area is used for coal mining in Mackenzie River, Queensland, Australia. The polygon in Fig. 1a was derived from the <rs id="a12967604" type="software">Sentinel-2 cloudless</rs> mosaic (Fig. 1b), which shows the largest extent of the mine among all three images sources. The <rs id="a12967605" type="software">Sentinel-2 cloudless</rs> mosaic is composed by images from the years 2017 and 2018 33 while <rs id="a12967606" type="publisher" corresp="#a12967607">Microsoft</rs> <rs id="a12967607" type="software">Bing</rs> (Fig. <rs type="figure">1c</rs>) and <rs id="a12967608" type="software">Google Satellite</rs> (Fig. 1d) only offered out-of-date images for that location, respectively taken in July 2011 and December 2007. Nevertheless, all three data sources contributed to providing pieces of evidence of mining in the mapped area. The delineated polygons cover all infrastructure and land cover types directly related to mining activities. This can produce large polygons, such as in the case of the Salar de Atacama, Chile. In that area, we delineated a polygon of approximately 1,354 km 2 , covering almost the whole nucleus of the salt flat, which extends over 1,360 km 2 and is used as a source to extract lithium, boron, potassium, iodine, sodium chloride, and bischofite 41 . Figure 2 shows the delineated polygon extent and a detailed view of one of the mining plants. Some pipelines and wells are more than 10 km away from the core infrastructure of the mine. We decided to map the whole area because the mining plants, in fact, have brine pumping and monitoring wells spreading over the entire salt flat far beyond the actual evaporation ponds 41 . Alternative assumptions mapping only the evaporation ponds estimated an area of only 80.53 km 2 in 2017 42 . However, it is important to note that the case of Salar de Atacama was rather isolated; in most cases, no features such as pipelines and wells outside the main mining sites could be identified from the available satellite images. In many cases, mines are located following the structure of mineral deposits, making it easy to map them from satellite images. We selected three mines to illustrate these large-scale concentrated activities (Fig. 3). The first example (Fig. 3a) shows the main open cut of the Carajás iron ore mine complex in the Brazilian Amazon, which is among the world's largest iron ore mining operations 43 . Figure 3b shows the Batu Hijau copper-gold mine.</p>
<p>The mapped mining extents presented in this work can be subject to many sources of error, ranging from experts' interpretation to the temporal availability and precision of the satellite images. The precision of the delineated mining borders can vary according to the satellite data source and the location. In general, the satellite sources used in this work provide sufficient spatial resolution and georeferencing accuracy to map mining areas 9 . Images available from <rs id="a12967609" type="software">Google Earth</rs>, for instance, have an overall positional root mean squared error (RMSE) of 39.7 m related to the reality on the ground 47 . Sentinel-2, on the other hand, has a RMSE below its pixel size (10 × 10 m) 48 . These errors are acceptable for global scale environmental assessments.</p>
<p>• COUNTRY_NAME: A string with the country name in English Our spatially explicit data records can be combined with other geographical data to perform further statistical analysis, for example, to test spatially stratified heterogeneity 54 and non-stationarity of variables 55,56 . For that, users can open the data records using software that support Geographic Information System (GIS), including, <rs id="a12967610" type="software">QGIS</rs> <rs id="a12967611" type="bibr">57</rs> , <rs id="a12967612" type="software">R</rs> <rs id="a12967613" type="bibr">58</rs> , and <rs id="a12967614" type="software">Python</rs> <rs id="a12967615" type="bibr">59</rs> . Besides, we also provide a <rs id="a12967646" type="software" subtype="implicit">tool</rs> for visual analysis of the geographical data records at <rs id="a12967617" type="url" corresp="#a12967646">www.fineprint.global/viewer</rs> and a <rs id="a12967618" type="software">Web Map Service (WMS)</rs> <rs id="a12967619" type="bibr">60</rs> accessible from <rs id="a12967620" type="url" corresp="#a12967618">www.fineprint.global/geoserver/wms</rs>.</p>
<p>All the <rs id="a12967651" type="software" subtype="implicit">code</rs> and <rs id="a12967652" type="software" subtype="implicit">geoprocessing
scripts</rs> used to produce the results of this paper are distributed under the GNU General Public License v3.0 (GPL-v3) 61 from the repository <rs id="a12967653" type="url" corresp="#a12967652">www.github.com/fineprint-global/app-mining-areapolygonization</rs> <rs id="a12967654" type="bibr">35</rs> . The <rs id="a12967648" type="software" subtype="implicit" corresp="#a12967625">processing
scripts</rs> were written in <rs id="a12967621" type="language" corresp="#a12967648">R</rs> <rs id="a12967622" type="bibr">58</rs> , <rs id="a12967623" type="language" corresp="#a12967648">Python</rs> <rs id="a12967624" type="bibr">59</rs> , and <rs id="a12967625" type="software" subtype="environment">GDAL (Geospatial Data Abstraction Library</rs> <rs id="a12967626" type="bibr">62</rs> ). The <rs id="a12967627" type="software" subtype="implicit" corresp="#a12967629">web application</rs> to delineate the polygons was written in <rs id="a12967628" type="software" subtype="environment">R</rs> <rs id="a12967629" type="software" subtype="component" corresp="#a12967628">Shiny</rs> <rs id="a12967630" type="bibr">63</rs> using a <rs id="a12967631" type="software">PostgreSQL</rs> <rs id="a12967632" type="bibr">64</rs> database with <rs id="a12967633" type="software">PostGIS</rs> <rs id="a12967634" type="bibr">65</rs> extension for storage. The full app setup uses <rs id="a12967635" type="software">Docker</rs> <rs id="a12967636" type="bibr">65</rs> containers to facilitate management, portability, and reproducibility.</p>
<p>The <rs id="a2" type="software" subtype="implicit">web application</rs> supports the delineation of areas from the satellite images layers. It systematically displays the regions of interest (e.g., buffer around the mines) and several background options of satellite images, which the users can take into account to draw and edit polygons. Note that mining coordinates are not part of the web application and must be fed into the database by the user. To learn more about the application setup see <rs type="url" id="a3">www. github.com/fineprint-global/app-mining-area-polygonization</rs>. The current version of app provides image layers from <rs id="a12967637" type="software">Sentinel-2 Cloudless</rs> <rs id="a12967638" type="bibr">33</rs> , <rs id="a12967639" type="software">Google Satellite</rs>, and <rs id="a12967640" type="publisher" corresp="#a12967641">Microsoft</rs> <rs id="a12967641" type="software">Bing Imagery</rs>. Further sources of satellite images can be added to the application via <rs id="a12967642" type="software">WMS</rs>.</p>
</text>
</tei>
  <tei>
    <teiHeader>
        <fileDesc id="f82825942"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T16:10+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text lang="en">
        <p>Surface markers were assessed using a FACSCalibur flow cytometer with 
            <rs type="software" id="s1">CellQuest</rs> software (
            <rs type="creator" corresp="s1">BD Biosciences</rs>) as reported previously (<ref type="bibr">Sacchetti et al., 2007</ref>) with mouse anti-human monoclonal antibodies (Table S7A) and isotype controls. CD146 + , CD146 + / CD56 + , CD146 + /CD34 + cell subsets, and CD146 + /ALP + subsets were separated using a FACS DIVAntageSE flow cytometer (BD Labware). CD146 + , CD146 + /CD56 + , CD146 + /CD34 + , and CD146 + / ALP + fractions were separated using a MiniMACS magnetic column separation unit (Miltenyi Biotec) according to the manufacturer's instructions.
        </p>
        <p>GSEA was performed using 
            <rs type="software" id="s2 ">GSEA</rs> software (
            <rs type="url" corresp="s2">http://www. broadinstitute.org/gsea/index.jsp</rs>) (<ref type="bibr">Subramanian et al., 2005</ref>) as described in Supplemental Experimental Procedures.
        </p>
        </text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f562025239"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:33+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Binding affinity of viruses to the host receptors is mainly affected by different protein-protein electrostatic interactions [10,11]. A change in different pivotal residues at the binding site could affect viral-host cells fusion and hence the infectivity of the virus [5,[12][13][14]. Therefore, we herein study how the N501Y mutation in the RBD of the SARS-CoV2 can alter the binding of the virus to ACE2. We focus on the role of the electrostatic interactions on the binding energy, where it is known to be dominant among different protein-protein interactions [15]. In this study, we used a combined Molecular dynamic (MD) and Monte Carlo (MC) simulations to assess the molecular interactions between RBD of S-protein and ACE2 for the N501Y mutant and compared our results with the wild type. The crystal structure (PDB ID: 6M17) [12] was optimized using <rs id="a12965902" type="software">openMM</rs> <rs id="a12965903" type="bibr">[16]</rs>. Then, several rotamers were built by <rs id="a12965904" type="software">MCCE</rs> to by rotating each rotatable bond by 60 o to appropriately sample the sidechains conformations. In order to build the N501Y mutant, the sidechain of N501 is replaced by aromatic. sidechain of tyrosine using <rs id="a12965905" type="software">MCCE</rs>.</p>
<p>The electrostatic interactions between the different conformers were calculated using <rs id="a12965906" type="software">DELPHI</rs> <rs id="a12965907" type="bibr">[17]</rs>. Then, <rs id="a12965908" type="software">MCCE</rs> is used to generate Boltzmann distribution for all conformer using MC sampling for the wild type and the N501Y mutant at pH 7. The most occupied conformers were subjected to MD minimization again using <rs id="a12965909" type="software">openMM</rs> <rs id="a12965910" type="bibr">[16]</rs>. The resulting structures were used to calculate the electrostatic energies between the RBD of SARS-CoV-2 and the ACE2 using <rs id="a12965911" type="software">DELPHI</rs>. In WT, the maximum vdW interaction of -1.889 kcal/mol was reported between residue RBD-N501 and ACE2-K353. While For the mutated structure, a maximum vdW interactions of -2.441 Kcal/mol was between RBD-Q498 and ACE2-Y41. The Maximum electrostatic interactions of -9.55 kcal/mol was observed between RBD-K417 and ACE2-D30 in wild type, and of -8.39 kcal/mol between RBD-K458 and ACE2-E23 in mutated structure. The electrostatic interactions between the SARS-CoV-2 and ACE2 in the wild type and the mutant These residues contribute by ~20 Kcal/mol of the total electrostatic interactions between the SARS-CoV-2 and ACE2 (Table S1). However, the binding between the SARS-CoV-2 and ACE2 is favored by ~4 Kcal/mol in the mutant (Table 1) due to a stronger hydrogen bond between SARS-CoV-2-T500 and ACE2-D355 (Fig. 2a, Fig. 3A).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f326653207"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T09:12+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Raw data were processed in <rs id="a12872470" type="software">Proteome</rs> (v <rs id="a12872471" type="version" corresp="#a12872470">2.2</rs>) with <rs id="a12872472" type="software">SequestHT</rs> search engine ( <rs id="a12872473" type="publisher" corresp="#a12872472">Thermo Scientific</rs>) using reviewed UniProt 52 human protein entries for protein identification and quantification. The precursor mass tolerance was set at 20 ppm and the fragment ion mass tolerance was 0.5 Da. Spectra were searched for fully tryptic peptides with maximum 2 miss-cleavages. TMT6plex at N-terminus/K and Carbamidomethyl at C were used as static modifications. Dynamic modifications included oxidation of M and deamidation of N/Q. Peptide confidence was estimated with the Percolator node. Peptide FDR was set at 0.01 and validation was based on q-value and decoy database search. The reporter ion quantifier node included a TMT10plex quantification method with an integration window tolerance of 15 ppm and integration method based on the most confident centroid peak at the MS3 level. Only unique peptides for the protein groups were used for quantification. Peptides with average reporter signal-to-noise &lt;3 were excluded from protein quantification.</p>
<p>Flow cytometry. Cells were washed with FACS buffer (PBS buffer supplemented with 1% FCS and 1 mM EDTA) by centrifugation and stained with the respective antibodies. Reactions were incubated for 30 min at 4 °C. Following two washes with FACS buffer, samples were resuspended in 200 μl of FACS buffer and data was acquired using a Fortessa analyser (BD Bioscience). All data were processed with <rs id="a12872474" type="software">FlowJo</rs> (v <rs id="a12872475" type="version" corresp="#a12872474">9.9</rs>, <rs id="a12872476" type="publisher" corresp="#a12872474">TreeStar</rs>). Antibodies used in this study: anti-human CD4, APC (BioLegend; Clone: OKT4, Catalog No: 317416, Dilution: 1:100), anti-human CD45RA, Brilliant Violet 785 (BioLegend; Clone: HI100, Catalog No: 304140, Dilution: 1:100), anti-human CD45RO, PE-Cyanine7 (BioLegend; Clone: UCHL1, Catalog No: 304229, Dilution: 1:100), anti-human CD197 (CCR7), (BD Bioscience; Clone: 150503, Catalog No: 561271, Dilution: 1:100), anti-human IFN gamma, PE-Cyanine7 (eBioscience; Clone: 4s.B3, Catalog No: 25-7319-82, Dilution: 1:50), antihuman IL-9, PE (BD bioscience; Clone: MH9A3, Catalog No: 560814, Dilution: 1:50).</p>
<p>RNA-seq data analysis. Sequencing reads were aligned to the reference human genome using <rs id="a12872477" type="software">STAR</rs> <rs id="a12872478" type="bibr">53</rs> (v <rs id="a12872479" type="version" corresp="#a12872477">2.5.3</rs>) and annotated using the hg38 build of the genome (GRCh38) and <rs id="a12872480" type="software">Ensembl</rs> (v<rs id="a12872481" type="version" corresp="#a12872480">87</rs>). Next, the number of reads mapping to each gene was quantified using <rs id="a12872482" type="software">featureCounts</rs> <rs id="a12872483" type="bibr">54</rs> (v <rs id="a12872484" type="version" corresp="#a12872482">1.22.2</rs>). After quantification, reads mapping to the Y chromosome and the major histocompatibility complex (HLA) region (chr6:25,000,000-47,825,000) were removed from the analysis. The final result from this process was a counts table of RNA expression in each sequenced sample.</p>
<p>RNA counts were imported into <rs id="a12872485" type="software" subtype="environment">R</rs> (v <rs id="a12872486" type="version" corresp="#a12872485">3.5.1</rs>) where normalization for library size and regularized-logarithmic transformation of counts was performed using <rs id="a12872487" type="software" corresp="#a12872485">DESeq2</rs> <rs id="a12872488" type="bibr">55</rs> (v<rs id="a12872489" type="version" corresp="#a12872487">1.19.52</rs>). We identified and removed batch effects using <rs id="a12872490" type="software">limma</rs> <rs id="a12872491" type="bibr">56</rs> (v<rs id="a12872492" type="version" corresp="#a12872490">3.35
.15</rs>). Exploratory data analysis was performed using <rs id="a12872493" type="software">ggplot
2</rs> (v <rs id="a12872494" type="version" corresp="#a12872493">3.0.0</rs>) and the base <rs id="a12872495" type="software" subtype="environment">R</rs> functions for principal component analysis. Differential expression analysis was performed with <rs id="a12872496" type="software">DESeq2</rs>. More specifically, pairwise combinations were performed between any two conditions of interest, usually setting either resting or Th0-stimulated cells as controls. Differentially expressed genes were defined as any genes with absolute log-fold changes (LFC) larger than 1 at a false discovery rate (FDR) of 0.05.</p>
<p>Proteomics data analysis. After quantification, protein abundances were normalised in order to allow comparisons between samples and plexes (mass spectrometry batches). Namely, protein abundance values were normalized to the total abundance of the respective sample (sample-wise normalization) and then scaled to the maximum abundance of the respective protein (protein-wise scaling). Data were then imported into <rs id="a12872497" type="software" subtype="environment">R</rs>, where principal component analysis was performed using all the proteins with no missing values (proteins detected in all batches and samples) with base <rs id="a12872498" type="software" subtype="environment">R</rs> functions. Finally, differential protein expression was analyzed by performing pairwise comparisons between any two conditions of interest. This was done using the moderated T test implemented in <rs id="a12872499" type="software" subtype="environment">limma</rs>'s <rs id="a12872500" type="software" subtype="component" corresp="#a12872499">eBayes</rs> function <rs id="a12872501" type="bibr">56</rs> . When testing for differential protein expression, only proteins detected in at least two biological replicates per condition were kept. Multiple testing correction was performed using the Benjamini-Hochberg procedure 57 . Finally, differentially expressed proteins were defined as any proteins with an absolute log-fold change larger than 0.5 at an FDR of 0.1.</p>
<p>Pathway enrichment analysis. Pathway enrichment analysis was performed using proteomics and RNA-seq data. To do so, genes detected at both the RNA and protein level were identified by matching gene names. Next, genes were ranked by differential gene or protein expression, respectively, compared to either resting or Th0-stimulated T N and T M cells. Finally, pathway enrichment analysis was performed independently in the RNA and protein data using the <rs id="a12872502" type="software">Perseus</rs> software <rs id="a12872503" type="bibr">58</rs> (v <rs id="a12872504" type="version" corresp="#a12872502">1.6</rs>) and the 1D-annotation enrichment method 59 . The enrichment scores indicated whether the RNAs and proteins in a given pathway tended to be systematically up-regulated or down-regulated based on a Wilcoxon-Mann-Whitney test. A term was defined as differentially enriched if it had a Benjamini-Hochberg FDR &lt; 0.05. Firstly, all significantly enriched pathways with absolute enrichment scores higher than 0.25 in both RNA and protein data for the same cell types and cytokine conditions were retrieved. The enrichment scores estimated for these pathways were used to estimate the correlation between RNA and protein using Pearson correlation coefficients (Supplementary Fig. 3C,D). Next, a subset of strongly enriched pathways was selected for visualization in <rs id="a12872505" type="software" subtype="environment">R</rs> using the <rs id="a12872507" type="software" subtype="component" corresp="#a12872505">pheatmap</rs> package (v <rs id="a12872508" type="version" corresp="#a12872507">1.0.10</rs>). This selection included all pathways with an absolute enrichment score higher than 0.7, an FDR &lt; 0.05 which were included in either Reactome, KEGG or CORUM [60][61][62] and were of relevance to CD4 + T cell biology.</p>
<p>To test which genes were more specific to one cell state than expected by chance, sample labels were randomly permuted and the specificity score was recalculated. Empirical P values were computed as the proportion of times the observed specificity score of a gene in a given cell state was larger than the corresponding permuted value. P values were corrected for the number of genes tested using the Benjamini-Hochberg procedure 57 . A total of permutations were performed. Finally, proteogenomic signatures for each cytokine condition were defined as any genes with a specificity score larger than 0.7 and an FDRadjusted P value lower than 0.1. This analysis was performed separately for naïve and memory T cells. The <rs id="a128755991" type="software" subtype="implicit" corresp="#a12875599">functions</rs> used to derive proteogenomic signatures are publicly available as an <rs id="a12875599" type="software" subtype="environment">R</rs> package on
GitHub ( <rs id="a12872511" type="url" corresp="#a128755991">https://github.com/eddiecg/ proteogenomic</rs>).</p>
<p>Single-cell RNA-seq data analysis. Single-cell RNA-sequencing data were processed using the <rs id="a12872512" type="software">Cell Ranger Single-Cell Software Suite</rs> <rs id="a12872513" type="bibr">33</rs> (v <rs id="a12872514" type="version" corresp="#a12872512">2.2.0</rs>, <rs id="a12872515" type="publisher" corresp="#a12872512">10×-Genomics</rs>). Namely, reads were first assigned to cells and then aligned to the human genome using <rs id="a12872516" type="software">STAR</rs> <rs id="a12872517" type="bibr">53</rs> , using the hg38 build of the human genome (GRCh38). Reads were annotated using <rs id="a12872518" type="software">Ensembl</rs> (v <rs id="a12872519" type="version">87</rs>). Gene expression was then quantified using reads assigned to cells and confidently mapped to the genome.</p>
<p>Because each of the samples consisted of a pool of four individuals, natural genetic variation was used to identify which cells corresponded to which person. A list of common genetic variants was collected, defined as any SNP included in gnomAD 64 with a minor allele frequency higher than 1% in the Non-Finish European (NFE) population. Next, <rs id="a12872520" type="software">cellSNP</rs> (<rs id="a12872521" type="version" corresp="#a12872520">v0.99</rs>) <rs id="a12872522" type="bibr">65</rs> was used to generate pileups at these SNPs, resulting in one VCF file per sample. This information was then used by <rs id="a12872523" type="software">Cardelino</rs> <rs id="a12872524" type="bibr">65</rs> (v<rs id="a12872525" type="version" corresp="#a12872523">0.99</rs>, now <rs id="a12872526" type="software">Vireo</rs>) to infer which cells belong to the same individual. Any cells which remained unassigned (with &lt;0.9 posterior probability of belonging to any individual) or were flagged as doublets were discarded. In general, over 85% of cells were unambiguously assigned to an individual (Supplementary Fig. 4). This analysis was performed separately for each sample. To identify which individual from a given sample corresponded to an individual in a different sample, results from <rs id="a12872527" type="software">Cardelino</rs> were hierarchically clustered by genotypic distances between individuals. Clustering separated genotypes into four distinct groups, each group corresponding to one of the profiled individuals.</p>
<p>Results from RNA quantification and genotype deconvolution were imported into <rs id="a12872528" type="software" subtype="environment">R</rs> and analysed using <rs id="a12872529" type="software" subtype="component" corresp="#a12872528">Seurat</rs> (v <rs id="a12872530" type="version" corresp="#a12872529">2.3.4</rs>) <rs id="a12872531" type="bibr">66</rs> . Cells with less than 500 genes detected or with more than 7.5% mitochondrial genes were removed from the data set. Cells expressing high levels of hemoglobin genes (i.e. HBA and HBB) were also removed from the data set, as they likely represent contamination during cell culture or sample processing. Counts were next normalized for library size and logtransformed using <rs id="a12875581" type="software">Seurat</rs>'s default normalization parameters. Next, a publicly available list of cell cycle genes 67 was used to perform cell cycle scoring and assign cells to their respective stage of the cell cycle. Cell cycle, as well as any known sources of unwanted variation (mitochondrial content, cell size as reflected by UMI content, biological replicate and library preparation batch) were regressed using Seurat's built-in regression model. Highly variable genes were identified using <rs id="a12875598" type="software">Seurat</rs> and used to perform principal component analysis. The first 30 principal components were used as an input for SNN clustering and for embedding using the uniform manifold approximation and projection (UMAP) 34 . Marker genes for each cluster were identified computationally using the Wilcoxon rank sum test implemented in <rs id="a12875597" type="software">Seurat</rs>. Multiple testing correction was performed using FDR. Cell cycle genes were excluded from this analysis. Moreover, marker genes were required to be expressed by at least 10% of the cells in the cluster at a minimum fold change of 0.25. A total of five clusters were identified in resting cells and 17 clusters were found in stimulated cells. Clusters were manually annotated according to their gene expression pattern, the cytokine which cells in the cluster were exposed to and the presence or absence of hallmark genes compiled from the literature.</p>
<p>UniFrac distance analysis 38 was used to test if cells exposed to two different cytokine conditions tended to form the same clusters. Pairwise UniFrac distances were computed for all combinations of cytokine conditions using all the cells captured for the respective conditions. The <rs id="a12872536" type="software" subtype="environment">R</rs> package <rs id="a12872537" type="software" subtype="component" corresp="#a12872536">scUnifrac</rs> (v <rs id="a12872538" type="version" corresp="#a12872537">0.9.6</rs>) <rs id="a12872539" type="bibr">68</rs> was used as it was specifically adapted to deal with scRNA-seq data. All parameters were set to the default values (1000 permutations, nDim = 4, ncluster = 10).</p>
<p>Pseudotime ordering. Cells were ordered into a branched pseudotime trajectory using <rs id="a12872540" type="software">Monocle</rs> (v<rs id="a12872541" type="version" corresp="#a12872540">2.12.0</rs>) and restricting the analysis to the highly variable genes identified by <rs id="a12875596" type="software">Seurat</rs>. This was done separately for each cytokine condition (resting, Th0, Th2, Th17 and iTreg), including both T N and T M . This resulted in five condition-specific pseudotime trajectories. <rs id="a12872543" type="software">Monocle</rs> was used to test for a significant correlation between gene expression and pseudotime in each trajectory. A gene was defined as significantly associated with pseudotime if its estimated q value was lower than 0.01.</p>
<p>Alignment of resting and Th0-stimulated cells. The single-cell transcriptomes of resting and Th0-stimulated T cells were analyzed separately according to the methods described above. This allowed the identification and annotation of clusters in resting T cells, as well as the estimation of an effectorness value for every cell in both data sets. Next, canonical correlation analysis (CCA) 66 , as implemented in <rs id="a12875595" type="software">Seurat</rs> v <rs id="a12872545" type="version" corresp="#a12875595">2.3.4</rs>, was used to identify correlated features between the two conditions and to align resting and Th0-stimulated cells into a common space of lower dimensionality. The first 30 CCA dimensions were used to perform UMAP embedding and visualization of cells. The cluster labels defined for resting T cells, as well as the effectorness values independently estimated for resting and Th0stimulated cells were examined in these visualizations and used to annotate Th0stimulated cells based on the corresponding resting T cell annotations.</p>
<p>Modelling interaction between effectorness and cytokines. The association between gene expression, effectorness and cytokine-stimulation was tested with the <rs id="a12872546" type="software" subtype="component" corresp="#a12872547">lm()</rs> function from base <rs id="a12872547" type="software" subtype="environment">R</rs>. The expression of each gene was modelled as a linear function of T cell effectorness (a numeric variable in the [0, 1] range) and cytokinestimulation (a categorical variable with levels Th0, Th2, Th17 and iTreg). An additional term was incorporated which accounted for potential interactions between these two variables, as specified in the following equation:</p>
<p>Where X is the expression of gene i in cell j (log2 of normalized UMIs), E the effectorness of cell j, C the cytokine cocktail cell j was exposed to and ε a random error term, which was assumed to follow a normal distribution with a mean of zero. The regression coefficients for the intercept, effectorness, cytokine stimulation and the effectorness-cytokine interaction were represented, respectively, by α, β, γ and δ. An estimate and a P value were derived for each of these coefficients in each tested gene. P values were corrected for the number of genes tested using the Benjamini-Hochberg procedure 57 . This analysis was restricted to the top variable genes identified by <rs id="a12872548" type="software">Seurat</rs>. All cells with zero-expression for a given gene were omitted. A coefficient was defined as significant if its corresponding FDR-adjusted P value was lower than 0.05.</p>
<p>TCR clonotype IDs for each single-cell in the study were retrieved. These clonotypes were next used to estimate three diversity metrics: (1) the fraction of unique clones (i.e. the number of unique TCR clonotypes divided by the total number of clonotypes), (2) the Shannon entropy 69 , and (3) the expansion index (defined as 1 -Shannon entropy) 36 at different ranges of T cell effectorness. To do so, cells were ordered by increasing effectorness (i.e. from the least to the most effector) and the diversity metrics were estimated using a sliding window of size 100 (i.e. first analyzing the cells ranked 1 to 100, next the cells ranked 2 to 101, and so on until reaching the end of the data set). The three diversity metrics, along with the mean effectorness value, were repeatedly calculated within each window. Importantly, the fixed window size allowed us to avoid the need for any normalization by cell numbers. Sliding windows were created using the <rs id="a12872550" type="software" subtype="component" corresp="#a12875594">rollapply()</rs> function included in <rs id="a12872551" type="software" subtype="environment">R</rs>'s <rs id="a12875594" type="software" subtype="component" corresp="#a12872551">zoo</rs> package <rs id="a12872553" type="bibr">70</rs> .</p>
<p>Finally, the amino acid sequence of the TCRβ1 CDR3 region of each cell 71 was retrieved from the study supplementary data. The grouping of lymphocyte interactions by paratope hotspots (GLIPH) 37 algorithm was then used to cluster TCR sequences by pMHC specificity based on their CDR3 sequences. The TCR network and clonotype groups inferred by <rs id="a12872554" type="software">GLIPH</rs> were imported into <rs id="a12872555" type="software" subtype="environment">R</rs> and visualized using the <rs id="a12875593" type="software" subtype="component" corresp="#a12872555">igraph</rs> package <rs id="a12872558" type="bibr">72</rs> , focusing on the clonotypes of cells with an estimated effectorness value &gt; 0.7.</p>
<p>All the <rs id="a12875591" type="software" subtype="implicit">codes</rs> used for processing and analyzing the data in this study were compiled into a single publicly available
GitHub repository [ <rs id="a12872560" type="url" corresp="#a12875591">https://github.com/eddiecg/T-celleffectorness</rs>]. Additionally, the <rs id="a128755911" type="software" subtype="implicit" corresp="#a12875600">functions</rs> used for deriving cell state-specific proteogenomic signatures are available as an <rs id="a12875600" type="software" subtype="environment">R</rs> package in
GitHub [ <rs id="a12872563" type="url" corresp="#a128755911">https://github.com/ eddiecg/proteogenomic</rs>].</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f479167285"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:11+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Online platforms, such as the <rs id="a12972754" type="software">Global Biodiversity Information Facility (GBIF)</rs> (<rs id="a12972755" type="url" corresp="#a12972754">www.gbif.org</rs>), are repositories for specimen and species occurrence data from museum collections, national and regional recording schemes, and citizen science projects, and their data are openly available. However, <rs id="a12972756" type="software">GBIF</rs> has a strong geographical bias (e.g., &gt;266 million records from the United States, but only 9.9 million from Brazil and 1.8 million from Indonesia) and a strong taxonomic bias favoring birds and some other vertebrate and plant groups (Troudet et al. 2017). A more strategic approach to data collection is required to obtain enough information for the lesser-known taxonomic groups from understudied regions because it is unlikely that these biases will change in the near future under current efforts.</p>
<p>One important step forward would be for ecological, taxonomic, and evolutionary journals to make it mandatory for authors to submit spatial occurrence data to platforms or databases that feed <rs id="a12972757" type="software">GBIF</rs> <rs id="a12972758" type="bibr">(Meier &amp; Dikow 2004)</rs>, similar to the mandatory submission of genetic data to GenBank (Benson et al. 2011), <rs id="a12972761" type="software">BOLD</rs> <rs id="a12972762" type="bibr">(Ratnasingham &amp; Hebert 2007)</rs>, or other online databases. The same should apply for environmental impact assessments (EIAs), which are a legal requirement in many countries; yet, data from EIAs are rarely shared and made publicly available. The private sector could play a major role in enhancing the availability of EIA data. These requirements to share spatial biodiversity data would lead to more comprehensive distribution information for lesser-known taxonomic groups of the kind that is crucial for assessing the red-list status of a species. It may be necessary to change legal regulations to avoid contractual obligations hampering the release of such data. Sensitive data (e.g., for species targeted by collectors) could also be hidden from the public as recommended by the IUCN (2018).</p>
<p>These changes may also require development of guidelines to ensure data providers are invited to be coauthors of red-list assessments or other analyses if, for example, &gt;10% of the data used in a study are from a single provider. Many global databases, such as <rs id="a12972763" type="software">GBIF</rs> and Genbank, include erroneous data, including incorrect identifications, out-of-date names, and incorrect taxon localities. We recommend the development of a mechanism to validate and correct entries in GBIF by qualified experts and addition of a quality-control flag, as already happens with some citizen science platforms, such as <rs id="a12972765" type="software">Observation.org</rs> or <rs id="a12972766" type="software">iNaturalist</rs> <rs id="a12972767" type="bibr">(Pereira et al. 2017)</rs>.</p>
<p>The IUCN Red List criteria allow one to infer population trends from habitat trends, but assessors working on lesser-known species groups in tropical countries are often based in the northern hemisphere and may lack detailed knowledge of changes in habitat trends associated with local anthropogenic impacts. Global land-cover data sets can help address this gap. The <rs id="a12972768" type="software">Global Forest Watch</rs> database <rs id="a12972769" type="bibr">(Hansen et al. 2013)</rs>, for example, collects information on changes in forest cover that can be used to infer population trends of forest-dependent species (Li et al. 2016;Santini et al. 2019). Databases are also available for a range of other pressures on species, such as dams, wildfires, roads, pollution, and invasive species. The PREDICTS database also provides some mapping capability for human pressures and calculation of a local biodiversity intactness index; the biggest data gaps relate to insects, soil invertebrates, and fungi (Hudson et al. 2017). The use of proxy data for threats can be enhanced by creating a single threat database that uses the best-available analytical tools to offer spatially explicit information on threats to biodiversity (e.g., agricultural land-use change, deforestation, urbanization, unselective fishing, spread of invasive species, climatic extremes, wildfires, quarrying, and dams) at a fine scale. This information would greatly enhance the ability to infer population and habitat trends for lesser-known taxa required for red-list assessments and would improve assessments for those species for which lack of information on threats has led to DD status (Murray et al. 2014).</p>
<p>Often only a few taxonomic experts and dedicated citizen scientists have adequate knowledge to conduct redlist assessments for lesser-known taxa. As long as species knowledge resides with a few experts, who often live in species-poor countries, it will remain difficult to keep pace with the ongoing rapid loss of biodiversity. It is, therefore, vital to build capacity for taxonomic, ecological, and species monitoring in countries and regions with high species richness or endemism (Tittensor et al. 2014;Schmeller et al. 2017) by engaging more scientists and citizen scientists in local field research and conservation and by training students and government and nongovernmental organizations (NGO) staff. Conservation authorities and NGOs should employ staff with knowledge of lesser-known taxa. It is particularly important to bridge the gap between hard science and citizen science by producing print or online field guides or easy-to-use identification apps to allow the public to engage in surveys and species monitoring. Tools available to local conservation practitioners should also be improved. Automated image recognition systems (such as the apps <rs id="a12972770" type="software">ObsIdentify</rs> and <rs id="a12972771" type="software">iNaturalist Seek</rs>) work remarkably well for some lesserknown taxa, such as plants, moths, and bugs in northwestern Europe (Schermer &amp; Hogeweg 2018), but they need constant support by species experts to calibrate the system and a high number of photos to feed the deep-learning algorithms. National capacity building in biodiversity-rich countries "should be linked to existing monitoring plans, such as those associated with national biodiversity strategies, to ensure government agencies are supported in implementing multilateral environmental agreements such as CBD" (Stephenson et al. 2017b). A positive example is the South African Custodians of Rare and Endangered Wildflowers (CREW) programme. Building capacity of scientists and conservation officials to conduct red-list assessments, compile conservation strategies, and implement conservation action in underrepresented countries is required.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f344913446"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T10:07+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>To investigate the consequence of EA on the kinase activity of CDK6, enzyme activity assay was carried out with increasing concentrations of EA (1-10 µM) (Figure 7C). Enzyme activity was performed as per our previously published protocols [29,30]. It is quite apparent from the inhibition data that the activity of CDK6 decreases with increasing concentration of EA and shows a concentration-dependent relationship. IC50 was calculated with an <rs id="1" type="software">AAT Bioquest calculator</rs> <re corresp="#a1" type="bibr">[31]</re> (Figure S3C) and was found to be 3.053 µM. These observations suggested that EA is a strong inhibitor of CDK6.</p>
<p>To investigate the consequence of EA on the kinase activity of CDK6, enzyme activity assay was carried out with increasing concentrations of EA (1-10 µM) (Figure 7C). Enzyme activity was performed as per our previously published protocols [29,30]. It is quite apparent from the inhibition data that the activity of CDK6 decreases with increasing concentration of EA and shows a concentration-dependent relationship. IC 50 was calculated with an <rs id="1" type="software">AAT Bioquest calculator</rs> <re corresp="#a1" type="bibr">[31]</re> (Figure S3C) and was found to be 3.053 µM. These observations suggested that EA is a strong inhibitor of CDK6.</p>
<p>The atomics coordinates of the CDK6 structure were downloaded from the Protein Data Bank (PDB ID -3NUP) and subsequently optimized for modeling the gap in the structures using the "<rs id="a12893292" type="software">PRIME</rs>" module of <rs id="a12893293" type="publisher" corresp="#a12893292">Schrödinger</rs> <rs id="a12953642" type="bibr">[47,48]</rs>. The structure of EA was constructed using drawing utilities present in <rs id="a12893296" type="software">MAESTRO</rs> (
Maestro, <rs id="a12893297" type="publisher" corresp="#a12893296">Schrödinger, LLC</rs>, New York, NY, 2018) and the geometries of the resultant ligand structure were optimized by <rs id="a12893298" type="software">JAGUAR</rs> <rs id="a12953643" type="bibr">[49]</rs>.</p>
    <p>The <rs id="a12893300" type="software">Autodock</rs> <rs id="a12893301" type="version" corresp="#a12893300">4</rs> was used to perform the docking between the CDK6 and EA [50], generated output in the form of inhibition constant along with the free energy of binding. The <rs id="a12893302" type="software">Autodock</rs> performs rigid docking, which involves the usage of free energy factors for the classification of bound conformation. The energy factors are derived from the combination of the available empirical force field as well as the Lamarckian Genetic Algorithm [50]. In the primary step, the grid dimensions were set to 44 × 54 × 50 Å along with the XYZ directions using the <rs id="a12893303" type="software">AutoGrid</rs> with a spacing of 0.375 Å. The maximum efficiency values were set for the Lamarckian genetic algorithm, with the population control was set to 250 as well as the "longer" intervals, which were used for the energy evaluations. The docking was performed on the cluster of Center for High-Performance Computing (CHPC), South Africa, and 100 bound conformations which were grouped based on 2.0 Å RMSD tolerance were generated for CDK6 and EA system. The <rs id="a12893304" type="software">DrugScoreX</rs> was used for performing the re-scoring of the generated docked conformations <rs id="a12953644" type="bibr">[51]</rs>. The best scoring docked complex was subjected to the MD simulations.</p>
<p>MD simulations on EA-bound and apo form of CDK6 were performed using <rs id="a12893306" type="software">GROMACS</rs> version <rs id="a12893307" type="version" corresp="#a12893306">2018-2</rs> <rs id="a12953645" type="bibr">[52]</rs>. Primarily, the <rs id="a12893309" type="software">GROMOS96</rs>
53a6 force-field was used for the generation of topologies of protein structure in the docking-based generated complexes. Moreover, the topologies of the studied ligand compound were generated using the <rs id="a12893310" type="software">PRODRG</rs> server <rs id="a12953646" type="bibr">[53]</rs>. But the <rs id="a12893312" type="software">PRODRG</rs> server does not contain the functionality of generating the partial charges of the EA, therefore, the <rs id="a12893613" type="software">CHELPG</rs> program and B
3LYP 6-31G (d,p) basis set present in the DFT method of <rs id="a12893313" type="software">GAUSSIAN</rs> was used for the correction of the partial charges <rs id="a12953647" type="bibr">[54]</rs>. In the subsequent steps, the solvation of the docked complexes was performed using the SPC/E water model [55] and neutralization was performed by including the counter number of NA and CL ions. Furthermore, the <rs id="a12893315" type="software" subtype="component" corresp="#a12893316">steepest descent algorithm</rs> present in the <rs id="a12893316" type="software" subtype="environment">GROMACS</rs> was used for minimizing the neutralized solvated system with a convergence criterion of 0.005 kcal/mol. To keep the ligand molecules in the solvated box, the position restraints were applied.</p>
<p>The equilibration step includes a separate NVT (constant volume) stage followed by NPT (constant pressure) ensemble, each at a 100 ps time scale. Using the Berendsen weak coupling method and Parrinello-Rahman barostat, the temperature and the pressure of the simulating system were maintained at 300 K and 1 bar, respectively. The LINCS algorithm was used for the generation of the final conformational production stage for a 200 ns timescale, and trajectories were generated, which were analyzed to understand the behavior of each complex in the explicit water environment. The changes in the protein-ligand distance, H-bonds, RMSD, R g , RMSF, PCA, and free energy landscapes of the complex system were analyzed. Furthermore, the molecular mechanics Poisson-Boltzmann surface area (MM-PBSA) protocols implemented in the <rs id="a12893317" type="software">g_mmpbsa</rs> package <rs id="a12953648" type="bibr">[56]</rs> were used for the calculation of free energy of binding protein and the ligand molecules.</p>
<p>ITC measurements were carried out at 25 • C using a VP-ITC microcalorimeter (MicroCal, Inc, GE, MicroCal, USA). The sample cell was filled with 15-20 µM CDK6 and 500 µM EA solution was filled in the syringe. An automated titration was carried out with (including a first false injection of 2 µL) a successive injection of 10 µL EA solution into a CDK6 sample cell at a 260 s interval with 320 rpm stirring speed. For analysis, the heat of dilution of EA in the sample buffer was subtracted from the titration data. <rs id="a12893319" type="software">MicroCal Origin</rs> <rs id="a12893320" type="version" corresp="#a12893319">8
.0</rs> was used to analyze the stoichiometry of binding (n), enthalpy change (∆H), an association constant (K a ).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f78063267"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T07:07+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The Murchison Widefield Array (MWA) is a new low-frequency interferometric radio telescope built in Western Australia at one of the locations of the future Square Kilometre Array (SKA). We describe the automated radio-frequency interference (RFI) detection strategy implemented for the MWA, which is based on the <rs id="a12971927" type="software">AOFLAGGER</rs> platform, and present 72-231-MHz RFI statistics from 10 observing nights. RFI detection removes 1.1% of the data. RFI from digital TV (DTV) is observed 3% of the time due to occasional ionospheric or atmospheric propagation. After RFI detection and excision, almost all data can be calibrated and imaged without further RFI mitigation efforts, including observations within the FM and DTV bands. The results are compared to a previously published Low-Frequency Array (LOFAR) RFI survey. The remote location of the MWA results in a substantially cleaner RFI environment compared to LOFAR's radio environment, but adequate detection of RFI is still required before data can be analysed. We include specific recommendations designed to make the SKA more robust to RFI, including: the availability of sufficient computing power for RFI detection; accounting for RFI in the receiver design; a smooth band-pass response; and the capability of RFI detection at high time and frequency resolution (second and kHz-scale respectively).</p>
<p>RFI detection, often referred to as "data flagging", is one of the first steps in processing the data from any interferometer. One measure of the performance of an RFI detection method is its accuracy, which is often quantified by the average number of false-positive and true-positive detections resulting from the method. It is important to perform initial RFI detection and excision at high time and frequency resolution, because this increases detection accuracy and decreases the loss of data (Offringa et al. 2013a). Consequently, RFI detection has to work on large data volumes, and its computational cost is therefore a concern -in particular for manyelement arrays. In some projects, simple amplitude thresholding is used to mitigate the worst interference, which is computationally cheap but not very accurate. For example, a 3σ threshold is used for analysing PAPER data in Parsons et al. (2014). Several observatories or projects have designed pipelines that include more advanced RFI mitigation. Examples of such pipelines include <rs id="a12971928" type="software">AOFLAGGER</rs> <rs id="a12971929" type="bibr">(Offringa et al. 2010(Offringa et al. , 2012))</rs>, originally designed for LOFAR; <rs id="a12971930" type="software">FLAGCAL</rs> for preprocessing data from the Giant Metrewave Radio Telescope (GMRT; Prasad &amp; Chengalur 2012); <rs id="a12971931" type="software">PIEFLAG</rs> <rs id="a12971932" type="bibr">(Middelberg 2006)</rs> and <rs id="a12971933" type="software">MIRFLAG</rs> <rs id="a12971934" type="bibr">(Lenc 2010)</rs> mostly used for the Australia Telescope Compact Array (ATCA); and <rs id="a12971935" type="software">SER-PENT</rs> for preprocessing data from the Multi-Element Radio Linked Interferometer Network (e-MERLIN; Peck &amp; Fenech 2013). For RFI detection in MWA observations, LO-FAR's <rs id="a12971936" type="software">AOFLAGGER</rs> is used. It has been shown that this flagger has a good accuracy and is fast (Offringa et al. 2013a). It also has a library interfacefoot_0 , which allows it to be integrated in a pipeline.</p>
<p>To detect RFI in MWA observations, we have used <rs id="a12971937" type="software">AOFLAG-GER</rs> and implemented this as a standard MWA tool to flag all MWA data. <rs id="a12971938" type="software">AOFLAGGER</rs> is a general-purpose RFI flagging tool developed originally for LOFAR (Offringa et al. 2013a). Specific customizations, such as changing the threshold levels and expected smoothness of good data, can be made for Correlator output RMS with respect to frequency in an MWA high-frequency observation, calculated over all cross-correlated baselines and 112 seconds of data. In this band, the band-pass shows two large discontinuities over frequency, because the MWA receivers apply different digital gains at different frequencies to minimize quantization noise. The 1.28 MHz sub-bands have already been corrected for the band-pass shape of the poly-phase filter, but a residual 1.28-MHz pattern is visible due to aliasing. different telescopes to optimize the detection accuracy for different band-pass shapes, time and frequency resolutions, and fields of view. Strategies for several telescopes have been implemented in the <rs id="a12971939" type="software">AOFLAGGER</rs> software, including an MWA-specific strategy which is used in this work. In the LOFAR strategy, the sky contribution is estimated by applying a 2-dimensional high-pass filter on the visibility amplitudes of each baseline in the time and frequency domains. Subsequently, line-shaped features are detected by the SumThreshold method, which is a combinatorial threshold method (Offringa et al. 2010). After iterating these steps a few times, the scale-invariant rank (SIR) operator is applied on the two-dimensional flag mask. The SIR operator is a morphological technique to search for contaminated samples (Offringa et al. 2012).</p>
<p>In this work, <rs id="a12971940" type="software">AOFLAGGER</rs> version <rs id="a12971941" type="version" corresp="#a12971940">2.6</rs> released on 26 June 2014 is used.</p>
<p>The <rs id="a12971942" type="software">AOFLAGGER</rs> software provides a <rs id="a12971943" type="language" corresp="#a12971944">C++</rs> <rs id="a12971944" type="software" subtype="implicit">library</rs> that can be integrated in a pipeline such that intermediate data can be kept in memory. This minimizes the reading and writing of data. We have written an MWA-specific preprocessing pipeline named <rs id="a12971945" type="software">COTTER</rs>foot_1 that uses the <rs id="a12971946" type="software">AOFLAGGER</rs> library for RFI detection. RFI detection is only performed on cross-correlations. Auto-correlations are normally ignored, because they are not used in imaging. In addition to the RFI detection, <rs id="a12971947" type="software">COTTER</rs> performs the following steps: it converts the raw correlator files into CASA measurement sets or UV-FITS files; applies bandpass gain corrections; corrects the phases for varying cable lengths; calculates the u, v, wcoordinates; applies phase tracking to the desired sky coordinates; flags samples from the correlator that are missing or incorrect; and allows averaging the visibilities in frequency and/or time to reduce the data volume. It also collects various statistics and writes these into a measurement set using the LOFAR quality statistics formatfoot_2 . Tools are available to analyse these statistics, e.g. the <rs id="a12971948" type="software" subtype="component" corresp="#a12971949">AOQPLOT</rs> tool that is part of the <rs id="a12971949" type="software" subtype="environment">AOFLAGGER</rs> software can plot the statistics over various dimensions in an interactive manner.</p>
<p>MWA observations are split into snapshots of a few minutes by the correlator. The MWA archive stores the raw correlator outputs for each snapshot as an observation that can be referenced by its observation ID. For more details about the correlator, see Ord et al. (submitted). Currently, the <rs id="a12971950" type="software">COT-TER</rs> preprocessing pipeline is run by the scientist that calibrates and images the data. Once the scientist has downloaded the raw files for a given observation ID, there are various ways of processing MWA data. For imaging MWA data with the Real-Time System (RTS; Mitchell et al. 2008), <rs id="a12971951" type="software">COT-TER</rs> is run in a special mode such that it only flags the data, and does not convert the raw correlator files. After running <rs id="a12971952" type="software">COTTER</rs>, the raw files and a flag mask are given as input to the RTS. For data processing with the
Common Astronomy Software Applications (CASA; Jaegar 2008), <rs id="a12971954" type="software">MIRIAD</rs> <rs id="a12971955" type="bibr">(Sault et al. 1995)</rs>, <rs id="a12971956" type="software">WSCLEAN</rs> <rs id="a12971957" type="bibr">(Offringa et al. 2014)</rs> and/or the <rs id="a12971958" type="software">Fast Holographic Deconvolution</rs> software ( <rs id="a12971959" type="software">FHD</rs>, <rs id="a12971960" type="bibr">Sullivan et al. 2012</rs>), the <rs id="a12971961" type="software">COTTER</rs> output is set to either the CASA or UV-FITS format. The output is then directly readable by the most common astronomical software packages.</p>
<p>One particular issue in implementing <rs id="a12971962" type="software">COTTER</rs> is that both the raw correlator files and the desired output files are ordered in time, but flagging is done baseline by baseline. The SumThreshold and SIR operator algorithms that are used by the flagging strategy use statistics calculated over large time and frequency ranges. Therefore, detection accuracy is improved when the time-frequency flagging intervals are as large as possible. However, a typical snapshot is about 50 GB in size and without increasing disk I/O overhead it requires 50 GB of memory to perform flagging on the full data. When less memory is available, <rs id="a12971963" type="software">COTTER</rs> will split the observation into a number of shorter time segments and flag these independently. This is similar to the partitioning that is used in the <rs id="a12971964" type="software">NDPPP</rs> software used by
LOFAR <rs id="a12971965" type="bibr">(Pizzo 2014)</rs> to overcome the time-ordering problem. Partitioning the data has the undesirable consequence that executing <rs id="a12971966" type="software">COTTER</rs> on a low-memory machine decreases its flagging accuracy. As not all astronomers have easy access to large-memory machines, a platform with sufficient memory has been set up that runs a partial <rs id="a12971967" type="software">COTTER</rs> preprocessing on all observations. In this use-case, <rs id="a12971968" type="software">COTTER</rs> is run in a flagging-only mode on the large-memory machine. The astronomer downloads the raw files and the flagging output files, and reruns <rs id="a12971969" type="software">COTTER</rs> to apply the RFI detection from the first run and convert the raw files to his/her preferred output format.</p>
<p>When time or frequency averaging is requested, <rs id="a12971970" type="software">COTTER</rs> averages samples together that have not been flagged. When all input samples are flagged, the average of all input visibilities is stored into the output sample, and the output sample is flagged. This method makes it possible to superficially analyse flagged samples in the output, even though information is lost in the averaging. <rs id="a12971971" type="software">COTTER</rs> stores a weight for each visibility in the output file, which is scaled to the number of unflagged input samples that were used for the output sample. Because of this, when no averaging is requested, the output is 50% larger than the input (one extra float per complex float visibility). In practice, most MWA observations are recorded at a resolution of 0.5 s and 40 kHz, and averaged to a resolution of 2-4 s and 40-80 kHz to reduce the data volume. This does not cause any significant time or bandwidth decorrelation up to the first beam null for MWA data. <rs id="a12971972" type="software">COTTER</rs> performs the phase shifting and cable delay corrections before averaging, and recalculates the u, v, w-values for the central time and frequency of the output sample. This helps to prevent time/frequency decorrelation. The central time/frequency of an output sample is set to the time/frequency mean of the corresponding input samples, independently of what input samples are flagged.</p>
<p>The poly-phase filter bank that performs the first separation into sub-bands introduces a 1.28 MHz periodic spectral signature. As shown in Fig. 1, the poly-phase filter bandpass shape is hard to remove completely, because of aliasing each sub-band is affected by leakage from signals in its adjacent sub-bands. The leakage from adjacent sub-bands manifests itself as if the sub-band band-pass is direction dependent. To solve this, the bordering 80 kHz on both sides of a 1.28 MHz subband are normally flagged by <rs id="a12971973" type="software">COTTER</rs>. This implies that in normal observations, 13% of the data are lost due to the poly-phase filter. However, while 80 kHz is sufficient to prevent imaging artefacts, we noticed that the RFI statistics were still slightly biased by the edge channels. In particular, the detected fraction of RFI over frequency is about 0.5% higher (i.e., ∼ 1.5% instead of ∼ 1%) in the edge channels after flagging 80 kHz of the edges. Therefore, for the analyses in this paper we increase the removed bandwidth to 200 kHz on either side of each 1.28 MHz sub-band, or 32% in total.</p>
<p>The "RFI" column of Table 1 lists the fraction of samples that were classified as RFI by the <rs id="a12971974" type="software">AOFLAGGER</rs> in the crosscorrelations for each observing night. This includes false detections, which are estimated in Offringa et al. (2013a) to account for approximately 0.4% of detections. Compared to the EoR observations, the GLEAM observations have a higher average of 1.05% of RFI. This is mostly caused by the FM bands, which are only observed in GLEAM observations. The EoR low-band night shows 0.81% RFI, while the EoR high-band observations have an average of 0.58%. The total RFI occupancy in all observations is 0.96%. Weighting the occupancy in each channel by the inverse time that it is observed results in a global RFI occupancy of 1.13% in the 72.3-230.8 MHz range. 2014-03-17. The latter is the only night affected by interference from digital TV (DTV), and will be analysed later in this section. RFI occupancy is calculated as the percentage of discrete visibilities that are detected as RFI by the flagger at the resolution of the correlator output. The FM bands around 100 MHz and the ORBCOMM bands around 138 MHz are clearly present in the data. Excluding the RFI from DTV, the EoR high band is slightly cleaner than the EoR low band, and its worst subband at 188.2 MHz has 1.03% occupancy.</p>
<p>The residual noise levels after flagging can be used to validate whether the flagged data are free of RFI. In Fig. 3, the residual noise levels are plotted per high-resolution channel for each of the observations. Observation 'GLEAM 2014-03-17' shows residual DTV interference, both in the frequency range 174-195 MHz (radio frequencies (RF) 6, 7 and 8) and 216-230 MHz (RF 11 and 12), and it is clear that this RFI has not been adequately flagged. Therefore, DTV interference has to be detected with another method. Additionally, some channels in the FM radio band show higher standard deviations as a result of the smaller amount of available data after flagging and possibly because of RFI leakage. Nevertheless, because the effect is small these frequencies can be calibrated and imaged without a problem. FM-band RFI is noticeably worse when pointing at the southern horizon, however beyond this we do not have sufficient data to explore any correlation between pointing direction and RFI. The subband at 137 MHz that is occupied by ORBCOMM is hard to calibrate because of the small amount of residual data per channel, and possibly also because of residual RFI. With the exception of the ORBCOMM frequencies and DTV affected nights, the RFI detection employed in <rs id="a12971975" type="software">COTTER</rs> is sufficient to allow calibration and imaging without further RFI mitigation efforts. This has been verified by early imaging results from the GLEAM survey and the MWA commission- ing survey (Hindson et al. 2014;Hurley-Walker et al. 2014;Murphy et al. 2014, in press.).</p>
<p>Because significant DTV interference residuals are visible in 'GLEAM 2014-03-17' after flagging, we have analysed this night more extensively. The DTV transmitters are terrestrial, and the fact that only this night observes the DTV, implies the RFI must originate from an over-the-horizon transmitter that is reflected by unusually strong ionospheric activity or tropospheric ducting. As can be seen in the left plot of Fig. 4, this kind of RFI can fully contaminate frequencies 174-195 MHz, thus over half of the instantaneous bandwidth. Because the <rs id="a12971976" type="software">AOFLAGGER</rs> determines its thresholding levels from the data, and because it needs to be insensitive to steep RMS jumps over frequency due to the varying coarse channel gains (see Fig. 1), it is insensitive to RFI covering such a broad spectrum.</p>
<p>Although the flagger does not adequately flag DTV interference, the right plot of Fig. 4 shows that from the data statistics it is possible to determine whether a snapshot is af-fected by DTV interference. An option is to test whether the visibility RMS of a snapshot in any of the occupied DTV bands exceeds the RMS in the Digital Video Broadcasting (DVB) band 4 (167-174 MHz), which is not used for broadcasting. These statistics are calculated by <rs id="a12971977" type="software">COTTER</rs>, hence this can be validated without having to read the data again. When DTV is detected in a snapshot, the entire snapshot can be removed or the residual unaffected data within the snapshot can be used. This could be dealt with by accounting for lower SNR and a decreased uv coverage, however putting such a mechanism in place (rather than just deleting affected snapshots entirely) may not be worth the effort. Because 2 out of 7 hours are affected in 1 out of the 9 randomly selected nights, the probability that DTV interference occurs is roughly 3%. The bands involved are RF 5, 6, 7, 11 and 12, each of 7 MHz bandwidth. Therefore, when DTV interference occurs, it affects 35 of the 159 MHz bandwidth. The visibility ratios that are lost because of DTV RFI are detailed in the "DTV" column of Table 1. Because these frequencies overlap with the 21-cm HI frequencies redshifted to the EoR, in reality these frequencies are observed more often, so the actual loss is somewhat higher.</p>
<p>A variety of RFI events are observed in the data sets. While there are too many transmitters to show examples for each, it is helpful to understand what kind of events are visible and how they are flagged by the <rs id="a12971978" type="software">AOFLAGGER</rs>. Therefore, a few typical examples are shown. of a single correlation, in which a transmitter has been observed in the 2-m amateur band (146 MHz). This is the worst example of contamination in our data sets by this transmitter, and there are snapshots in which the transmitter is not visible at all. This variability might be caused by intrinsic variation, movement of the transmitter or varying propagation conditions. While a change in pointing can also change the appearance of the transmitter, the beam does not change within a single snapshot, while in Fig. 6 the strength and affected bandwidth of the transmitter does change during the snapshot. In the bottom panel of Fig. 6, the same snapshot is shown but is zoomed in on a briefly-observed RFI event at 150.17 MHz. This event occupies only a single 40 kHz channel for 2 seconds, and thus is an event which requires flagging at approximately the observed time and frequency resolution or higher for accurate detection.</p>
<p>Besides persistent transmissions that occupy a few channels, transient broad-band events are observed as well. Oc-casionally, DTV RFI is visible for a brief moment, as for example visible in Fig. 7. As can be seen in the right-hand plot of Fig. 7, such a brief interference event is well flagged by the <rs id="a12971979" type="software">AOFLAGGER</rs>. Another transient broad-band example is displayed in Fig. 8, which shows strong broad-band pulses of a second in length. Strong pulses such as these are rare and well flagged, but a few weak broad-band pulses are observed in almost every 2-min snapshot. These weak pulses are not visible nor detected in a single baseline correlation, but can be seen in a dynamic spectrum when the power on all baselines is added together. We currently do not know what their origin is. The MWA Voltage Capture System (VCS; Tremblay et al. submitted), which allows high time resolution observations, might help to analyse these signals.</p>
<p>Fig. 9 shows a longer RFI contamination at 156.66 MHz that is hardly visible in a single correlation. The <rs id="a12971980" type="software">AOFLAG-GER</rs> detects this RFI partially (Fig. 9, top-right plot), but when plotting the standard deviation over all correlations in 4.3 Computational performance of <rs id="a12971981" type="software">COTTER</rs> Because <rs id="a12971982" type="software">COTTER</rs> processes the data at high time and frequency resolution, its computational performance is an important consideration. A major contribution to the runtime is the reading and writing of the data, and the runtime is thus influenced by the input-output (IO) disk performance of the host system. Excluding IO, the main computational burden of <rs id="a12971983" type="software">COTTER</rs> consists of running <rs id="a12971984" type="software">AOFLAGGER</rs> on the data and collecting the statistics; performing all its other tasks such as applying the cable delays and averaging increases the runtime by approximately 1%. To time <rs id="a12971985" type="software">COTTER</rs>, we use a high-end desktop computer with 32 GB of memory and a 3.20-GHz Intel Core i7-3930K processor with six cores, and with a 5-disc RAID5 setup. The wall-clock runtime for processing a single 2 min snapshot of 50 GB with a 0.5 s / 40 kHz input resolution, using common averaging settings to output at 2 s / 80 kHz resolution, is split up as follows: 3 min are spent on reading the data, 5 min on RFI detection, and 3.5 min on writing the data. Real-time processing can therefore be achieved by using 6 of such nodes in parallel. Assuming a 138 GFLOPS (Giga-floating-point operations per second) performance of the host computer, the RFI detection requires 25 FLOP/visibility. When expressed as visibilities per time unit, the computational performance of the flagger is independent of the frequency resolution, time resolution and number of antennas. This number can therefore be extrapolated to other telescopes, although the performance of a pipeline which incorporates RFI detection will be strongly dependent on available IO performance, memory bandwidth and other system properties.</p>
<p>We have described the automated RFI detection strategy for the MWA and shown RFI statistics at various frequencies and over various nights. After RFI detection and excision, data can be calibrated and imaged without artefacts visible at the thermal or confusion noise level, with the exception of the ORBCOMM bands at 137 MHz. Also, DTV signals are seen ∼ 3% of the time, and when present make observing in the 174-195 MHz range impossible. Over the full GLEAM range of 72-231 MHz, 1.1% of the data are detected and flagged by the <rs id="a12971986" type="software">AOFLAGGER</rs>, and these RFI events are attributed to several different transmitters. Some residual RFI is seen in the FM-station bands, but these frequencies are usable after our described automated RFI detection. The issue of RFI has become smaller by building at a radio-quiet site, but still requires adequate mitigation. When observing continuously with the MWA, a few fast computing nodes are permanently required for real-time RFI detection.</p>
<p>SKA-low will be built at the same location as the MWA, and hence several lessons can be learned. First of all, it is clear that the SKA will need to be able to handle some amount of RFI. This requires computational power to perform the RFI detection, and a receiver signal path with headroom sufficient to avoid gain compression by RFI. Second, <rs id="a12971987" type="software">COTTER</rs> relies on the fact that a single computing node can still hold a reasonable amount of MWA data in memory for the RFI detection, but because of SKA's large number of elements and high time and frequency resolution, this will be a more challenging problem. Thirdly, flagging MWA data is slightly complicated due to the first poly-phase filter and digital gains. The extra per-subband gain correction that is required for the MWA makes the detection less stable, and the sub-band bandpass makes it harder to recognize RFI patterns in frequency direction. Therefore, for accurate RFI detection it is best to have a smooth response over a large instantaneous bandwidth. Finally, the presence of short and spectrally-narrow RFI events confirms that detection at high time and frequency resolution improves accuracy.</p>
<p>The <rs id="a12971988" type="software">AOFLAGGER</rs> RFI detection strategy, originally developed for LOFAR, works well for the MWA. Faint RFI events such as the ones in Figs. 8 and 9, or complex events such as the one in Fig. 7, are not adequately detected by a single-sample thresholding algorithm, but AOFLAGGER's SumThreshold and SIR-operator algorithms are able to flag such events. These algorithms have gained some popularity; besides the use of <rs id="a12971989" type="software">AOFLAGGER</rs> by individual astronomers, <rs id="a12971990" type="software" subtype="environment">MIRIAD</rs>'s <rs id="a12971991" type="software" subtype="component" corresp="#a12971990">PGFLAG</rs> task implements both the SumThreshold and SIR-operator algorithms, and the pipeline for eMER-LIN (Peck &amp; Fenech 2013) implements the SumThreshold algorithm. Nevertheless, other projects still use singlesample thresholding, e.g. PAPER (Parsons et al. 2014). Since strong RFI is seen practically everywhere, and because the apparent strength of RFI events will follow a power-law distribution, many faint transmitters will interfere with observations of any (terrestrial) radio observatory. Using algorithms with low sensitivity will detect fewer of these, and thus result in RFI becoming more quickly a problem in deep integration projects.</p>
<p>The current sensitivity of <rs id="a12971992" type="software">AOFLAGGER</rs> is enough for calibration and imaging of MWA data. However, <rs id="a12971993" type="software">AOFLAGGER</rs> does not perform well on continuous broadband RFI such as DTV, which is occasionally present due to tropospheric ducting or ionospheric activity. To remove DTV, a second detection round is required, and the current methodology to handle DTV RFI is to delete all affected snapshots, or even the entire night. This requires hardly any computational power, because the required visibility statistics are collected in <rs id="a12971994" type="software">COTTER</rs>. For deep-integration projects, such as the EoR projects, it might be that low-level RFI will show up at lower noise levels. One way to increase the sensitivity of the flagger would be to operate on the summed power of multiple baselines. Fig. 9 shows that this increases the detectability of certain RFI events significantly.</p>
<p>The documentation for the <rs id="a12971995" type="software">AOFLAGGER</rs> library interface can be found at <rs id="a12971996" type="url" corresp="#a12971995">http://aoflagger.sourceforge.net/doc/api/</rs></p>
<p>a dynamic spectrum, it is evident that this RFI event extends in time beyond what is detected (Fig. 9, bottom-left plot). The detection becomes more complete when the <rs id="a12971997" type="software">AOFLAG-GER</rs> is executed on the standard deviations over all baselines (Fig. 9, bottom-right plot). This kind of detection is currently not implemented in <rs id="a12971998" type="software">COTTER</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f343505442"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T16:38+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Animals were deeply anaesthetized by 3% isoflurane inhalation, decapitated and 300 mm parasagittal slices containing the primary somatosensory cortex were cut with a Vibratome (1200S, Leica Microsystems) within ice-cold artificial cerebrospinal fluid (ACSF) of the following composition (in mM): 125 NaCl, 3 KCl, 25 glucose, 25 NaHCO 3 , 1.25 NaH 2 PO 4 , 1 CaCl 2 , 6 MgCl 2 , saturated with 95% O 2 and 5% CO 2 (pH 7.4). Following a recovery period at 35 C for 45 minutes slices were stored at room temperature in the cutting ACSF. Slices were transferred to an upright microscope (BX51WI, Olympus Nederland BV) equipped with oblique illumination optics (WI-OBCD) and visualized using a 603 (1.00W) water immersion objective (Olympus). The microscope bath was perfused with oxygenated (95% O 2 , CO 2 5%) ACSF consisting of (in mM): 125 NaCl, 3 KCl, 25 glucose, 25 NaHCO 3 , 1.25 NaH 2 PO 4 , 2 CaCl 2 , and 1 MgCl 2 . Based on the myelin structure visualized in the bright-field image large L5 neurons with an intact axon parallel to the slice surface were targeted for simultaneous somatic, somato-dendritic or somato-axonal whole-cell current-clamp recording using Dagan BVC-700A amplifiers (Dagan Corporation, MN, USA). Bridge balance and capacitance were fully compensated based on small current injections leading to minimal voltage error (Figure S1). Voltage was analog low-pass filtered at 10 kHz (Bessel) and digitally sampled at 50 kHz for subthreshold data and 100 kHz for action potentials using an A/D converter (ITC-18, HEKA Elektronik Dr. Schulze GmbH, Germany) and data acquisition software <rs id="a12886224" type="software">Axograph X</rs> (v. <rs id="a12886225" type="version" corresp="#a12886224">1.5.4</rs>, <rs id="a12886226" type="publisher" corresp="#a12886224">Axograph Scientific</rs>, NSW, Australia). Patch pipettes were pulled from borosilicate glass (Harvard, Edenbridge, Kent, UK) pulled to an open tip resistance of 5-6 MU. The intracellular solution contained (in mM): 130 K-Gluconate, 10 KCl, 4 Mg-ATP, 0.3 Na 2 -GTP, 10 HEPES and 10 Na 2 -phosphocreatine (pH 7.4 adjusted with KOH, 280 mOsmol kg À1 ). Passive membrane responses were collected in the presence of a blocking solution in which 25 mM NaCl was replaced by 20 mM tetraethylammonium (TEA) chloride and 5 mM 4-aminopyridine (4-AP; a non-selective Kv1, Kv2 and Kv3 channel blocker) and by adding to the solution 1 mM tetrodotoxin (TTX) to block sodium channels, 20 mM ZD-7288 to block hyperpolarization-activated cyclic nucleotide-gated (HCN) channels, 10 mM XE-991 for Kv7 (KCNQ) channels and 0.2 mM CdCl 2 to block voltage-gated calcium channels. To further reduce synaptic depolarizations we added 20 mM of the AMPA receptor blocker 6-cyano-7-nitroquinoxaline-2,3-dione (CNQX).</p>
<p>For morphological reconstruction and/or nodal staining 5 mg ml À1 biocytin and/or 100 mM Alexa Fluor salts (488 or 594) were added to the intracellular solution. The 300 mm thick slices were rinsed in 0.1 M PBS and quenched in 3% H 2 O 2 twice for 30 min. Following increasing washing steps in fresh 0.1 M PBS sections were incubated in series of increasing sucrose solutions (10%, 20% and 30% in PBS for 45 minutes). The membrane was permeabilized by several cycles of rapid liquid nitrogen freezing and thawing after which they were incubated in 1% avidin-biotinylated horseradish peroxidase H complex (Vector Laboratories) overnight at 4 C. The peroxidase was localized with 0.05% 3,3 0 -diaminobenzidine peroxidase substrate chromogen for visualization. Slices were washed and mounted in Mowiol. Neuronal morphology was fully reconstructed, including axonal and dendritic branch lengths and diameters, using a 633 oil-immersion objective (numerical aperture NA of 1.4, Zeiss) coupled with the 3D tracing software <rs id="a12886227" type="software">Neurolucida</rs> (v. <rs id="a12886228" type="version" corresp="#a12886227">11</rs>, <rs id="a12886259" type="publisher" corresp="#a12886227">MicroBrightField Europe</rs>, Magdeburg, Germany). Consistent with previous observations (Kole et al., 2007), when comparing axon lengths based on the bright-field image during the physiological recording in slices with the final 3D reconstructions from the same cell, the shrinkage was found to be minimal in the x-and y-direction (&lt; 2%, n = 8). No correction factor was therefore applied to the neuronal reconstructions. The locations and dimensions of nodes of Ranvier are well visible as an increased intensity of the biocytin-DAB signal or presence of collaterals (Figures S2 andS3). To confirm the locations of reconstructed nodes of Ranvier (Figure S2) using immunofluorescence (Figure S3), L5 neurons (n = 8) were filled with biocytin and fixed for 20 minutes in 4% paraformaldehyde (PFA), then stored in 0.1 M PBS (pH 7.4). Sections were blocked in 5% normal goat serum (NGS) followed by 24 h incubation in primary antibodies diluted in 0.1 M PBS containing 5% NGS, and 2% Triton X-100. Sections were stained with rabbit anti-bIV-spectrin (1:200, gift from M. N. Rasband, Baylor College of Medicine, TX) and biocytin was visualized using Streptavidin-488.</p>
<p>Four of the six biocytin-filled L5 neurons used for modeling were recovered for EM. Coverslips were carefully removed and the object glass with the section was placed in milliQ water for several days. Afterward, sections were rinsed 0.1 M sodium cacodylate buffer pH 7.4 for a few hours and placed in a 1% osmium tetraoxide solution containing 1.5% potassium ferricyanide in 0.1 M sodium cacodylate buffer pH 7.4 for 10-15 mins. Sections were subsequently dehydrated in a sequence of ethanol dilutions, pure acetone and an acetone/epoxy resin mixture for 30-35 mins, followed by 30 mins in pure epoxy. Slices were then embedded between thin sheets of polymer and sealed at 60 C. For three L5 neurons filled with horseradish peroxidase (HRP) brain slices were fixed in 4% paraformaldehyde (PFA) and/or 5% glutaraldehyde and rinsed thereafter in 0.1 M sodium cacodylate buffer at pH 7.4. Slices were placed in a 25% sucrose solution in 0.1 M sodium cacodylate buffer (pH 7.4). When the slices were saturated, they were embedded in Tissue-Tek in an aluminum boat and frozen by dry ice. Sections were re-cut at 40 mm and subsequently rinsed in a Tris-HCl buffer (pH 7.4). To visualize the peroxidase, the sections were incubated in a Tris-HCl diaminobenzidine (DAB) solution containing 0.03% H 2 O 2 . The DAB reaction product was then intensified by a gold-substituted silver peroxidase. Sections were rinsed in a sodium cacodylate buffer of 0.1 M (pH 7.4) and post-fixed for 20 min in 1% OsO 4 supplemented with 1.5% potassium ferricyanide in a sodium cacodylate buffer of 0.1 M (pH 7.4). The material was subsequently dehydrated and embedded in epoxy resin, then cut in ultrathin sections. All ultrathin sections were examined and photographed with a FEI Tecnai G12 electron microscope (FEI, Europe NanoPort, Eindhoven, the Netherlands). Images were saved in tiff format and analyzed using <rs id="a12886231" type="software">Fiji</rs> (<rs id="a128862319" type="software">ImageJ</rs>) graphic software <rs type="bibr" corresp="#a12886231">(Schindelin et al., 2012)</rs>(v<rs id="a12886232" type="version" corresp="#a12886231">1.47p</rs>, <rs id="a12886233" type="publisher" corresp="#a12886231">NIH</rs>, USA).</p>
<p>To prevent artifacts with aldehyde fixation methods such as splitting, loosening or shrinking of intraperiod spaces in the myelin sheath we used HPF EM (Mo ¨bius et al., 2016) to examine the periaxonal width dimensions. Wistar rats (P80-90) were anesthetized and terminated by cervical dislocation. The brain was removed quickly and then cut with a Leica vibratome VT1200S in 200 mm sections. The cortex was cryofixed in 20% poly(vinyl-pyrrolidinone) (Sigma-Aldrich, Munich, Germany) using the high pressure freezer Leica HPM100 (Leica, Vienna, Austria). The freeze-substitution and the subsequent Epon-embedding of the tissue was carried out as described previously (Mo ¨bius et al., 2010;Snaidero et al., 2014) using the Leica AFS II. The Epon embedded tissue was cut with the Leica Ultracut S ultramicrotome in 500 nm semithin sections or in 50 nm ultrathin sections that were contrasted with 4% uranylacetate (SPI-Chem, West Chester, USA) (Mo ¨bius et al., 2010). Electron micrographs were obtained with the electron microscope LEO EM912AB (Zeiss, Oberkochen, Germany) equipped with an on-axis 2k CCD-camera (TRS, Moorenweis, Germany) using the <rs id="a12886234" type="software">ITEM</rs> (<rs id="a12886235" type="publisher" corresp="#a12886234">Olympus</rs>, Mu ¨nster, Germany) software. For quantification of the periaxonal width, only the intercellular distance between clearly cross-sectioned outer axonal and myelin membranes was measured. Between 51 and 73 measurements were made from 20 images per animal (n = 3).</p>
<p>Electrophysiological recordings were combined with reconstructed morphologies in the <rs id="a12886236" type="software">NEURON</rs> simulation environment (v. <rs id="a12886237" type="version" corresp="#a12886236">7.3-7.5</rs>) <rs id="a12886260" type="bibr" corresp="#a12886236">(Hines and Carnevale, 2001)</rs> and <rs id="a12886239" type="software" subtype="implicit">custom-written
software</rs>. Neuron morphologies were uploaded to <rs id="a12886240" type="software" subtype="environment">NEURON</rs> via its <rs id="a12886241" type="software" subtype="component" corresp="#a12886240">Import3D</rs> tool.</p>
<p>Nodal and internodal domains were incorporated into the morphological reconstructions as described above. Paranodal domains were implemented post hoc with a fixed length of 2.3 mm, based on our longitudinal EM data and consistent with previous estimates in mammalian central nervous system (Shepherd et al., 2012). An evolutionary algorithm was developed to robustly search the parameter space of each circuit model, running for a total modeling time of over 3 million core hours on the Comet and Stampede2 supercomputers at the Neuroscience Gateway (Sivagnanam et al., 2013). The massively parallel computational approach enabled each parameter space to be thoroughly and unbiasedly searched, and resulted in thousands of unique solutions for each circuit implementation, with the unbiased selection of the lowest error solution representing each simulation. To substantially reduce runtimes (up to 50%), DC models were run in a recompiled version of <rs id="a12886242" type="software">NEURON</rs> to the necessary 1 extracellular layer instead of the default 2 (currently available at the Neuroscience Gateway), allowing for the combined modeling of V m and V my (v and vext[0], respectively, in the single layer extracellular version of <rs id="a12886243" type="software">NEURON</rs>).</p>
<p>An evolutionary algorithm for model optimizations was employed for the following reasons: 1) to find with an unsupervised approach the lowest available error within the solution search-space; 2) to compare multiple circuit-specific solutions statistically to a randomized one for parameter, as well as direct circuit-to-circuit, sensitivity analysis and 3) to solve the initial parameter value sensitivity problem by stochastically canvasing the initial value space. 1) was achieved via 3) and a modified optimization procedure embedded within the evolutionary algorithm, based on Brent's PRAXIS method (Brent, 1973), which is built into <rs id="a12886244" type="software">NEURON</rs>. Briefly, our optimization procedure addresses the problem of non-uniformly distributed parameters, such as myelin parameters, being less constrained than uniformly distributed ones, such as cellular membrane C m (over an entire cell), by constraining optimization to experimental noise. Essentially, optimization was forced to exit if error improvement was marginal and within experimental noise (defined as signal variance over the delay to current injection). This approach led to greater simulation efficiency, producing runtimes orders of magnitude faster than the default search routine. To further maximize PRAXIS efficiency, parameters were normalized and log transformed.</p>
<p>The authors are grateful to Ted Carnevale and Michael Hines for their critical reading of the manuscript and support with <rs id="a12886245" type="software">NEURON</rs>. We also thank the entire Neuroscience Gateway (NSG) team for excellent computing support (UC, San Diego). Greg Stuart, Romain Brette, and Stefan Hallermann provided constructive feedback on earlier versions of this manuscript. The study has been funded in part by grants from the ERC (Starting Grant 261114), the National Multiple Sclerosis Society (RG 4924A1/1), and the Netherlands Organization for Scientific Research (NWO Vici 865.17.003) to M.H.P.K., as well as by ERC Advanced Grants AxoGLIA and MyeliNANO to K.N. Part of this work (M.W.) was funded through the Cluster of Excellence and DFG Research Center Nanoscale Microscopy and Molecular Physiology of the Brain, Go ¨ttingen, Germany. We are grateful to the Electron Microscopy Centre Amsterdam (EMCA) of the Amsterdam UMC providing additional support.</p>
<p>The source data for Figure 1 in the paper and alternative circuit implementations is available on Mendeley (https://data.mendeley. com/) via https://doi.org/10.17632/xkh45t8dmm.1. All <rs id="a12886246" type="software" subtype="implicit">custom software</rs> for data processing, analysis, automated optimization and model comparisons are available from
GitHub ( <rs id="a12886247" type="url" corresp="#a12886246">https://github.com/Kolelab</rs>). The <rs id="a12886248" type="software">NEURON</rs> model of cell #6 with a double cable implementation of myelin to generate Figure 7 and Videos S3 and S4 in this paper can be downloaded from ModelDB (https://modeldb. yale.edu/260967).</p>
<p>The authors declare no competing interests. Onset latency was defined as the delay to half-maximum amplitude. Conduction velocity was defined according to the onset time of the first rapidly changing phase of the AP, i.e., its voltage threshold (Kole and Stuart, 2008). For recorded APs, threshold onset was defined as a voltage rate-of-rise minimum in dV/dt greater than 33 the standard deviation of the recording noise (initial, non-currentinjected part of the voltage response), and ranged from 60-120 V s -1 . For model APs, threshold onset was defined directly as occurrence of the first peak in d 3 V/dt 3 . The distance used in determining conduction velocity was the path length between the two given locations, determined by morphological reconstruction and <rs id="a12886249" type="software">NEURON</rs>'s distance function. All data plotting and statistical analyses were performed in <rs id="a12886250" type="software">Prism</rs> (version <rs id="a12886251" type="version" corresp="#a12886250">7</rs>, <rs id="a12886252" type="publisher" corresp="#a12886250">GraphPad</rs>), <rs id="a12886253" type="software">Igor Pro</rs> (version <rs id="a12886254" type="version" corresp="#a12886253">6.37</rs>, <rs id="a12886255" type="publisher" corresp="#a12886253">WaveMetrics</rs>) or <rs id="a12886256" type="software">SPSS</rs> (version <rs id="a12886257" type="version" corresp="#a12886256">23</rs>, <rs id="a12886258" type="publisher" corresp="#a12886256">IBM Analytics</rs>). Details of the statistical analyses, including tests, representation and value of n, center, dispersion and precision measures can be found in corresponding figure legends. Normality (D'Agostino Pearson omnibus) was tested wherever possible to justify the use of parametric tests; otherwise non-parametric tests were used.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f589952941"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:33+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Artificial intelligence is paving the way for a new era of algorithms focusing directly on the information contained in the data, autonomously extracting relevant features for a given application. While the initial paradigm was to have these applications run by a server hosted processor, recent advances in microelectronics provide hardware accelerators with an efficient ratio between computation and energy consumption, enabling the implementation of artificial intelligence algorithms 'at the edge'. In this way only the meaningful and useful data are transmitted to the end-user, minimising the required data bandwidth, and reducing the latency with respect to the cloud computing model. In recent years, European Space Agency is promoting the development of disruptive innovative technologies on-board Earth Observation missions. In this field, the most advanced experiment to date is the Φ-sat-1, which has demonstrated the potential of Artificial Intelligence as a reliable and accurate tool for cloud detection on-board a hyperspectral imaging mission. The activities involved included demonstrating the robustness of the Intel Movidius Myriad 2 hardware accelerator against ionising radiation, developing a <rs id="a12972315" type="software">Cloudscout</rs> segmentation neural network, run on Myriad 2, to identify, classify, and eventually discard on-board the cloudy images, and assessing of the innovative Hyperscout-2 hyperspectral sensor. This mission represents the first official attempt to successfully run an AI Deep Convolutional Neural Network (CNN) directly inferencing on a dedicated accelerator on-board a satellite, opening the way for a new era of discovery and commercial applications driven by the deployment of on-board AI.</p>
<p>The <rs id="a12972316" type="software">CloudScout</rs> network, shown in Fig. 10, was inspired by U-Net [25] which is a network used to segment different scenes, with particular attention to False Negative values [35]. Moreover, this network owes its success to the low number of training images required compared to the mean Intersection over Union (mIoU) obtained. Exploiting the same criteria, the <rs id="a12972317" type="software">CloudScout</rs> network uses only the lowest section of the U-Net.</p>
<p>The Receiver Operating Characteristic (ROC) analysis in Fig. 12 shows the variation of the performance with respect to each pixel confidence score threshold value of the last layer. This threshold represents the minimum confidence score needed by the network to define pixels cloudy or not cloudy, providing a fine control of the final output. Furthermore, it is not applied by the NN, but it is computed by the on-board processor and can be changed to adjust the percentage of FP/FN without retrain the network. The red dot represents the best trade-off for the <rs id="a12972318" type="software">CloudScout</rs> network in terms of pixel-wise accuracy and False Positive Rate, as shown by the Confusion Matrix in Table II. This is obtained using a threshold of 0.6 on the output mask of the last layer. The final pixel-wise accuracy is 88.4%. Although this is indicative of the overall quality of the inference, the main parameter that sets the performance and represents the actual index of quality is the false positive (FP) rate. The main reason for this is that using this inference in an operative mission to decide which of the images are worth downloading to ground and which can be discarded on-satellite, the false positives, being images actually not cloudy but detected as cloudy, represent the net loss of good data, containing useful information, that is discarded. Therefore, the chosen quality index is the percentage of FP which for pixel classification is equal to 5.6% (Tab.II). Moving from pixel classification back to image classification, with the definition of cloudy images as the images whose percentage of cloudy pixels is higher than 70%, the associated confusion matrix is that of Table III. It is worth noting that the 88.4% of accuracy of the segmentation algorithm corresponds to 95.1% of tiles accuracy with only 0.8% of the images were classified as False Positive within the synthetic dataset. It is possible to further reduce the number of FPs by increasing via software the threshold of the last layer, at the expenses of some percentage reduction in the pixel wise accuracy. The inference time is approximately 102 ms exploiting 8 of the 12 available SHAVE vector processors of the
Myriad 2, and consuming only 62.9 KB of memory footprint.</p>
<p>The <rs id="a12972319" type="software">CloudScout</rs> Segmentation has gone through two completely different testing phases. The first was conducted during the design phase at the end of the training and validation stages, using the synthetized data set, and the second one was conducted in-flight using the available images acquired by
HyperScout-2 during the Φ-Sat-1 mission. While the first testing phase aimed to assess the network's capabilities and performances using the synthetic dataset to validate the inference against the requirements, the second phase was a direct evaluation of the performance exploiting the hypercubes acquired in-flight during the Φ-Sat-1 mission by the HyperScout-2 camera. The results of the first phase have been already presented and summarised in the confusion matrix in Table III. As already highlighted they respect the most stringent requirement: false positive under 1% on images for the entire test set.</p>
<p>In addition to the <rs id="a12972320" type="software">CloudScout</rs> results acquired during Φ-Sat-1's mission, in-flight performance data was also acquired for the EoT inference engine. Four separate EoT hardware test phases were executed over a 70 day period of the mission. During each phase the EoT BIST routine was initiated, which performed both EoT chip-level and board-level diagnostics and reported back results. Chip-level tests coverage encompassed memories, caches, interfaces, and functional tests to dynamically exercise the SHAVEs and multi-ported memory. Board-level diagnostics evaluated the PMU, flash and SD card. The executed diagnostics and their results are summarised in Table VI. It is seen that every diagnostic test passed at each phase, with the exception of the SD card test. 2 and 3 bit errors were observed in two of the data readbacks from the SD card, where the temporal gap between write and readback was 41 seconds. Note that the SD card functionality was not used on the Φ-Sat-1 mission. Test 027A included an additional 240 second test run during which NN inference with an exemplar TinyYOLO [36] model was continuously executed, with all inference outputs exactly matching the reference values. The in-flight diagnostics tests for the EoT inference engine indicate that the device performed as expected onboard Φ-Sat-1 without experiencing any functional upsets, or any functional degradation effects due to radiation. All future installations of the Myriad VPU in space will be equipped with this Built In Test (BIST) that will allow to monitor correct performance of the hardware in time. The
HyperScout-2 (Sec.III-A), developed and produced by Cosine Remote Sensing B.V., is a hyperspectral camera based on a 2D sensor used in push broom mode. This Hyper-Scout model provides hyperspectral imaging in the visible and near infrared to analyze the Earth composition, along with three thermal infrared bands to retrieve the temperature distribution, boosting and improving the number of Earth Observation applications. As part of the Φ-Sat-1 mission it has been demonstrated that it is possible to run the full preprocessing chain before inference onboard the HyperScout OBDH, including computing spectral data cubes at pixel accuracy without relying on platform ADCS data but instead solely on machine vision techniques, allowing robustness and independence from small satellites' performance.</p>
<p>In order to demonstrate the potential of using AI directly on board, the <rs id="a12972321" type="software">CloudScout</rs> segmentation neural network was developed by the <rs id="a12972322" type="software">University of Pisa</rs> (Sec.IV-B). It assigns to each pixel a binary classification: cloudy or not cloudy. The <rs id="a12972323" type="software">CloudScout</rs> NN exploits only 3 of the 10 bands available from the
HyperScout 2 for two main reasons: memory constraints on the Myriad 2, and power limits derived from the satellite power budget. Hence, to perform the NN training, a synthetic dataset was developed by Sinergise (IV-A), starting from the Sentinel 2 images. The dataset was built following three phases: I) Principal component analysis to select the best bands combination for our goal; II) re-binning the Sentinel-2 images from 10m GSD to 70m GSD; III) using the Sinergise s2cloudless algorithm to construct a label/ground truth mask for the input images.</p>
<p>The training of the network aimed to obtain the highest accuracy while maintaining a low number of false positives. Maintaining a low false positive rate is of paramount importance for the application, as images wrongly classified as cloudy would be not transmitted to ground, resulting in a potential loss of interesting data. The <rs id="a12972324" type="software">CloudScout</rs> neural network was tested on both synthetic images on ground, and subsequently in-orbit on Φ-Sat-1 for live images acquired from the HyperScout-2 sensor. In both cases the solution demonstrated an accuracy in excess of 95.9% with respect to the tile level and a commensurate low FP rate, achieving a 96% accuracy when performing cloud detection on live images on-satellite as stated in Tab.V.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f533450706"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:42+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>All analyses were conducted using <rs id="a12894655" type="software" subtype="environment">R</rs> version <rs id="a12894656" type="version" corresp="#a12894655">4.0.0</rs> <rs id="a12953670" type="bibr">[46]</rs>, with packages " <rs id="a12894658" type="software" subtype="component" corresp="#a12894655">plm</rs>" and " <rs id="a12894659" type="software" subtype="component" corresp="#a12894655">pvclust</rs>" [<rs id="a12894660" type="bibr">47</rs>,<rs id="a12894661" type="bibr">48</rs>]. <rs id="a12894662" type="software" subtype="implicit">Code</rs> is available at <rs id="a12894663" type="url" corresp="#a12894662">https://github.com/yangclaraliu/COVID19_ NPIs_vs_Rt</rs>.</p>
<p>This study relies entirely on data that are either publically available or from published literature. <rs id="a12894664" type="software" subtype="implicit">Code</rs> used can be found at [<rs id="a12894665" type="url" corresp="#a12894664">https://github.com/ yangclaraliu/COVID19_NPIs_vs_Rt</rs>].</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f208135961"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T05:56+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The results show that coverage of funding information differs significantly among Scopus, Web of Science, and PubMed databases in a sample of the same medical journals. Moreover, we found that, currently, funding data in PubMed is more difficult to obtain and analyze compared with that in the other two databases. brought to you by <rs id="a12952187" type="software">CORE View</rs> metadata, citation and similar papers at core.ac.uk provided by Crossref Kokol a n d B la ž un V ošn er</p>
<p>WoS and Scopus databases allowed us to directly extract the number of all articles and number of FAs for each journal using built-in services. However, for PubMed, we first exported the corpus to BibTex and then to <rs id="a12952221" type="software">MS
Excel</rs> ( <rs id="a12900873" type="publisher" corresp="#a12952221">Microsoft</rs>, USA), in which we performed the analysis using the <rs id="a12900874" type="software" subtype="component" corresp="#a12952221">crosstab</rs> function.</p>
<p>A recent study reported that articles and reviews were the most consistently covered publication types that contained funding information in WoS <rs id="a12952220" type="bibr">[14]</rs>. Articles were also the most common publication type identified in our study. However, in addition to articles, we found considerable numbers of editorials and letters that contained funding information in WoS and PubMed. Hence, the notable presence of editorials and letters among funded publication types might also partially explain differences among the three databases.</p>
<p>We thank the journal editor and reviewers for their constructive comments and suggestion of an algorithm that enabled us to transfer PubMed funding data to <rs id="a12900914" type="software">Excel</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f555914157"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T08:15+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Frequency of social network site use was assessed with the question: ''How often did you visit <rs id="a12951646" type="software">Hyves.nl</rs> in the past 6 months?'' The response options were 0 (never), 1 (sometimes), 2 (regularly), 3 (often), and 4 (always) (M = 2.4, SD = 1.5 at time 1; M = 2.6, SD = 1.4 at time 2). A single item was used as recommended for constructs that are concrete and singular (Bergkvist and Rossiter 2007). Although there were other social network sites on which some adolescents had a profile (e.g., <rs id="a12951626" type="software">MySpace</rs>), <rs id="a12951647" type="software">Hyves.nl</rs> was the dominant and most popular social network site at the time of the study (like <rs id="a12951628" type="software">Facebook</rs> is currently). Of adolescents aged 12-17, 75 % had a profile on the website (Mijn Kind Online 2009). The current survey also assessed the use of other social network sites that were around at the time of the survey. However, the use of these websites in this sample was negligible and, therefore, we only focused on popular social network site <rs id="a12951629" type="software">Hyves</rs>. <rs id="a12951648" type="software">Hyves.nl</rs> was comparable to <rs id="a12951631" type="software">Facebook</rs> in terms of its goal, set-up, and technological possibilities. Important for the current study is that on <rs id="a12951632" type="software">Hyves</rs> (like on <rs id="a12951633" type="software">Facebook</rs>, <rs id="a12951634" type="software">Instagram</rs>, <rs id="a12951635" type="software">Rate</rs>, and <rs id="a12951636" type="software">Bebo</rs>), posting personal photographs and comments on friends' profiles played a central role (Mijn Kind Online 2009).</p>
<p>First, correlations were calculated between all measures at both time points for the sample as a whole and for boys and girls separately. Because the distributions of the scores on our key measures were skewed we calculated non-parametric correlations in <rs id="a12951637" type="software">Stata</rs> <rs id="a12951638" type="version" corresp="#a12951637">12</rs>, namely Kendall's tau-a, and converted this value to an approximation of Pearson's r using Greiner's relation to aid interpretation, as recommended by Newson (2002). Subsequently, we tested the first hypothesis and the second set of hypotheses in four separate models (see Figs. 1, 2, 3 and 4) using structural equation modeling in <rs id="a12951639" type="software">SPSS</rs>, <rs id="a12951640" type="software">AMOS</rs> version <rs id="a12951641" type="version" corresp="#a12951640">19</rs>. To test the direction of effects hypothesized in H1, H2a, and H2b, we used three cross-lagged models (Figs. 1, 2 and 3), testing both the hypothesized effect and the effect in the opposing direction. Then we modeled the hypothesized mediation effect postulated in H2c using the mediation model displayed in Fig. 4. In order to test the hypothesized moderation of gender specified in the third set of hypotheses, the models were subjected to multiple group analysis.</p>
<p>The assumption of multivariate normality required for the traditional parametric tests was not met according to results of Shapiro-Wilk tests. In addition, Breusch-Pagan/ Cook-Weisberg tests showed evidence of heteroscedasticity. To alleviate statistical problems due to violation of the assumption of normality we applied the bootstrap method to all models (1,000 bootstrap samples, N = 604 each) (Efron and Tibshirani 1993), and based our conclusions both on the bootstrap bias-corrected and accelerated 95 % confidence intervals as well as on the results of the parametric tests for the estimates. We only considered a hypothesis supported if the results of both tests were consistent. In addition, because parametric regression-based tests with heteroscedastic data can yield biased and inconsistent standard errors, resulting in Type 1 errors, we also conducted regression analyses with heteroscedasticityconsistent standard errors (hc3) in <rs id="a12951642" type="software">Stata</rs> <rs id="a12951643" type="version" corresp="#a12951642">12</rs>, as recommended by Hayes and Cai (2007).</p>
<p>A number of variables may impact social network site use, peer appearance-related feedback, and/or body dissatisfaction: BMI, parental monitoring of the adolescent's online behavior, age, pubertal status, and socioeconomic status. These variables may thus act as third variables in the hypothesized models. As our models control for past behavior of each outcome, the models are expected to be fairly robust against third variable effects. However, as confounding effects may still occur, we also ran the models (Figs. 1, 2, 3 and 4) controlling for these factors. The control factors were entered in the model as manifest variables, covaried with the predictor variables, and paths representing the influence of the control factors on the outcome variables were drawn. The results are not included in the body of text of the results section, as we had not formulated specific hypotheses about the impact of these variables. Moreover, <rs id="a12951644" type="software">AMOS</rs> does not allow for analyses with missing cases and including the covariate BMI would result in a smaller and biased sample (525/604) because some adolescents did not report their height or weight. We provide information regarding these additional analyses in footnotes 1-4.</p>
<p>The second set of hypotheses together stated that social network site use would predict increased peer appearancerelated feedback (H2a), which in turn would predict increased body dissatisfaction (H2b), and therefore frequency of social network site use would indirectly lead to body dissatisfaction through peer appearance-related feedback (H2c). The second set of hypotheses was also tested in the <rs id="a12951645" type="software">AMOS</rs> model displayed in Fig. 4, in which the effects of social network site use (time 1) on peer appearance-related feedback (time 1 and time 2), and of peer appearancerelated feedback (time 1) on body dissatisfaction (time 2) were modeled, in addition to the autoregressive paths. This model yielded a good fit, v 2 (df = 98, N = 604) = 180.88, p = .000, CFI = .98, RMSEA = .04 (90 % CI .03/.05).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f81202658"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:57+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>For quantitative measurements, the stack of eight contiguous short-axis slices of both cine CMR images was assessed independently by the two radiologists using the dedicated software package <rs id="a12972355" type="software">SYNAPSE VINCENT</rs> ( <rs id="a12972356" type="publisher" corresp="#a12972355">Fujifilm Corp., Ltd</rs>, Tokyo, Japan). The epicardial and endocardial contours were automatically traced on the short-axis images. Contours rendered by automated analysis were reviewed and manually corrected, as necessary. The endocardial trabeculations and papillary muscles of the left ventricle were included in the LV cavity volumes [22][23][24]. The most basal slice with at least a semicircular muscular ring at the end-systolic phase was regarded as the base, and the most apical slice with a visible cavity at end-diastolic phase was regarded as the apex [25]. The LV volume and LV mass were calculated using the Simpson method. The end-systolic and end-diastolic phases were detected automatically by software, based on the smallest and largest LV volumes over the entire cardiac cycle.</p>
<p>The continuous data are expressed as the mean ± the standard deviation (SD) or as the median (first quartile, third quartile), as appropriate, based on distribution. The Wilcoxon matched-pairs signed-rank test was used to compare image quality between standard cine CMR and CS cine CMR. The interobserver agreement on image quality was determined using the kappa test. The paired t test was used to compare the scan time. The results of EDV, end-systolic volume (ESV), SV, LV mass, and EF on standard cine CMR and CS cine CMR were compared with the Wilcoxon matched-pairs signed-rank test. Linear regression and Bland-Altman analysis were used to evaluate the correlation and agreement between these LV measurements. In addition, interobserver and intraobserver variabilities in CS cine CMR were also determined by the same analysis. A p value of less than 0.05 was considered statistically significant. All statistical analyses were performed by commercially available software ( <rs id="a12972357" type="software">JMP</rs> version <rs id="a12972358" type="version" corresp="#a12972357">11</rs>; <rs id="a12972359" type="publisher" corresp="#a12972357">SAS Institute</rs>, Cary, NC, USA). Sample size calculation was based on the primary outcome of the difference between the EF measures obtained from the two cine methods. Seventy-four participants were calculated to provide 80 % power to detect more than 5 % absolute difference in EF measures with a two-sided significance level of 0.05, assuming a common SD for the mean EF measurement of 15 %. The EF margin was considered the clinically acceptable range, based on previous research [26][27][28][29][30]. We ultimately enrolled 90 participants with the expectation of 18 % attrition.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f322335236"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T14:22+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>In order to facilitate decision-making in selecting suitable solutions where there are more than one objectives, there have been some methodologies in place which can be classified under the categories of priori and multiobjective optimisation (MOO) approaches (Wang et al. 2014;Ascione et al. 2014). Most of the developed methods are simulation-based optimisations in which the optimisation algorithms are implemented using a programming language, and the energy-related objectives (energy consumption or gas emission) are calculated by a Building Performance Simulation (BPS) tools such as <rs id="a12895625" type="software">EnergyPlus</rs> <rs id="a12899242" type="bibr">(Crawley et al. 2001)</rs>, <rs id="a12895627" type="software">TRNSYS</rs> (<rs id="a12895628" type="publisher" corresp="#a12895627">University
of Wisconsin-Madison</rs> 2015), <rs id="a12895629" type="software">ESP-r</rs> (<rs id="a12895630" type="publisher" corresp="#a12895629">The Energy Systems Research Unit (ESRU)</rs> 2011), etc. This approach limits the computation complexity of the algorithm to BPS's calculation time, in essence when a large number of solutions are defined the process may become extremely costly to handle. This time overhead is the main reason that most related studies have only investigated simple models or retrofitted only one or two parts of the studied envelopes. For the same reason, most of the studies targeted residential buildings, and there are only few reports on optimisation of retrofitting commercial properties (Smarra et al. 2018).</p>
<p>Wong et al. 2010 used ANN for assessing the dynamic energy performance of a commercial building with daylighting in Hong Kong. <rs id="a12895631" type="software" subtype="environment">EnergyPlus</rs> software along with <rs id="a12895632" type="software" subtype="implicit">algorithms for calculation of interior reflection</rs> is applied to generate the building daily energy usage. Nash-Sutcliffe Efficiency Coefficient (NSEC) is used as the primary measurement to investigate ANN accuracy in predicting cooling, heating, electric lighting and total electricity consumption.</p>
<p>ANN can be used for determination of parameters for energy performance assessment of buildings. Lundin et al. 2004 proposes a method for prediction of total heat loss coefficient, the total heat capacity and the gain factor that are key elements in the estimation of energy efficiency. Buratti et al. 2014 employs ANN as a tool for evaluation of building energy certificates accuracy using 6500 energy labels in Italy. The study investigates a different combination of input variables to minimise the number of training features. Using the outcome of the ANN, a new index is proposed to check the accuracy of declared data for energy certificates with a low error of 3.6%. Hong et al. 2014 applied ANN for benchmarking of schools buildings in the UK and investigate the limitations of the assessment. An extensive database including 120000 DEC records is used for training and testing the model (Hong et al. 2014b). Reviewing outcomes of the research and comparison with bottom-up models, authors suggest the combinational use of top-down and bottomup methods to achieve higher accuracy. Khayatian et al. 2016 predicts energy performance certificates for residential building using an ANN model and Italian <rs id="a12895633" type="software">CENED</rs> database as training records. A combination set of direct and calculated features are used as inputs and heat demand indicators (derived using <rs id="a12895634" type="software">CENED</rs> software) as the output target of ANN.</p>
<p>Ascinoe et al. 2017 proposed an ANN for evaluation of energy consumption and inhabitants' thermal comfort to predict energy performance of the building. Energy assessment of the buildings are performed using <rs id="a12895635" type="software">Energy-Plus</rs> software, and a simulation-based sensitivity/ uncertainty analysis is proposed for further improvement of network parameters. New buildings and retrofitted stock in presence of energy retrofit measures are considered separately. For the latter case, ANN is employed for optimisation of retrofit parameters. For the first one, three single output ANN is developed to predict primary energy consumption of space heating and cooling and the ratio of yearly discomfort hours by setting whole-building parameters as network inputs (i.e. geometry, envelope, operation and HVAC). At the same time, Beccali et al. 2017 propose the use of ANN fast forecasting as a decision support tool for optimising the retrofit actions of buildings located in Italy.</p>
<p>Kalogirou &amp; Bojic (Kalogirou and Bojic 2000;Kalogirou 2000) applies RNN to predict hourly energy demand of a passive solar building. <rs id="a12895636" type="software">ZID</rs> software has been employed to calculate the output target. Although results demonstrate high accuracy of estimation, the number of input features (season, insulation, wall thickness and time of the day) and total training records (forty simulated cases) are insufficient. Later in 2001, Kalogitrou (Kalogirou et al. 2001) applies ANN for estimating the daily heat loads of model house buildings with different calumniations of the wall (single and double) and roof (different insulations) types using a typical meteorological data for Cyprus. In this study, <rs id="a12895637" type="software">TRNSYS</rs> software was used as an energy evaluation engine for all cases and the data validated by comparison of one building energy consumption with the actual measurement. Karatasou et al. 2006 develops an FFN model for hourly prediction of energy loads in residential buildings. The impact of various parameters on the accuracy of a trained model is also investigated, and it is shown that parameters such as humidity and wind speed are less significant and can be eliminated from training features. Furthermore, the application of statistical analysis for enhancement of ANN model and 24 hours ahead prediction of energy consumption is demonstrated. These methods consist of hypothesis testing, information criteria and cross-validation in pre-processing and model development. However, there is less enlightenment about the main distinctions of applied FFN models. In 2010, Dombayci (Dombayci 2010) used ANN to prediction hourly energy consumption of a simple model house based on Turkish standards. The degree-hour method is applied to derive the hourly energy consumption to be used in ANN training. The models are suitable for single building energy management of simple residential buildings as it does not take many characteristics into account.</p>
<p>Neto &amp; Fiorelli 2008 compared predicted energy demand of a building in Brazil using ANN model and simulation software, <rs id="a12895638" type="software">EnergyPlus</rs>. The research investigates the impact of using hidden layer showing an insignificant difference in accuracy of the models. Furthermore, it reveals that external temperature is more important than humidity and solar radiation in estimating energy consumption of the study case. The authors show that ANN is more accurate that detailed simulation model, especially in short-term prediction. They conclude that improper assessment of lighting and occupancy would be the main reason for uncertainty in engineering models. Popesco et al. 2009 developed an original simulation and ANNbased models for predicting hourly heating energy demand of buildings connected to district heating system. Climate and mass flow rate variables of prior 24h are used as inputs. Deb et al. 2016 also used five previous day's data as ANN model inputs to forecast daily cooling demand of three institutional buildings in Singapore.</p>
<p>Ben-Nakhi 2004 used a general RNN for prediction of public buildings profile of the next days using hourly energy consumption data, intending to optimise HVAC thermal energy storage. Data from a public office building in Kuwait constructed from 1997 to 2001 is used for training and testing the ANN model. Energy consumption value of buildings is calculated using <rs id="a12895639" type="software">ESP-r</rs> simulation software and considering climate information, various densities of occupancy and orientation characteristics. The results show that ANN only needs external temperature for accurate prediction of cooling loads, whereas simulation software demand for intricate climate detail. Hou et al. 2006 predicted hourly cooling loads in an air-conditioned building integrating rough set theory and ANN. Input features of ANN are determined and optimised by analyses relevant parameters to cooling load using rough set theory. The proposed model with different combinations of input sets is compared with the autoregressive integrated moving-average model all showing better accuracy. Yokoyama et al. 2009 used back-propagation ANN to predict cooling load demand by introducing a global optimisation method for the improvement of network parameters. The effect of the number of hidden layers and the number of neurons in each layer is investigated to optimise the accuracy of the proposed ANN.</p>
<p>Yan &amp; Yao 2010 has proposed an investigation of the climate information effect on energy consumption in various climate zones. Back-propagation ANN is used to predict heating and cooling load to assist new building designs. Later, Biswas et al. 2016 applied the similar approach on residential sector and demonstration houses in the USA using <rs id="a12895640" type="software" subtype="environment">Matlab</rs> <rs id="a12895641" type="software" subtype="component" corresp="#a12895640">toolbox</rs>. Aydinalp et al. 2002 models the Appliance, Lighting and space Cooling (ALC) in residential buildings located in Canada. ANN for prediction of energy consumption shows better accuracy in comparison with engineering calculation methods. Later, they used ANN to predict Space heating and domestic hot water for the same buildings (Aydinalp et al. 2004).</p>
<p>Zhao &amp; Magoules 2010 predicted energy consumption of office building using parallel implementation of SVM. They aim to optimise the building characteristics of a model case. They utilised <rs id="a12895642" type="software">EnergyPlus</rs> software to calculate the energy demands. The results show a slight improvement regarding accuracy. Later in 2012, the authors apply gradient guided feature selection and the correlation coefficients methods to decrease the number of features for RBF and polynomial based SVM models (Zhao and Magoulès 2012a).</p>
<p>Noh &amp; Rajagopal 2013 propose a long-term GP prediction model for total energy consumption of a campus building using smart meter measurements and weather data. Nghiem &amp; Jones 2017 propose a GP based model for demand response service by predicting building energy consumption. Rastogi et al. 2017 compare the accuracy of GP and linear regression in emulating of a building performance simulation and show that the accuracy of GP is four times better than linear regression testing on <rs id="a12895643" type="software">EnergyPlus</rs> simulated case studies located in the US. Burkhart et al. 2014 integrate GP with a Monte Carlo expectation maximisation algorithm to train the model under data uncertainty. The aim is to optimise office building HVAC system performance by predicting its daily energy demand. Relative humidity and ambient temperature are considered as specific input variables and daily occupancy with two different scenarios (moderate and vigorous) as uncertain data. The results indicate that the models can be trained even with limited data or sparse measurements employing rough approximation and data range instead of sensor data. Manfren et al. 2013 develop a method for calibration and uncertainty analysis of building energy simulation model. They used detailed simulation, GP with RFB kernel and MLR to predict monthly electricity and gas usage of heating and cooling systems. The results indicate that GP not only provides a tool for optimisation and uncertainty analysis of building energy models but also shows higher accuracy in comparison with a piece-wise regression model.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f580457534"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:58+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The Fermi -LAT data set used in this work corresponds to 6 years of observations of 15 dSphs. We analyzed the data with the latest (Pass 8) event-level analysis [34], using the <rs id="a12970102" type="software">Fermi Science Tools</rs> version <rs id="a12970103" type="version" corresp="#a12970102">10-01-01</rs> and the
P8R2 SOURCE V
6 IRFs. DM signal morphological and spectral templates are used in a three-dimensional likelihood fit to the distribution of events in reconstructed energy and direction within the 10 • × 10 • dSph region-of-interest (ROI). The model for each ROI contains the dSph DM intensity template, templates for Galactic and isotropic diffuse backgrounds, and point sources taken from the third LAT source catalog (3FGL) [49] within 15 • of the ROI center. A broadband fit in each ROI is performed fitting the normalizations of the Galactic and isotropic diffuse components and 3FGL sources that fall within the ROI boundary. After performing the broadband fit, a set of likelihoods is extracted for each energy bin by scanning the flux normalization of a putative DM source modeled as a power law with spectral index 2 at the location of the dSph. Tables with likelihood values versus energy flux for each energy bin are produced for all considered targets. The likelihood tables used for the present work are taken from Ref. [34] and can be found in the corresponding online material. 10 These tables allow the computation of jointlikelihood values for any gamma-ray spectrum, and are used as input to the present analysis (see Section 3.2 for more details).</p>
<p>Using the <rs id="a12970104" type="software">PYTHIA</rs> simulation package version <rs id="a12970105" type="version" corresp="#a12970104">8.205</rs> <rs id="a12970106" type="bibr">[50]</rs>, we have computed the average gamma-ray spectrum per annihilation process (dN/dE) for a set of DM particles of masses between 10 GeV and 100 TeV (i.e. in the WIMP range), annihilating into SM pairs b b, W + W -, τ + τ -and µ + µ -. For each channel and mass, we average the gamma-ray spectrum resulting from 10 7 decay events of a generic resonance with mass 2 × m DM into the specified pairs. For each simulated event, we trace all the decay chains, including the muon radiative decay (µ -→ e -νe ν µ γ, not active in <rs id="a12970107" type="software">PYTHIA</rs> by default), down to stable particles.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f567874821"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:45+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The COVID-19 pandemic has posed unprecedented challenges and threats to the health care system, particularly affecting the effective delivery of essential health services in resource-poor countries such as Nepal. This study aimed to explore community perceptions of COVID-19 and their experiences towards health services utilization during the pandemic in Province-2 of Nepal. Methods: The semi-structured qualitative interviews were conducted among purposively selected participants (n = 41) from a mix of rural and urban settings in all districts (n = 8) of the Province 2 of Nepal. Virtual interviews were conducted between July and August 2020 in local languages. The data were analyzed using thematic network analysis in <rs id="a12953682" type="software">NVivo</rs> <rs id="a12953683" type="version" corresp="#a12953682">12 Pro</rs>. Results: The findings of this research are categorized into four global themes: i) Community and stakeholders' perceptions towards COVID-19; ii) Impact of COVID-19 and lockdown on health services delivery; iii) Community perceptions and experiences of health services during COVID-19; and iv) COVID-19: testing, isolation, and quarantine services. Most participants shared their experience of being worried and anxious about COVID-19 and reported a lack of awareness, misinformation, and stigma as major factors contributing to the spread of COVID-19. Maternity services, immunization, and supply of essential medicine were found to be the most affected areas of health care delivery during the lockdown. Participants reported that the interruptions in health services were mostly due to the closure of health services at local health care facilities, limited affordability, and involvement of private health sectors during the pandemic, fears of COVID-19 transmission among health care workers and within health centers, and disruption of transportation services. In addition, the participants expressed frustrations on poor testing, isolation, and quarantine services related to COVID-19, and poor accountability from the government at all levels towards health services continuation/management during the COVID-19 pandemic.</p>
<p>The data were analyzed using thematic network analysis [33] in <rs id="a12953684" type="software">NVivo</rs> <rs id="a12953685" type="version" corresp="#a12953684">12 Pro</rs> ( <rs id="a12953686" type="publisher" corresp="#a12953684">QRS International</rs>, London, United Kingdom). The data analysis started after completing 23 interviews. An inductive approach to coding was undertaken to identify basic themes. Three investigators (DRSi (MSc), RKS (MSc, PhD), and LKS (MSc)) identified and reviewed the basic themes from the first five interviews to avoid lone researcher bias in qualitative research [34].</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f481727773"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:18+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>With increasing amounts of digitally available data from all over the world, manual annotation of cognates in multi-lingual word lists becomes more and more time-consuming in historical linguistics. Using available software packages to pre-process the data prior to manual analysis can drastically speed-up the process of cognate detection. Furthermore, it allows us to get a quick overview on data which have not yet been intensively studied by experts. <rs id="a12952088" type="software" subtype="component" corresp="#a12952089">LingPy</rs> is a <rs id="a12952089" type="software" subtype="environment">Python</rs> library which provides a large arsenal of routines for sequence comparison in historical linguistics. With <rs id="a12952090" type="software">LingPy</rs>, linguists can not only automatically search for cognates in lexical data, but they can also align the automatically identified words, and output them in various forms, which aim at facilitating manual inspection. In this tutorial, we will briefly introduce the basic concepts behind the algorithms employed by <rs id="a12952091" type="software">LingPy</rs> and then illustrate in concrete workflows how automatic sequence comparison can be applied to multi-lingual word lists. The goal is to provide the readers with all information they need to (1) carry out cognate detection and alignment analyses in <rs id="a12952092" type="software">LingPy</rs>, (2) select the appropriate algorithms for the appropriate task, (3) evaluate how well automatic cognate detection algorithms perform compared to experts, and (4) export their data into various formats useful for additional analyses or data sharing. While basic knowledge of the Python language is useful for all analyses, our tutorial is structured in such a way that scholars with basic knowledge of computing can follow through all steps as well.</p>
<p><rs id="a12952099" type="software">LingPy</rs> <rs id="a12952143" type="bibr">(List et al., 2017a)</rs> provides these algorithms as part of a stable open-source software package that works on all major platforms. Given the complexity of the problems involving sequence comparison in historical linguistics, computers will not be able to replace human judgments any time soon, but with the recent advancements, the methods are definitely good enough to provide substantial help for classical historical linguists to pre-analyze the data to be later corrected by experts, or to check the consistency of human cognate judgments. Over the long run, computational methods can also contribute to the bigger questions of language evolution, be it indirectly, by increasing the amount of digitally available high-quality annotated data, or directly, by providing scholars' access to data too large to be processed by humans alone.</p>
<p>This article is supplemented by a detailed interactive tutorial in form of an <rs id="a12952101" type="software">IPython Notebook</rs> <rs id="a12952144" type="bibr">(Pe ´rez and Granger, 2007)</rs> which illustrates how all methods discussed here can be practically applied (see the Supplementary material for more information). Having installed the necessary software (Tutorial: 1), readers can follow the tutorial step by step and investigate how the algorithms work in practise. Our data is based on a small sample of Polynesian languages taken from the ABVD, which we substantially revised, both with respect to the phonetic transcriptions and the expert cognate judgments. All data needed to replicate the analyses discussed here are supplemented. We give more information in the interactive tutorial (Tutorial: 2.1).</p>
<p>When searching for cognates across languages, we usually assume that our data are given in some kind of wordlist, a list in which a number of concepts is translated into various languages. How many concepts we select depends on the research question, and various concept lists and questionnaires, ranging from 40 (Brown et al., 2008) up to more than 1,000 concepts (Haspelmath and Tadmor, 2009) have been proposed so far (see the overview in List et al. (2016a)). Our data example for this tutorial is based on the questionnaire of the ABVD project (Greenhill et al., 2008), consisting of 210 concepts, which were translated into 31 different Polynesian languages. For closely related languages, such as those in the Polynesian family, this gives us enough information to infer regular correspondences automatically, although it is clear that for analyses of Dixon and Kroeber, 1919), contrasted with a 'modern' representation using the <rs id="a12952104" type="software">EDICTOR</rs> tool <rs id="a12952145" type="bibr">(List, 2017)</rs>. more distant language relationship the number of words per language may not be enough. The basic format used by <rs id="a12952106" type="software">LingPy</rs> is a tab-separated input file in which the first row serves as a header and defines the content of the rest of the rows. The very first column is reserved for numerical identifiers (which all need to be unique), while the order of the other columns is arbitrary, with specific columns being required, and others being optional. Essential columns which always must be provided are the language name (DOCULECT), the comparison concept (CONCEPT), the original transcription (International Phonetic Alphabet (IPA), FORM, or VALUE), and a space-segmented form of the transcription (TOKENS). Multiple synonyms for the same comparison concept in the same language should be written in separate rows and given a separate ID each. The data in the TOKENS-column should supply the transcriptions in space-segmented form, that is, instead of transcribing the Fila word for 'all' as [eutSi], the software expects [e u tS i], which is internally interpreted as a sequence of five segments, namely [e], [u], [tS] and [i], with [tS] representing a voiceless post-alveolar affricate. If the TOKENS are not supplied to the algorithm, it will try to segment the data automatically, provided it can find the column IPA, which is otherwise not necessarily required to appear in the data. This however, may lead to various problems and unexpected behavior. We therefore urge all users of <rs id="a12952107" type="software">LingPy</rs> to make sure that they supply segmented data to the algorithm, making furthermore sure that they adhere to the general standards of transcription as they are represented in the IPA (IPA, 1999). 1 The format can be created manually by using either a text editor, or a spreadsheet program that allows to export to tab-separated format. To a large degree, this input format is compatible with the one advocated by the Cross-Linguistic Data Formats (CLDF) initiative (Forkel et al., 2017), the main difference being that <rs id="a12952108" type="software">LingPy</rs> requires a flat single file with tabstop as separators, while CLDF supports multiple files. CLDF furthermore encourages the use of reference catalogs, such as Glottolog (Hammarstro ¨m et al., 2017) or Concepticon (List et al., 2018), in order to increase the comparability of linguistic data across datasets, while <rs id="a12952109" type="software">LingPy</rs> is indifferent regarding the overall comparability as long as the data is internally consistent. As of version <rs id="a12952110" type="version" corresp="#a12952111">2.6</rs>, <rs id="a12952111" type="software">LingPy</rs> offers routines to convert to and from CLDF (see Tutorial: 6.3). Figure 2 provides a basic summary on <rs id="a12952112" type="software">LingPy</rs>'s input formats. More information on the format, and how it can be loaded into <rs id="a12952113" type="software">LingPy</rs> can be found in the supplemented interactive tutorial (Tutorial: 2.2-3).</p>
<p>Data quality and consistency plays a crucial role in the outcome of an automatic sequence comparison. As a general rule of thumb, we recommend all linguists who apply <rs id="a12952114" type="software">LingPy</rs> or other software to carry out automatic sequence comparison, to pay careful attention to what we call the SANE rules for data sanity: users should pay close attention to providing a sensible segmentation of their data, they should aim for high coverage, there should be no mixing of data from different sources (as this usually leads to inconsistent transcriptions and may also increase the number of synonyms), and synonyms should be evaded. 2 These rules are summarized in Table 2. If the original data does not provide reliable phonetic transcriptions, as it was the case with the Polynesian data we use in this tutorial, orthography profiles (Moran and Cysouw, 2017) provide an easy way to refine transcriptions while at the same time segmenting the data, and the <rs id="a12952115" type="software">EDICTOR</rs> tool <rs id="a12952147" type="bibr">(List, 2017)</rs> offers convenient ways to check phonological inventories of all varieties (Tutorial: 2.4). Various coverage statistics can be computed in <rs id="a12952117" type="software">LingPy</rs> (see Tutorial: 2.5). Synonym statistics can also be easily computed (see Tutorial: 2.6). Users should always keep in mind that the quality of automatic sequence comparison crucially depends on the quality of the data submitted to the algorithms.</p>
<p>Tahitian 'sea' small alphabets, in linguistics, the numbers of possible sounds in the languages of the world amounts to the thousands (Moran et al., 2014). It is not practical to design a matrix containing and confronting all sounds with each other, and most algorithms reduce the size of the alphabet by lumping similar sounds into a set of predefined sound classes (Fig. 3B, Tutorial: 3.1.2), for which transition probabilities can be efficiently defined, and which are then given as input for the alignment algorithm (List, 2012a;Holman et al., 2008). The introduction of gaps in an alignment (Fig. 3C, Tutorial: 3.1.3) can be seen as a special case of a scoring function. Instead of comparing two segments, the algorithm checks whether the introduction of a gap might be preferable. While gaps were originally given the same penalty, independent of the element with which they were compared, later studies showed that they could even be individually adjusted for each position in a sequence (Thompson et al., 1994). In linguistics, we know that sounds in certain positions (like initial consonants) are less likely to be lost and that new sounds tend to appear in specific contexts as well. In <rs id="a12952118" type="software">LingPy</rs>, positionspecific gap penalties are derived from the prosodic profiles of sequences (List, 2012a). Prosodic profiles essentially reflect for each segment of a word whether it occurs in weak or strong prosodic positions, and the user-defined gap penalty is modified accordingly.</p>
<p>What should users keep in mind when carrying out pairwise alignment analyses? As a rule of thumb, we recommend caution with local alignment analyses, since these can show unexpected behavior. We also recommend care with custom changes applied to the scoring or the gap function. Users often naively think by just 'telling' the computers which sound changes, this would automatically lead to excellent alignments and at times complain that <rs id="a12952148" type="software">LingPy</rs>'s standard algorithms fail to 'detect certain obvious changes'. However, alignments are no way to determine sound changes, they are at best a first step for linguistic reconstruction, and none of the algorithms which have been proposed so far models any kind of change. What is modeled instead are correspondences of sounds. It is difficult, if not impossible, to design an algorithm that aligns sequences of all kinds of diversity without proposing certain analyses which look awkward to a trained linguist. But remember, automatic sequence comparison is not there to replace the experts, but to help them.</p>
<p>As mentioned in the previous section, we can only meaningfully align words if we know they are historically related. In order to identify which words are related, however, we still need to compare them, and most automatic approaches, including the core methods available in <rs id="a12952121" type="software">LingPy</rs>, make use of pairwise sequence comparison techniques in order to find historically related words in linguistic datasets.</p>
<p>The performance of LexStat is not surprising, if we take its more sophisticated working procedure into account. LexStat uses global and local pairwise alignments to pre-analyze the data, computing language-specific scoring functions (List, 2012b), in which the similarity of the segments in a given language pair depends on the overall number of matches that could be found in the preprocessing stage. 6 In these scoring functions, sound segments for all languages in the data are represented as sound-class strings in a certain prosodic environment. This representation is useful to handle sound correspondences in different contexts (word-initial, wordfinal, etc.). For each language pair in the data, LexStat creates an attested and an expected distribution of sound correspondences. The attested distribution is computed for words with the same meaning and whose SCA score is beyond a user-defined threshold. The expected distribution is computed by shuffling the word lists in such a way that words with different meanings are aligned and compared, with the users defining how often word lists should be shuffled. This permutation test following suggestions by Kessler (2001) makes sure that the sound correspondences identified are unlikely to have arisen by chance. The distributions resulting from this permutation test are then combined in log-odds scores (see Fig. 3 above) which can then in turn be used to realign all words and determine their LexStat-distance. 7 These scores are then again used to create a matrix of pairwise distances as shown in Fig. 5. Our interactive tutorial shows how input data can be quickly checked before carrying out the (at times time-consuming) computation (Tutorial: 4.1) and provides additional information regarding the differences between the cognate detection methods available in <rs id="a12952122" type="software">LingPy</rs> (Tutorial: 4.2) and illustrates in detail how each of them can be applied (Tutorial: 4.3).</p>
<p>Once the words have been clustered into cognate sets, it is advisable to align all cognate words with each other, using a multiple alignment algorithm (Tutorial: 4.4). Alignments are useful in multiple ways. First, users can easily inspect them with web-based tools (Tutorial: 4.5). Second, they can be used to statistically investigate the identified sound correspondence patterns in the data (see Tutorial: 4.6). Both the manual and the automatic check of the results provided by automatic cognate detection methods are essential for a successful application of the methods. Only in this way can users either convince themselves that the results come close to their expectations or that something weird is going on. In the latter situation, we recommend that users thoroughly check to which degree they have conformed to our SANE rules for dataset sanity outlined above in Section 3. We also recommend that users do not change the different parameters too much, especially when applying <rs id="a12952125" type="software">LingPy</rs> the first time. Instead of trying to fix minor errors (such as obvious cognates missed or lookalikes marked as cognates) by changing parameters, it is often more efficient to correct errors manually. Although Rama et al. (2018) report promising results on fully automated workflows, we do not recommend relying entirely on automatic cognate detection when it comes to phylogenetic reconstruction, since the algorithms tend to be too conservative, often missing valid cognates (List et al., 2017b), but we are confident enough to recommend it for initial data exploration, and for the preparsing of data in order to increase the efficiency of cognate annotation.</p>
<p>We have claimed above that automatic cognate detection had made great progress of late. We make this claim based on tests in which the performance of automatic cognate detection algorithms was compared with expert cognate judgments (List et al., 2017b). There are different ways to compare expert cognate judgments with algorithmic ones. A very simple but nevertheless important one is to compare different cognate judgments manually, by eyeballing the data. Even if one lacks expert cognate judgments for a given dataset, this may be useful, as it helps to get a quick impression on potential weaknesses of the algorithm used for a given analysis. Comparing cognate judgments in concrete, however, can be quite tedious, especially if the data are not presented in any ordered fashion. For this reason, <rs id="a12952126" type="software">LingPy</rs> offers a specific format that helps to compare different cognate judgments in a rather convenient way. How this comparison can be carried out is illustrated in Table 4, where we use the numeric annotation for cognate clusters as described in Fig. 6 to compare expert cognate judgments for 'to turn' in eight East Polynesian languages with those produced by edit distance, the SCA, and the LexStat method, respectively. As can be seen from the table, NED lumps all words into one cluster, obviously being confused by the similarity of the vowels across all words. SCA comes close to the expert annotation, but wrongly separates Hawaiian [wili] from the first cluster, obviously being confused by the dissimilarity of the sound classes. LexStat correctly identifies all cognates, obviously thanks to its initial search for language-specific similarities between sound classes. In the interactive tutorial, we show how users can compute similar overviews on differences in cognate detection analyses and conveniently compare them (Tutorial: 5.1).</p>
<p>In Table 5, we report the results achieved by four automatic cognate detection methods on a small subset of ten East Polynesian languages which we retrieved from our Polynesian dataset for illustrative purposes. 8 In addition to the three methods reported already in Table 4, we added a random cognate detector which was sampled from 100 trials, and the Infomap version of the LexStat algorithm (LS-Infomap), in which the cognate set partitioning is carried out with the Infomap algorithm instead of the flat version of UPGMA (see Section 5 above). 9 NED shows a rather low precision compared to the other nonrandom approaches, indicating that it proposes many false positives (as we could see above in Table 4). On the other hand, its recall is very high, indicating that it does not miss many cognate sets. SCA obviously has a lot of problems with the data, performing worse than NED in general, with a rather low precision and recall. Both LexStat approaches largely outperform the other approaches in general, and especially the very high precision is very comforting, since it indicates that the algorithms do not propose too many false positives. That the Infomap version of LexStat performs better than LexStat with UPGMA is also shown in this comparison, although the differences are much lower than reported in List et al. (2017b). It would be very interesting to compare the scores we achieved with general scores of levels of agreement among human experts. Unfortunately, no systematic study has been carried out so far. 10 The interactive tutorial gives a detailed introduction into the computation of B-Cubed scores with <rs id="a12952127" type="software">LingPy</rs> (Tutorial: 5.2). Given the differences in the results regarding precision, recall, and generalized F-scores, it is obvious that the choice of the algorithm to use depends on the task at hand. If users plan to invest much time into manual data correction, having an algorithm with high recall that identifies most of the cognates in the data while proposing a couple of erroneous ones is probably the best choice. Users can achieve this by choosing a high threshold or an algorithm such as NED, which yields a rather high recall in form of the B-Cubed scores, at least for the Polynesian data in our sample. In other cases, however, when usercorrection is not feasible because of the size of the dataset, it is useful to choose low thresholds or generally conservative algorithms with high B-Cubed precision in order to minimize the amount of false positives.</p>
<p><rs id="a12952128" type="software">LingPy</rs> provides direct export of the cognate judgments to the Nexus format (Maddison et al., 1997), allowing users to analyze automated cognate judgments with popular packages for phylogenetic reconstruction, such as <rs id="a12952129" type="software">SplitsTree</rs> (Huson, 1998), <rs id="a12952130" type="software">MrBayes</rs> <rs id="a12952150" type="bibr">(Ronquist et al., 2009)</rs>, or <rs id="a12952132" type="software">BEAST</rs> <rs id="a12952133" type="version" corresp="#a12952132">2</rs> (<rs id="a12952134" type="bibr">Bouckaert et al., 2014</rs>, see Tutorial: 6.1). If phylogenetic trees are computed from distance matrices, both matrices and trees can be written to file and further imported in software packages
for tree manipulation and visualization (Tutorial: 6.2). In addition, data can be exported (and also be imported) to the wordlist format proposed by the CLDF initiative (Forkel et al., 2017), which is intended to serve as a generic format for data sharing in cross-linguistic studies (Tutorial: 6.3).</p>
<p>In this tutorial we have tried to show how automatic sequence comparison in <rs id="a12952135" type="software">LingPy</rs> can be carried out. Given the scope of this article, it is clear that we could not cover all aspects of alignments and cognate detection in all due detail. We hope, however, that we could help readers understand what they should keep in mind if they want to carry out sequence comparison analyses on their own. Additional questions will be answered in an interactive tutorial supplemented with this article, and for deeper questions going beyond the pure application of sequence comparison algorithms-such as additional analyses (e.g. the minimal lateral network method for borrowing detection, List et al., 2014, or an algorithm for the detection of partial cognates, List et al., 2016b), routines for plotting and data visualization, or customization routines for user-defined sound-class modelswe recommend the readers to turn to the extensive online documentation of the <rs id="a12952136" type="software">LingPy</rs> package ( <rs id="a12952137" type="url" corresp="#a12952136">http:// lingpy.org</rs>). We have emphasized multiple times throughout this article that the algorithms cannot and should not be used to replace trained linguists. Instead, they should be seen as a useful complement to the large arsenal of methods for historical language comparison which can help experts to derive initial hypotheses on cognacy, speed up tedious annotation of cognate sets, and increase their efficiency and consistency.</p>
<p>Notes 1. Linguists are often skeptical when they hear that <rs id="a12952138" type="software">LingPy</rs> requires explicit phonetic transcriptions, and often, they are even reluctant to interpret their data along the lines of the IPA. But in order to give the algorithms a fair chance to interpret the data in the same way in which they would be interpreted by linguists, a general practice for phonetic transcriptions is indispensable, and the IPA is the most widely employed transcription system. 2. We know well how difficult it is to conform to the latter point. What is clear is that tossing coins to select one out of many synonyms, as originally suggested by Gudschinsky (1956), will have a deleterious impact on any analysis (List, 2018). In order to avoid synonyms in qualitative work, we recommend to thoroughly review the guidelines in Kassian et al. (2010). 3. It would go beyond the scope of this tutorial to explain these famous algorithms in all detail. Instead, we refer the readers to Kondrak (2002: 20-65) as well as to an interactive demo of the Wagner-Fischer algorithm in List (2016). 4. In the normalized edit distance (NED), the edit distance between two strings is further normalized by dividing it by the length of the longer string. In this way, we can control for the length of the compared sequences. 5. The threshold for the algorithms are: NED: 0.75, SCA: 0.45, LexStat: 0.6. 6. For an example, consider the matches between Sikaiana and Tahitian shown in Table 1. Although Sikaiana [k] is different from [?], they are similar from a language-specific perspective, since they recur across many aligned cognate sets between both languages. When comparing [k] in English with [?] in German, however, they are not similar, as we will not find a cognate set in which those two sounds correspond. 7. As alignment algorithms yield similarity scores as a default, the similarity scores are converted to distance scores with help of the formula proposed by Downey et al. (2008). 8. We have not fully explored the practical limitations in terms of number of languages or number of concepts when comparing languages with <rs id="a12952139" type="software">LingPy</rs>. Ja ¨ger et al.</p>
<p>(2017) and Rama et al. (2017) report successful applications of <rs id="a12952140" type="software">LingPy</rs>'s cognate detection algorithms for as many as 100 languages. Although we think that the number might in fact be even higher, based on tests we carried out ourselves on 150 and more languages, we recommend to be careful when analyzing too many languages, as algorithmic performance may drastically drop when investigation samples are too large 9. The threshold for LexStat-Infomap was set to 0.55, following List et al. (2017b). The random cognate annotation algorithm was designed in such a way that it has the tendency to lump cognates to larger clusters. 10. The only study known to us addressing these problems is Geisler and List (2010), but it has, unfortunately, not been sufficiently quantified.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f81838093"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:46+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Over the past few decades, the term special needs has become a popular euphemism for disability (Berger, 2013). Rather than identifying a person as having a disability or having a certain disability (e.g., Anika is blind, Bruce has ADHD), the person is euphemized as having special needs. Figure 1 demonstrates the steeply rising popularity of the term special needs, based on <rs id="a12951709" type="publisher" corresp="#a12951710">Google</rs>'s <rs id="a12951710" type="software">NGram</rs> count in published books dating back to 1900. Currently, Google Scholar indexes over a million scholarly articles with the term special needs, and Amazon.com sells nearly 5000 books with the euphemism special needs in their title. Special needs is an increasingly popular euphemism.</p>
<p>The origin of special needs as a disability euphemism is unclear. Guralnick (1994) reports in the 1990s changing the wording of a 1980s questionnaire for parents from handicapped children to children with special needs, suggesting that the euphemism had taken hold by the end of the 20th century. Figure 2 demonstrates the declining popularity of the term handicapped based on <rs id="a12951712" type="publisher" corresp="#a12951713">Google</rs>'s <rs id="a12951713" type="software">NGram</rs>. Shapiro-Lacks (2013) proposes that the euphemism special needs morphed from the term Special Olympics, established in the late 1960s, and the concept of special education, also established in the 1960s. "At some point, people with disabilities began to be referred to as special, our needs as special needs, and our demographic as the special needs population," writes Shapiro-Lacks (2013).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f207501758"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T07:07+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>We present an improved version of the <rs id="a12971830" type="software">ECHO-QGP</rs> numerical code, which self-consistently includes for the first time the effects of electromagnetic fields within the framework of relativistic magneto-hydrodynamics (RMHD). We discuss results of its application in relativistic heavy-ion collisions in the limit of infinite electrical conductivity of the plasma. After reviewing the relevant covariant 3 + 1 formalisms, we illustrate the implementation of the evolution equations in the code and show the results of several tests aimed at assessing the accuracy and robustness of the implementation. After providing some estimates of the magnetic fields arising in non-central high-energy nuclear collisions, we perform full RMHD simulations of the evolution of the quark-gluon plasma in the presence of electromagnetic fields and discuss the results. In our ideal RMHD setup we find that the magnetic field developing in non-central collisions does not significantly modify the elliptic flow of the final hadrons. However, since there are uncertainties in the description of the pre-equilibrium phase and also in the properties of the medium, a more extensive survey of the possible initial conditions as well as the inclusion of dissipative effects are indeed necessary to validate this preliminary result.</p>
<p>Our paper is organized as follows. In Sect. 2 we present the RMHD equations in their most general form, focusing then on their ideal limit, i.e. on the case of a plasma with infinite electrical conductivity (and neglecting other dissipative effects such as viscosity and thermal conduction). Only the ideal case is considered for the present paper. In Sect. 3 we discuss the numerical implementation of the ideal-RMHD equations, written in a conservative form, within our improved <rs id="a12971831" type="software">ECHO-QGP</rs> code. In Sect. 4 we discuss the results of a large variety of numerical tests to prove the accuracy and the robustness of the implementation: the shock-tube problem, the description of Alfvén waves, the rotor test, the reproduction of the one-dimensional Bjorken expansion in a magnetic field and the accurate treatment of the in-vacuum selfsimilar expansion in transverse-MHD. In Sect. 5 we show the results obtained from the code with simplified (but reasonable) initial conditions for non-central nucleus-nucleus collisions. At least in the context of this simplified approach, the magnetic field is not able to modify the elliptic flow of the final hadrons substantially. Nevertheless, further and more realistic investigations are needed before solid conclusions can be drawn. Finally, in Sect. 6 we discuss our findings and the future perspectives of our work, with the idea of performing 3D + 1 simulations based on a much broader pool of different initial conditions, possibly including dissipative effects. Appendix 1 is devoted to a discussion of the propagation of linear perturbations in RMHD, focusing on the case of fast-magnetosonic and Alfvén waves, which are the ones relevant for the analysis carried out in this paper.</p>
<p>We now rewrite the evolution equations for ideal RMHD in a form suitable for numerical integration, for which we need a clear separation between time and space components (the so-called 3 + 1 split) and the preservation of the original conservative character of the equations, since shock-capturing numerical codes such as <rs id="a12971832" type="software">ECHO-QGP</rs> require one to solve a series of balance laws. Here we will provide the basic expressions, for further formal and technical details, see [6,35,36] and references therein.</p>
<p><rs id="a12971833" type="software">ECHO-QGP</rs> is based on finite difference schemes. At the beginning of the simulation, the initial values of the primitive variables n (the baryon density), v i (the contravariant components of the velocity of the fluid in the lab frame), p (the pressure of the fluid in the comoving frame) and B i (the contravariant components of the magnetic induction field in the lab frame) are discretized on the computational grid by evaluating them at the center of each cell. Time integration of conservative variables is performed using a second or third order Runge-Kutta algorithm, then, at each sub-timestep:</p>
<p>In this section we present some numerical test problems selected in order to validate the code. We avoid to repeat tests aimed at simply measuring the accuracy of the "core" algorithms, since <rs id="a12971834" type="software">ECHO-QGP</rs> for relativistic hydrodynamics [6,7] has been already validated against basic benchmarks, and many additional tests have been performed on the original <rs id="a12971835" type="software">ECHO</rs> code <rs id="a12971836" type="bibr">[36]</rs>, from which <rs id="a12971837" type="software">ECHO-QGP</rs> has been derived sharing the same base structure. Instead, here we focus on checking the correctness of its results in the ideal RMHD context. We use the ultra-relativistic EoS p = e/3, if not mentioned otherwise. We will use either Minkowski (t, x, y, z) or Milne [τ, x, y, η s ] coordinates, where τ ≡ √ t 2z 2 is the longitudinal proper-time and η s ≡ 1 2 ln t+z t-z the space-time rapidity. In the following, in writing four-vector components in Milne coordinates, we will employ square brackets. Notice that in the first case the three-metric is g i j = diag{1, 1, 1}, with |g| 1 2 = 1, whereas for Milne coordinates g i j = diag{1, 1, τ 2 }, with |g| 1 2 = τ . In both cases ∂ j g ik = 0 and the source terms in the evolution equations simplify considerably. Notice that in Milne coordinates, where g 33 = τ 2 , the source term for the energy equation contains a nonvanishing term proportional to 1 2 ∂ 0 g 33 = τ .</p>
<p>In order to test the shock-capturing properties of <rs id="a12971838" type="software">ECHO-QGP</rs> for relativistic MHD, we run a 1D shock-tube test in Minkowski coordinates comparing the numerical results against the solutions of the same problem computed by the exact Riemann solver developed by Giacomazzo and Rezzolla [51]. Since the cited solver works for an ideal-gas EoS, for the present test we impose p = ( -1)(e -ρ), (48) with an adiabatic index = 4/3, where ρ = nm stands for the mass density in the comoving frame (m is the rest mass and n is the number density of the conserved species), in a situation in which particle creation/annihilation is negligible, so that (e -ρ) is the thermal energy density. To employ Eq. ( 48) in this test, when retrieving the primitive variables we used the same method described in Ref. [36].</p>
<p>The test runs from an initial time t = 0 to a final time t = 4, with a grid resolution of 0.0025 (400 cells per unit of length). Results are displayed in Fig. 1. The comparison shows excellent agreement between the RMHD implementation in <rs id="a12971839" type="software">ECHO-QGP</rs> and the exact result.</p>
<p>With the purpose of performing a non-trivial validation of our numerical <rs id="a12971840" type="software" subtype="implicit">code</rs>, here we consider an exact solution of the so-called transverse RMHD equations, namely a situation in which a hot magnetized plasma flows along one direction, with the magnetic field perpendicular to the flow. Without loss of generality we can adopt a Minkowskian flat space in Cartesian coordinates and take the fluid flowing along the z-axis, while the magnetic field having only x-component</p>
<p>Fig. 7 Self-similar expansion into vacuum test, comparison of the <rs id="a12971841" type="software">ECHO-QGP</rs> results with the semi-analytic solution computed with <rs id="a12971842" type="software">Mathematica</rs> <rs id="a12971843" type="bibr">[58]</rs>. The graph shows s 1 = s/s 0 vs. ξ = z/t at t = 20 for three different values (10, 1 and 0.1 from top to bottom) of the B = 2 p 0 /B 2 0 parameter. We used a grid of 801 cells, the reconstruction algorithm
MPE5, the approximate Riemann solver HLL and the time integration algorithm was a second order Runge-Kutta. The initial pressure was: left side (z ≤ 0) p 0 = 1000, right side (z &gt; 0) p 0 = 5 • 10 -5 ≈ 0 (due to numerical reasons)</p>
<p>In Fig. 7 we display a comparison between the above semianalytic solution and the numerical result provided by our <rs id="a12971844" type="software" subtype="implicit">code</rs>. The graph shows s 1 = s/s 0 vs. ξ = z/t at t = 20 for three different values ( 10, 1 and 0.1 ) of the B = 2 p 0 /B 2 0 parameter. We used a grid of 801 cells, reconstruction algorithm: MPE5, approximate Riemann solver: HLL, time integration algorithm: second order Runge-Kutta. The initial pressure was: left side (z ≤ 0) p 0 = 1000, right side (z &gt; 0) p 0 = 5 • 10 -5 ≈ 0 (due to reasons, since <rs id="a12971845" type="software">ECHO-QGP</rs> cannot run with true null pressure). Again we observe excellent agreement between the numerical implementation and the analytical results for a large variety of parameters.</p>
<p>We plan to present a more extensive study of the QGP evolution in a subsequent article, nevertheless here we present some preliminary results to evaluate the impact that the interplay between magnetic-field and hydro evolution may have on some experimental observables. Although the whole 3D + 1 formalism has been already implemented into the <rs id="a12971846" type="software" subtype="implicit">code</rs>, for simplicity here we will show a basic 2D + 1 application to heavy ion collisions.</p>
<p>where σ in N N is the inelastic nucleon-nucleon cross-section. Since <rs id="a12971847" type="software">ECHO-QGP</rs> is not able to run with null energy density or if the thermal pressure is much smaller than the magnetic pressure, to ensure the stability of the <rs id="a12971848" type="software" subtype="implicit">code</rs>, we increase the initial energy density distribution by an additional small amount e min , negligible from the point of view of the dynamics of the system. We adopt Milne coordinates and we assume boost invariance along the η direction. The velocity components of the fluid are all null at the initial time τ 0 , i.e.</p>
<p>Then we approximate the electric charge distribution inside the two colliding nuclei as being uniform and spherical and we perform an integration over it to get the total magnetic field in each point of our computational grid. We assume that the motion and the distribution of the electric charges are unaffected by the collision between the nuclei. A detailed description of the whole procedure can be found in Ref. [23]. Since at the moment our <rs id="a12971849" type="software" subtype="implicit">code</rs> is not able to handle configurations where the magnetic pressure is much larger than the thermal pressure, which is the case in regions outside the fireball, where the initial energy density is less than 30 MeV/fm 3 we rescale the magnetic field so that the ratio between the magnetic and the thermal pressure does not exceed 0.1. This procedure does not affect the final results because at such a Fig. 8 The initial spatial pressure distribution in the transverse plane, obtained using the geometrical Glauber model given by Eq. ( 88) with the parameters listed in Table 3. The parameters are for the reaction Au + Au, b = 10 fm at √ s N N = 200 GeV low temperature there is no participating QCD matter and the hydrodynamic description of the medium would cease to be valid anyway.</p>
<p>Fig. 9 The initial spatial distribution of the components of the magnetic field B, computed using the method described in Ref. [23] (a) <rs id="a12971850" type="software">ECHO-QGP</rs> 2D + 1 RMHD evolution starting from initial conditions as described in this section, with σ = 5.8 MeV; (b) time evolution of the magnetic field computed using the same approach exploited to provide the initial conditions (explained in detail in Ref. [23]), assuming a medium with uniform and constant electrical conductivity σ = 5.8 MeV; (c) same as in case b), but assuming zero electrical conductivity σ = 0 MeV (vacuum); (d) exponential decay of magnetic field as modeled in</p>
<p>We notice that the expansion of the fluid in the transverse plane leads to a faster decrease compared to the case of a pure longitudinal Bjorken flow [56] and tends to become roughly exponential. However, the decay of the magnetic field of the fluid is still slower than in the case that the fields are generated Fig. 12 Comparison of the time evolution of the magnitude of the magnetic field (in neutral pion mass units squared) at the center of the grid in five different cases: a with <rs id="a12971851" type="software">ECHO-QGP</rs>, as described in this section, computing the initial conditions assuming σ = 5.8 MeV; b magnetic field generated by the electric charges of the two colliding nuclei moving in a medium with uniform and constant electrical conductivity σ = 5.8 MeV, i.e. the same approach exploited to provide the initial conditions (explained in details in Ref. [23]), but now adopted for the whole time interval; c same as in case b, but assuming zero electrical conductivity σ = 0 MeV (vacuum) d assuming an exponential decay of the magnetic field as modeled in Ref. [61], with t D = 1.9 e) Bjorken flow. The parameters are for the reaction Au + Au, b = 10 fm at √ s N N = 200</p>
<p>We presented the extension of the <rs id="a12971852" type="software">ECHO-QGP</rs> code to the relativistic magneto-hydrodynamic regime, in the limit of infinite electrical conductivity, i.e. without taking into account any resistive effect. In the present version, the <rs id="a12971853" type="software" subtype="implicit">code</rs> has been tested with an ideal-gas EoS, either in the presence of a finite mass density or in the ultra-relativistic regime ( p = e/3). After introducing the physics equations on which the <rs id="a12971854" type="software" subtype="implicit">code</rs> is based, we gave an overview of their numerical implementation. Then we illustrated the results of several tests to validate the implementation. Since our final aim is to exploit the <rs id="a12971855" type="software" subtype="implicit">code</rs> to study the evolution of the quark-gluon plasma formed in heavy-ion collisions, we showed first applications in this context, adopting simplified initial conditions.</p>
<p>The next development of the <rs id="a12971856" type="software" subtype="implicit">code</rs> will involve the inclusion of dissipative effects (shear and bulk viscosity and a finite electric conductivity), using the numerical techniques presented in [6,34] and already implemented in previous versions of the <rs id="a12971857" type="software">ECHO</rs> code, going beyond the approximation of an ideal plasma. A major conceptual achievement would be represented by the inclusion in our setup of anomalous currents, allowing one to provide a consistent description of the CME and to estimate the possibility of disentangling it from other charge-separating effects related to the presence of strong electromagnetic fields.</p>
<p>Finally, we deem that applications of numerical calculations performed with the present relativistic MHD version of the <rs id="a12971858" type="software">ECHO-QGP</rs> code could also be relevant for cosmological (generation of the primordial magnetic fields [69]) or astrophysical studies. For instance, the sudden transition from an hadronic to a QGP-like equation of state in a protomagnetar (phase transition to a quark star) has recently been suggested as a possible explanation for the observed cases of (long) gamma-ray burst events with double prompt emission peaks [70]. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecomm ons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. Funded by SCOAP 3 .</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f213419766"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:51+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>For all samples, the response of the CMS detector is simulated in detail using <rs id="a12970059" type="software">GEANT</rs> <rs id="a12970060" type="version" corresp="#a12970059">4.9.4</rs> <rs id="a12970061" type="bibr">[24]</rs>. The samples are then processed through the trigger emulation and event reconstruction chain of the CMS experiment. In addition, simulated minimum bias events are overlaid with the primary collision to model the pileup distribution from data. For the data used in this analysis, the average number of pileup interactions was 21 per bunch crossing.</p>
<p>In Fig. 5 we show the upper limits on the product of the cross section to produce H → 2X and the branching fraction squared B 2 for X to decay into q q. The upper limits on the squark production cross section (where each squark decays to a neutralino that decays into a quarkantiquark pair and a muon) are presented in Fig. 6. In order to increase the number of tested models, the lifetime distributions of the signal long-lived particles are reweighted to different mean values, between 0.4τ and 1.4τ, for every lifetime value τ and mass combination listed in Table IV. Event weights are computed as the product of weights assigned to each long-lived particle in the event. The reweighted signal reconstruction efficiencies are then used to compute the expected and observed limits for the additional mean lifetime values. The upper limits for the neutralino model are compared with NLO calculations of the squark production cross section, including next-toleading-logarithmic (NLL) corrections obtained with the program <rs id="a12970062" type="software">PROSPINO</rs> <rs id="a12970063" type="bibr">[35]</rs><rs id="a12970064" type="bibr">[36]</rs><rs id="a12970065" type="bibr">[37]</rs>. The theoretical cross section for q qÃ þ q q is 10, 0.139, 0.014, and 0.00067 pb for q masses of 350, 700, 1000, and 1500 GeV, respectively, assuming a gluino mass of 5 TeV. The cross section uncertainty band represents the variation of the QCD factorization and renormalization scales, each up and down by a factor of 2, as well as a variation obtained by using two different sets of NLO parton distribution functions (CTEQ6.6 and MSTW2008 [38]).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f81798595"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:18+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Data was entered into <rs id="a12951853" type="software" subtype="component" corresp="#a12951854">excel</rs> sheets ( <rs id="a12951844" type="publisher" corresp="#a12951854">Microsoft</rs> <rs id="a12951854" type="software" subtype="environment">Office</rs>, <rs id="a12951846" type="version" corresp="#a12951854">2007</rs>) and validated by checking all entries against all of the original data collection forms. Then data was exported to <rs id="a12951847" type="software">Stata</rs> ® Software package, Version <rs id="a12951848" type="version" corresp="#a12951847">13</rs> ( <rs id="a12951849" type="publisher" corresp="#a12951847">Stata Corporation</rs>, College Station, Texas 77845 USA, stata@stata.com) for analysis. Spearman's rank correlation coefficient was calculated to assess the relationship between pairs of continuous groups of variables which were not normally distributed. The Wilcoxon rank sum test was used to establish if there were differences in the number of lesions in animals between two groups or predilection sites. Mid p-exact tests were conducted using the <rs id="a12951850" type="software" subtype="component" corresp="#a12951851">rate2by2.test</rs> function as implemented in the <rs id="a12951851" type="software" subtype="component" corresp="#a12951852">epitools</rs> package in <rs id="a12951852" type="software" subtype="environment">R</rs>-software to establish the significance of differences between proportions. Only p-values of less than 0.05 were considered statistically significant.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f160795489"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T14:13+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>GIWAXS investigation. GIWAXS measurements were conducted on the PLS-II 6D UNIST-PAL beamline of the Pohang Accelerator Laboratory in the Republic of Korea. The X-rays from the bending magnet are monochromatized to λ = 0.62382 A ̊, using a double crystal monochromator and focused both horizontally and vertically (120 (V) × 150 (H) μm 2 in FWHM at sample position) using the sagittal Si(111) crystal and toroidal mirror. Incidence angles of 0.15°, 0.2°, and 0.3°were used in this study. The vacuum GIWAXS sample chamber is equipped with a 5-axis motorized stage for fine sample alignment. The incidence angle of the X-ray beam was set to 0.15°, which is close to the critical angle for HaP. Two-dimensional GIWAXS 2 dimension (2D) patterns were recorded with a 2D CCD detector (Rayonix MX 225-HS, USA). Diffraction angles were calibrated by a pre-calibrated sucrose (Monoclinic, P2 1 , a = 10.8631 A ̊, b = 8.7044 A ̊, c = 7.7624 A ̊, b = 102.938°) 24 and the sample-todetector distance was 246.4 mm. The samples were exposed to X-ray beam energy of 11.6 keV for only 10 s to prevent damage. GIWAXS 2D images were converted to one-dimensional patterns using <rs id="a12971557" type="software">IGOR Pro</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f519350778"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T14:18+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Data was compared using a Student's t test with <rs id="a12967912" type="software">Prism</rs> <rs id="a12967913" type="version" corresp="#a12967912">5</rs> software ( <rs id="a12967914" type="publisher" corresp="#a12967912">GraphPad Software, Inc.</rs>, San Diego, CA).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f210545553"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T14:16+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Statistical analysis. <rs id="a12967915" type="software">SPSS</rs> ver. <rs id="a12967916" type="version" corresp="#a12967915">17.0</rs> ( <rs id="a12967917" type="publisher" corresp="#a12967915">SPSS Inc.</rs>; Chicago, IL, USA) was used for statistical analysis. Data are expressed as mean ± S.D. Significant differences between control and the exposed groups were analyzed using the Student's paired t-test and one-way and/or multiple-comparison ANOVA followed by Tukey's test. P &lt; 0.05 was considered significant.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f534257263"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:41+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>This study complies with the Guidelines for Accurate and Transparent Health Estimates Reporting 17 statement (appendix pp 3-4). Analyses were completed with <rs id="a12965507" type="software">Stata</rs> (version <rs id="a12965508" type="version" corresp="#a12965507">17</rs>), <rs id="a12965509" type="software">Python</rs> version ( <rs id="a12965510" type="version" corresp="#a12965509">3.7.7</rs>), and <rs id="a12965511" type="software">R</rs> (version <rs id="a12965512" type="version" corresp="#a12965511">4.0.3</rs>). Statistical <rs id="a12970905" type="software" subtype="implicit">code</rs> used for these analyses can be found online. Results specific to the model run for this publication are accessible for each location online. The estimates viewable in our online <rs id="a12970906" type="software" subtype="implicit">tool</rs> will be iteratively updated as new data are incorporated and will ultimately supersede the results in this paper. The data used as inputs in these analyses are available for download on the Global Health Data Exchange website. These data include those available in public online repositories and those made available on request from the data provider. Data sources for reported COVID-19 mortality are also listed by location in the appendix (pp 8-37).</p>
<p>All maps presented in this study were generated by the authors using <rs id="a12965516" type="software" subtype="component" corresp="#a12965517">RStudio</rs> (<rs id="a12965517" type="software" subtype="environment">R</rs> version <rs id="a12965518" type="version" corresp="#a12965517">4.0.3</rs>) and <rs id="a12965519" type="software">ArcGIS Desktop</rs> (version <rs id="a12965520" type="version" corresp="#a12965519">10.6.1</rs>), and no permissions were required to publish them.</p>
<p>Second, in many European countries, registered deaths spiked in late July and early August (calendar weeks 31-33) of 2020, which was a period when reported COVID-19 deaths were extremely low as reported in our online <rs id="a12970907" type="software" subtype="implicit">tool</rs>. This period coincided with a heat wave; such spikes in all-cause mortality have been observed in Europe during similar timeframes in previous years. [19][20][21] Because our model cannot separate excess mortality due to COVID-19 from excess deaths occurring during a heat wave, we excluded these weeks of data for all countries in western Europe (classified according to the Global Burden of Diseases, Injuries, and Risk Factors Study [GBD] location groupings) from subsequent analyses to avoid potentially exaggerating the impact of COVID-19 on all-cause mortality during 2020. Late registration in all-cause mortality and its effect on computed excess mortality over time, USA Reported COVID-19 deaths (coloured lines) and estimated number of excess deaths due to the COVID-19 pandemic (dashed line) by week from series of weekly mortality data reported by calendar week from October, 2020, to February, 2022. The gap between the first calendar week during which mortality for a past week was reported and the subsequent reported levels indicates the gradual process by which completeness of reported deaths increases over time. Lines below 0 indicate reported deaths below the expected deaths value. Third, as under-reporting of deaths is common in most vital registration systems, especially those from outside the high-income group of countries, we corrected allcause mortality data for under-registration of death as estimated in GBD 2019; 22 six countries were corrected because GBD estimates suggested registration was less than 95% for the calendar year 2019. This under-reporting correction was in addition to, and distinct from, the late registration issue.</p>
<p>The highest ratios of excess deaths to reported COVID-19 deaths were observed in parts of central Asia and most countries in sub-Saharan Africa (excluding southern sub-Saharan Africa). These high ratios are unlikely to be fully explained by dramatic increases in other causes of death during the pandemic and must at least partly be related to a paucity in extensive testing, medical practices, or state guidance on what should count as a death from COVID-19. Behavioural data, available in our online <rs id="a12970908" type="software" subtype="implicit">tool</rs>, suggest that in comparison with high-income countries including Canada, many states in the USA, and some countries in western Europe, mask use across these regions was generally much lower throughout the pandemic; mobility, measured via cell phone application use, was much higher; and many fewer social distancing mandates were put in place. Furthermore, our variable selection analysis found that in addition to pandemicrelated indicators such as IDR and mobility, background population health-related metrics such as HAQ Index, crude death rate, and inpatient admission rates are also predictive of excess mortality due to the COVID-19 pandemic. These factors might explain the large and under-reported COVID-19 epidemics in sub-Saharan Africa and central Asia, but more data are needed to fully understand the patterns in these regions. More attention should be given to locations like these, where dramatically low IDRs and rates of reported COVID-19 deaths have masked the true severity of the pandemic.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f492545565"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:57+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>
<rs type="software">HaploTypeCaller</rs> --min-pruning 0 argument was added to increase mutation calling sensitivity near sequencing gaps. For Nanopore, low-coverage regions with poor alignment quality (&lt;85% variant homogeneity) near sequencing/amplicon ends were masked to be robust against primer drop-out experienced in the spike gene, and the sensitivity for detecting short inserts using a region-local global alignment of reads was increased. We also used the wf_artic (ARTIC SARS-CoV-2) pipeline as built using the
<rs type="software">Nextflow</rs> workflow framework 56 . In some instances, mutations were confirmed visually with .bam files using
<rs type="software" id="s1">Geneious</rs> v.
<rs type="version" corresp="s1">2020.1.2</rs> (Biomatters). The reference genome used throughout the assembly process was NC_045512.2 (numbering equivalent to MN908947.3).
</p>
<p>Raw reads from the Illumina COVIDSeq protocol were assembled using the
<rs type="software" id="s2">Exatype NGS</rs> SARS-CoV-2 pipeline v.1
<rs type="version" corresp="s2">.6.1</rs> (
<rs type="url" corresp="s2">https://sars-cov-2. exatype.com</rs>/). This pipeline performs quality control on reads and then maps the reads to a reference using
<rs type="software">Examap</rs>. The reference genome used throughout the assembly process was NC_045512.2 (accession number: MN908947.3).
</p>
<p>Several of the initial Ion Torrent genomes contained a number of frameshifts, which caused unknown variant calls. Manual inspection revealed that these were probably sequencing errors resulting in mis-assembled regions (probably due to the known error profile of Ion Torrent sequencers) 57 . To resolve this, the raw reads from the IonTorrent platform were assembled using the SARSCoV2 RECoVERY (Reconstruction of Coronavirus Genomes &amp;
<rs type="software">Rapid Analysis</rs>) pipeline implemented in the
<rs type="software" id="s3">Galaxy instance ARIES</rs> (
<rs type="url" corresp="s3">https://aries.iss.it)</rs>. This pipeline fixed the observed frameshifts, confirming that they were artefacts of mis-assembly; this subsequently resolved the variant calls. The
<rs type="software">Exatype</rs> and
<rs type="software">RECoVERY</rs> pipelines each produce a consensus sequence for each sample. These consensus sequences were manually inspected and polished using
<rs type="software" id="s4">Aliview</rs> v.
<rs type="version" corresp="s4">1.27</rs> (
<rs type="url" corresp="s4">http://ormbunkar.se/aliview/)</rs>.
</p>
<p>The number and position of the Omicron mutations has affected a number of primers and caused primer drop-outs across a range of sequencing protocols, especially within the RBD (https://primermonitor.neb.com/lineages). These primer drop-outs have resulted in a number of genomes missing stretches of the RBD, and can affect estimates of mutation prevalence and the determination of the true set of lineage-defining mutations. Given this, .bam files of all initial genomes were inspected using <rs type="software">IG Viewer</rs> to confirm mutation calls where reference calls were suspected to be from low coverage at primer dropout sites 58 .
</p>
<p>Lineage classification. We used the widespread dynamic lineage classification method from the <rs type="software" id="s5">Phylogenetic Assignment of Named Global Outbreak Lineages (PANGOLIN)</rs> software suite (<rs type="url" corresp="#s5">https://github.com/ hCoV-2019 /pangolin</rs>) 17 . This is aimed at identifying the most epidemiologically important lineages of SARS-CoV-2 at the time of analysis, enabling researchers to monitor the epidemic in a particular geographical region. For the Omicron variant described in this study, the corresponding PANGO lineage designation is BA.1 (lineages v.1.2.106). When first characterized, the lineage was designated B. 59,60 , we constructed a dataset containing 221 sequences by randomly sampling five sequences from each month for each continent. No Oceania samples were available from July or August, and no South American sequences were available from July 2021 (ref. 61 ). These sequences were aligned together with a set of five high-quality BA.1, six BA.2 and one BA.3 sequences (representing the known diversity of these clades on 5 December 2021) using <rs type="software">MAFFT</rs> 62 with the default settings. Whereas 3SEQ 37 and RDP5 (ref. 38 ) were used to analyse this dataset, a subsample of the 39 most divergent sequences from the dataset was analysed using the GARD recombination detection method 36 . As none of these recombination detection methods normally use potentially informative deletion patterns, deletions in these alignments were recoded as nucleotide substitutions (one substitution per contiguous run of deleted nucleotides). Furthermore, to minimize multiple testing issues, BA.1, BA.2 and BA.3 were tested for evidence of recombination among one another using individual sequences from each of these lineages (CERI-KRISP-K032254, EPI_ISL_7190366 and EPI_ISL_7526186, respectively) together with the Wuhan-Hu-1 sequence (which served as a reference point for rooting the four taxon phylogeny). The default program settings were used throughout for recombination analyses, with the exception of RDP5 analysis, in which sequences were treated as linear and the window sizes for the <rs type="software">SiScan</rs> and <rs type="software">BootScan</rs> methods (two of the seven recombination detection methods applied in RDP5) were changed to 2,000 nucleotides.</p>
<p>Selection analyses. We investigated the nature and extent of selective forces acting on BA.1, BA.2 and BA.3 genes encoding individual protein products (respectively, a median of 110, 3 and 2.5 unique BA.1, BA.2 and BA.3 sequences per protein product encoding genome region). A subset of publicly available sequences (from the Virus Pathogen Database and Analysis Resource (ViPR); https://www.viprbrc.org/) was included as background sequences to contextualize selection signals detectable within the BA.1, BA.2 and BA.3 lineages at the levels of complete protein product encoding regions, and individual codons (a median of ~100 sequences per protein coding region). Sequences were selected, quality-checked, aligned, and processed for <rs type="software" subtype="component" corresp="s6">BUSTED</rs>, <rs type="software" subtype="component" corresp="s6">RELAX</rs>, <rs type="software" subtype="component" corresp="s6">MEME</rs>, <rs type="software" subtype="component" corresp="s6">FADE</rs>, <rs type="software" subtype="component" corresp="s6">FEL</rs> and <rs type="software" subtype="component" corresp="s6">BGM</rs> selection analyses (all implemented in <rs type="software" id="s599">HyPhy</rs> v.<rs type="version" corresp="#s599">2.5.33</rs>) 63 using the automated <rs type="software">RASCL</rs> pipeline as outlined previously 2,9,34 .</p>
<p>Structure modelling. We modelled the spike protein on the basis of the Protein Data Bank coordinate set 7A94, showing the first step of the spike protein trimer activation with one RBD domain in the up position, bound to the human ACE2 receptor 64 . We used <rs type="software" id="s6">Pymol (The
PyMOL Molecular Graphics System</rs>, v. <rs type="version" corresp="s6">2.2.0</rs>) for visualization.</p>
<p>Phylogenetic analysis. All sequences on <rs type="software">GISAID</rs> 15,16 designated Omicron (n = 686; date of access: 7 December 2021) were analysed against a globally representative reference set of SARS-CoV-2 genotypes (n = 12,609) spanning the entire genetic diversity observed since the start of the pandemic. In brief, the reference set included: (1) all genomes from Africa assigned to PANGO lineage B.1.1 or any of its descendents, excluding those belonging to a VOC clade; (2) a representative subsampling of global data from the publicly maintained global build of <rs type="software" id="s7">Nexstrain</rs> (<rs type="url" corresp="s7">https://nextstrain.org/ncov/gisaid/global)</rs>; and (3) the top thirty <rs type="software">BLAST</rs> hits when querying <rs type="software">GISAID BLAST</rs> for BA.1 and BA.2 sequences. This sampling scheme ensures that we analyse Omicron against the closest variants of the virus. Omicron and reference sequences were aligned using <rs type="software">Nextalign 65</rs> . A maximum-likelihood tree topology was inferred in <rs type="software">FastTree</rs> 66 under the following parameters: a General Time Reversible model of nucleotide substitution and a total of 100 bootstrap replicates 67 . The resulting maximum-likelihood tree topology was transformed into a time-calibrated phylogeny in which branches along the tree were scaled in calendar time using <rs type="software">TreeTime</rs> 68 . The resulting tree was then visualized and annotated in <rs type="software">ggtree</rs> in R 69 .</p>
<p>Birth-death phylogenetic analysis. We analysed the full South Africa and Botswana dataset (n = 552, all BA.1 assigned), and the reduced dataset containing only Gauteng province genomes (n = 277) using the serially sampled birth-death skyline (BDSKY) model 19 , implemented in <rs type="software" id="s333333333">BEAST 2</rs> (v.<rs type="version" corresp="#s333333333">2.5.2</rs>) <rs type="bibr">72</rs> . To allow for changes in genomic sampling intensity shortly after the discovery of the new lineage, we allowed the sampling proportion to vary with time while keeping all other models parameters constant over the study period. The choice of prior distributions for the model parameters is summarized in Extended Data Table 3.</p>
<p>For each analysis, we ran two independent chains of 100 million MCMC steps and sampled parameters every 10,000 steps. We used <rs type="software" id="s8">Tracer</rs> (v. <rs type="version" corresp="s8">1.7</rs>) 75 to evaluate MCMC convergence for each of the individual chains (effective sample size (ESS) &gt; 200), which were then combined using <rs type="software">LogCombiner</rs> to obtain the final posterior distribution after removing 10% of each chain as burn-in. The results were analysed using the <rs type="software" id="s9">bdskytools</rs> package in <rs type="software" subtype="environment" corresp="s9">R</rs> ( <rs type="url" corresp="s9">https://github.com/laduplessis/ bdskytools</rs>).</p>
<p>We examined the sensitivity of our estimates to different assumptions regarding the average duration of infectiousness by repeating the analysis of the South Africa and Botswana dataset with different fixed values of the becoming non-infectious rate: 52.1 per year and 26.1 per year, which translate to an infectious period of 7 and 14 days, respectively. These values were selected as plausible bounds based on the infectious period of asymptomatic cases and the time from symptom onset to two negative RT-PCR tests 74 . The 4-epoch model was used with a fixed clock rate of 0.75 × 10 -3 s.s.y. in these analyses. For each analysis, we ran three independent chains of 35 million MCMC steps and sampled parameters every 10,000 steps. We used <rs type="software" id="s10">Tracer</rs> (v. <rs type="version" corresp="s10">1.7</rs>) 75 to evaluate MCMC convergence for each of the individual chains (ESS &gt; 200), which were then combined using LogCombiner to obtain the final posterior distribution after removing 10% of each chain as burn-in.</p>
<p>The results from the sensitivity analyses showed that our estimates are largely robust to alternative assumptions about the infectious period. On doubling of the mean duration of infectiousness from 7 to 14 days, the TMRCA remained mostly the same (10 October 2021 (95% HPD = 2 October-17 October) compared with 11 October 2021 (95% HPD = 3 October-17 October), while the doubling time shifted from 4.4 (95% HPD = 3.9-5.0) days to 3.5 (95% HPD = 3.2-3.9) days. This change in the doubling time is partially explained by differing estimates of the sampling proportion. For most of the epochs, the sampling proportion increases with the doubling time to explain the same number of sequences observed in each instance, that is, if we assume a shorter average duration of infectiousness, then we infer a slower transmission of which a greater proportion of sequences has been sampled. Phylogeographic analysis. MCMC analyses were run in duplicate in <rs type="software" id="s11">BEAST</rs> (v.<rs type="version" corresp="s11">1.10.4</rs>) 70,71 for a total of 100 million iterations sampling every 10,000 steps in the chain. Convergence of runs was assessed in <rs type="software" id="s12">Tracer</rs> (v.<rs type="version" corresp="s12">1.7.1</rs>) 75 based on high effective sample sizes (&gt;200) and good mixing in the chains. Maximum clade credibility trees for each run were summarized in <rs type="software">TreeAnnotator</rs> after discarding the first 10% of the chain as burn in. Finally, the spatiotemporal dispersal of <rs type="software">Omicron</rs> was mapped using the <rs type="software" subtype="environment">R</rs> package <rs type="software">seraphim</rs> 76 .</p>
<p>Estimating transmission advantage. We analysed 805 SARS-CoV-2 sequences from Gauteng, South Africa, that were uploaded to GISAID with sample collection dates from 1 September to 1 December 2021 (ref. 15 ). We used a multinomial logistic regression model to estimate the growth advantage of Omicron compared with Delta at the time point at which the proportion of Omicron reached 50% (refs. 77,78 ). We fitted the model using the multinom function of the <rs type="software">nnet</rs> package and estimated the growth advantage using the package <rs type="software">emmeans</rs> in <rs type="software" subtype="environment">R</rs>.</p>
<p>All SARS-CoV-2 whole-genome sequences produced by NGS-SA are deposited in the GISAID sequence database and are publicly available subject to the terms and conditions of the GISAID database. The GISAID accession numbers of sequences used in the phylogenetic analysis, including Omicron and global references, are provided in the Supplementary Table 1. Raw reads for our sequences have also been deposited at the NCBI Sequence Read Archive (SRA) (BioProject: PRJNA784038). Other raw data for this study are provided as a supplementary dataset at our GitHub repository (https://github.com/krisp-kwazulu-natal/ SARSCoV2_Omicron_Southern_Africa). The reference SARS-CoV-2 genome (MN908947.3) was downloaded from the NCBI database (https://www.ncbi.nlm.nih.gov/). Other publicly available data used in this study are as follows: NCBI SARS-CoV-2 Data Hub (https://www. ncbi.nlm.nih.gov/sars-cov-2/), Protein Data Bank coordinate set 7A94 (https://www.rcsb.org/), Nexstrain global build (https://nextstrain. org/ncov/gisaid/global), Covid-19 Re repository (https://github.com/ covid-19-Re), daily Covid-19 case numbers from the Data Science for Social Impact Research Group at the University of Pretoria (https:// github.com/dsfsi/covid19za), daily case numbers from OWID (<rs type="url">https:// github.com/owid/covid-19-data)</rs> and the Virus Pathogen Database and Analysis Resource (ViPR) (https://www.viprbrc.org/).</p>
<p>All input files (such as raw data for figures, alignments or XML files), along with all resulting output files and <rs type="software" subtype="implicit" id="s13">scripts</rs> used in the present study are publicly shared at
GitHub (<rs type="url" corresp="s13">https://github.com/krisp-kwazulu-natal/ SARSCoV2_Omicron_Southern_Africa</rs>). 50. Marivate, V. et al. Coronavirus disease (COVID-19) case data-South Africa. Zenodo https://doi.org/10.5281/zenodo.3819126 (2020).</p>
<p>We analysed daily cases of SARS-CoV-2 in South Africa up to 14 December 2021 from publicly released data provided by the National Department of Health and the National Institute for Communicable Diseases. This was accessible through the repository of the Data Science for Social Impact Research Group at the University of Pretoria (https:// github.com/dsfsi/covid19za) 49,50 . The National Department of Health releases daily updates on the number of confirmed new cases, deaths and recoveries, with a breakdown by province. Daily case numbers for Botswana were obtained through Our World in Data (OWID) COVID-19 data repository (https://github.com/owid/covid-19-data). We obtained test positivity data from weekly reports from the National Institute for Communicable Diseases (NICD) 51 . Data to calculate the proportion of positive TaqPath COVID-19 PCR tests (Thermo Fisher Scientific) with SGTF in South Africa was obtained from the National Health Laboratory Service and Lancet Laboratories. Test positivity data for Botswana was obtained from the National Health Laboratory up to 6 December 2021. All data visualization was generated through the <rs type="software">ggplot</rs> package in <rs type="software" subtype="environment">R</rs> 52 .</p>
<p>Midnight protocol. For Oxford Nanopore sequencing, the Midnight primer kit was used as described previously 54 . cDNA synthesis was performed on the extracted RNA using the LunaScript RT mastermix (New England BioLabs) followed by gene-specific multiplex PCR using the Midnight primer pools, which produce 1,200 bp amplicons that overlap to cover the 30 kb SARS-CoV-2 genome. Amplicons from each pool were pooled and used neat for barcoding with the Oxford Nanopore Rapid Barcoding kit according to the manufacturer's protocol. Barcoded samples were pooled and bead-purified. After the bead clean-up, the library was loaded on a prepared R9.4.1 flow-cell. A GridION X5 or MinION sequencing run was initiated using <rs type="software">MinKNOW</rs> software with the base-call setting switched off.</p>
<p>Genome assembly. We assembled paired-end and Nanopore .fastq reads using <rs type="software" id="s14">Genome Detective</rs> v.<rs type="version" corresp="s14">1.132</rs> (<rs type="url" corresp="s14">https://www.genomedetec-tive.com)</rs>, which was updated for the accurate assembly and variant calling of tiled primer amplicon Illumina or Oxford Nanopore reads, and the Coronavirus Typing Tool 55 . For Illumina assembly, the GATK</p>
</text>
</tei>
  <tei>
    <teiHeader>
        <fileDesc id="f593677746"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T16:14+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text lang="en">
        <p>Observations of horizontal drifting snow transport fluxes (TR ds ) were performed in Adélie Land, East Antarctica (Fig. 1), where surface atmospheric conditions are well monitored at the permanent French Dumont d'Urville station (Favier et al., 2011). The coastal region is characterised by frequent strong katabatic winds that are regularly associated with aeolian snow transport events (Trouvilliez et al., 2014). The evaluation data set includes snowdrift acoustic measurements collected using second-generation FlowCapt ™ devices during 2013 at a coastal location of Adélie Land, as described in Amory et al. (2017). The measurement system consists of two 1 m long tubes superimposed vertically to sample the first two metres above the snow surface, which largely represent the total snow mass flux (Mann et al., 2000). Monthly values of horizontal snowdrift transport from 0 to 2 m are computed from half-hourly estimates of the snow mass flux and are compared with snowdrift transport fluxes from the drifting snow routine of 
            <rs type="software" id="s1">RACMO</rs>
            <rs type="version" corresp="s1">2.3</rs>. The measured flux represents a minimum estimate of the total column flux.
        </p>
        <p>-Kohnen radiosonde data (this study). Contact: gerit.birnbaum@awi.de. financial and logistical support of the French Polar Institute IPEV (programme CALVA-1013) Graphics and calculations were made using the 
            <rs type="software" id="s2">NCAR Command Language</rs> (version
            <rs type="version" corresp="s2">6.3.0</rs>,
            2017).
        </p>
        </text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f226041587"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T08:16+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Bivariate correlations were employed to analyze the patterns of associations between social networking addiction, IGD, attachment styles, and age. A multiple regression analysis was undertaken using forward method and entering demographic variables, including age and gender, to examine the predictive associations between attachment styles and social networking addiction. A p value of .05 was set as the critical level for statistical significance. All statistical analyses were performed using <rs id="a12901063" type="software">Mplus</rs> <rs id="a12901064" type="version" corresp="#a12901063">7.2</rs> and <rs id="a12901065" type="software">SPSS</rs> for Windows
20.0.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f288620267"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:31+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>(referred to as FEP+) currently enable end-to-end application. Here, we present for the first time an approach based on the open-source software <rs id="a12966002" type="software">pmx</rs> that allows to easily set up and run alchemical calculations for diverse sets of small molecules using the <rs id="a12966003" type="software">GROMACS</rs> MD engine. The method relies on theoretically rigorous non-equilibrium thermodynamic integration (TI) foundations, and its flexibility allows calculations with multiple force fields. In this study, results from the <rs id="a12966004" type="software">Amber</rs> and <rs id="a12966005" type="software">Charmm</rs> force fields were combined to yield a consensus outcome performing on par with the <rs id="a1234567" type="software" subtype="implict">commercial FEP+</rs> approach. A large dataset of 482 perturbations from 13 different protein-ligand datasets led to an average unsigned error (AUE) of 3.64 AE 0.14 kJ mol À1 , equivalent to <rs type="publisher" corresp="#a1234">Schr ödinger</rs>'s <rs id="a1234" type="software">FEP+</rs> AUE of 3.66 AE 0.14 kJ mol À1 . For the first time, a setup is presented for overall high precision and high accuracy relative protein-ligand alchemical free energy calculations based on open-source software.</p><p>The lead optimization (LO) stage of drug discovery involves the synthesis of hundreds of lead compound analogs, with the aim to improve multiple properties in parallel. Among these are selectivity against related targets, enhanced metabolic stability, permeability, solubility, reduced side effects, efflux, and plasma protein binding. Thus, LO is a multi-objective optimization problem in which chemists try to identify structure-property relationships that will allow to tune the chemical and biophysical properties of the lead compound. Ligand binding affinity for the primary protein target is central to all LO efforts as it impacts drug efficacy, and thus its dose and selectivity margins versus off-target effects. Computationally-driven guidance to LO requires precision and accuracy, and the predictive power of empirical scoring functions alone is rarely enough at this stage of drug discovery. [1][2][3][4] For this data-scarce yet multiparameter problem it remains to be seen if data-driven methods are able to predict new primary target activities. On the other hand, an approach that has shown the required level of performance is alchemical relative binding free energy (RBFE) calculations based on molecular dynamics (MD) simulations. [5][6][7] Free energy perturbation (FEP) 8,9 and thermodynamic integration (TI) 10 are popular methods used for alchemical RBFE estimation. The application of FEP in alchemical calculations dates back several decades and it typically uses molecular dynamics (MD) or Monte Carlo simulations to compute the freeenergy difference between two structurally related ligands, making it ideal for LO. [11][12][13][14][15] Equilibrium FEP is arguably the most common implementation of alchemical calculations and involves many distinct equilibrium MD simulations for all states along a l coordinate that alchemically modies the rst ligand into the second. It is common to use 12, 15 or more socalled l intermediates wherein atoms that need to appear, disappear, or mutate between the two ligands are represented by a linear combination of end-state Hamiltonians. During alchemical transformations, van der Waals and sometimes electrostatic interactions are soened to avoid singularities and numerical instabilities. [16][17][18] Various methods exist to calculate the free energy associated with a change of the l coordinate, but a requisite for convergence is an overlap in conformational space between neighboring simulations along the l path. TI differs from FEP in the way free energy difference is calculated as a function of l: integration of the derivative of the Hamiltonian with respect to l results in the free energy difference between end states. For FEP, if 1, 5 or 10 ns trajectories are required per l window in both solvent and complex the computation becomes expensive when performing hundreds or thousands of perturbations in a drug discovery LO program. Recently, however, this cost has been dramatically reduced by using graphics processing units (GPU) or massively parallel resources. [19][20][21] For instance, <rs type="publisher" corresp="#a12345">Schrödinger</rs>'s <rs id="a12345" type="software">FEP+</rs> <rs type="bibr">3</rs> implementation uses the GPU-enabled MD-engine <rs type="software">Desmond</rs>. <rs type="bibr">22</rs> This has led to an explosion of interest in this approach. In turn, application of FEP to a vast range of protein-ligand systems revealed that the method can indeed deliver accurate relative binding affinity predictions with an error of &lt;1 kcal mol À1 with respect to experiment. [23][24][25][26][27][28][29][30][31][32][33][34][35][36] However, the application of FEP using most MD soware remains challenging, preventing its widescale uptake.</p>
<p>Contrary to naturally-occurring amino acids, small molecules cover an almost innite chemical space. Hence, deriving appropriate force eld parameters for ligands can itself be challenging, and several recent reports address this. [37][38][39] The challenge in the RBFE calculations setup is to automatically recognize the structural differences between the ligands and prepare a sensible hybrid topology for MD simulations. Several programs that help with this [40][41][42][43] and other steps in the process 44,45 have been reported. Work from the de Groot lab has led to the development of <rs id="a12966006" type="software">pmx</rs>, <rs id="a12966007" type="bibr">46</rs>,<rs id="a12966008" type="bibr">47</rs> a tool to prepare inputs for alchemical free energy calculations 48 in <rs id="a12966009" type="software">GROMACS</rs>. <rs id="a12966010" type="bibr">49</rs> So far, <rs id="a12966011" type="software">pmx</rs> has delivered accurate results for the prediction of the effect of protein mutations on thermodynamic stabilities, 27,35,50,51 changes in protein-protein interaction free energies, 27 shis in the equilibria between protein conformational substates, 52 as well as DNA nucleotide mutations. 29 In this report, we demonstrate the rst application of <rs id="a12966012" type="software">pmx</rs> to relative protein-ligand binding free energies.</p>
<p>In our approach, <rs id="a12966013" type="software">pmx</rs> is used to identify optimal mappings between ligand atoms and generate hybrid structures and topologies for subsequent <rs type="software">GROMACS</rs>-based free energy calculations. In contrast to the typical FEP approach based on equilibrium sampling described above, we estimate free energy differences with alchemical non-equilibrium transitions using a TI approach. Equilibrium simulations are rst performed on the ligand-bound and -unbound states; then, short non-equilibrium simulations are used to perturb the ligands. Hundreds of short perturbations can be performed in the forward and backward direction, starting from snapshots covering the conformational space sampled from the equilibrated end states. The resulting free energy difference is derived from the overlap of work distributions associated with the forward and backward transitions using the Crooks Fluctuation Theorem. 53 A primary feature that discriminates between equilibrium and non-equilibrium alchemical approaches is the amount of sampling performed at the physical end states. Equilibrium FEP employs a number of intermediate non-physical simulations along the alchemical path and only two simulations sample the physical end states. The free energy difference of interest is, however, solely dened by the end statesin fact, the role of the intermediate states is merely to ensure a converged DG estimate. The non-equilibrium approach, in contrast, invests more sampling time in the end states, as only very short simulations in alchemical space are performed to connect the physical end states. In a few studies, the efficiency of the non-equilibrium approaches was compared to that of equilibrium methods. However, which of the two approaches is more efficient in practice is yet to be determined conclusively. For example, Ytreberg et al. 54 and Goette and Grubmüller 55 found bidirectional non-equilibrium approaches to be more efficient than equilibrium FEP. In contrast, Yildirim et al. 56 found equilibrium FEP to be more efficient; however, criticism of this study with respect to how efficiency was dened was expressed. 57 Notwithstanding the lack of consensus in the scientic community on this matter, our non-equilibrium protocols 58,59 have already provided high-accuracy predictions in a number of applications involving amino acid and nucleotide mutations. 27,29,34,35,59,60 Here, we use <rs id="a12966014" type="software">pmx</rs> to calculate the difference in binding free energy for 482 ligand perturbations across 13 different ligandprotein activity datasets in two contemporary force elds. The calculated free energy differences were combined into a consensus estimate from the results of both force elds providing further increase in accuracy. In this case the consensus approach consists of a simple averaging, but future extensions may also involve more sophisticated schemes, e.g. employing machine learning approaches to assign different weights to force elds. 27 We also used the commercial <rs id="a123456" type="software">FEP+</rs> implementation from <rs type="publisher" corresp="#a123456">Schrödinger</rs> as a state-of-the-art comparison. This is one of the largest protein ligand relative free energy calculation studies to date, and amongst the rst providing a large-scale comparison of implementations on different MD-engine soware. 61 The overall average unsigned error (AUE) of the predicted DDG was 3.64 AE 0.14 kJ mol À1 with <rs id="a12966015" type="software">pmx</rs> and 3.66 AE 0.14 kJ mol À1 with FEP+. The <rs id="a12966016" type="software">pmx</rs> tool is freely available at <rs id="a12966017" type="url" corresp="#a12966016">https://github.com/deGrootLab/pmx</rs>.</p>
<p>To help comparison with the prior literature, we selected benchmark sets studied in previous FEP reports. These included the 8 datasets from Wang et al.: 3 JNK1, TYK2, BACE, MCL1, CDK2, Thrombin, PTP1B, and P38. Furthermore, we included protein-ligand systems that have appeared in subsequent FEP studies: Galectin-3, 62 PDE2, 33 cMET 63 (from: https://github.com/ choderalab/yank-benchmark) and two additional BACE datasets. 26,28,64 This provided a total of 482 perturbations with experimental DDG values ranging from À20.7 to 15.4 kJ mol À1 . sidechains, and loops were modelled, protein protonation states were assigned with <rs id="a12966018" type="software">PROPKA</rs> at pH 7.0, metals were retained and zero bond order constraints to neighboring atoms were assigned, the hydrogen bonding network was optimized and the ligand charges were assigned. To relieve local clashes, a restrained minimization was performed with a 0.5 Å heavyatom RMSD displacement cut-off, below which the minimization was terminated. FEP+ calculations were performed using <rs id="a12966019" type="version" corresp="#a12966021">v2018-1</rs> of the <rs id="a12966020" type="publisher" corresp="#a12966021">Schrödinger</rs> <rs id="a12966021" type="software" subtype="implicit">modeling suite</rs>. The OPLS v3 force eld, the <rs id="a1234a" type="software">Desmond</rs> (MD) engine v<rs type="version" corresp="#a1234a">3.8.5</rs>, the replica exchange with solute tempering (REST-2), 65 and the multistate Bennett acceptance ratio (MBAR) approach to obtain free energy estimates, 9 were used. The REST region was applied only to ligand heavy atoms. Missing force eld parameters were added by tting to QM calculations using the uilder module. The FEP+ calculations were performed with 12 l-windows and 5 ns of production MD simulations per window. Equilibration was performed in ve steps: (i) 100 ps at 10 K with Brownian dynamics, NVT ensemble, solute heavy atom restraints and small (1 fs) timestep; (ii) 12 ps at 10 K with Berendsen thermostat, NVT ensemble, solute heavy atom restraints and small timestep; (iii) 12 ps at 10 K with Berendsen, NPT ensemble, solute heavy atom restraints, increase to default timesteps; (iv) 24 ps at 300 K with Berendsen, NPT ensemble, solute heavy atom restraints; (v) nally, 240 ps at 300 K with Berendsen, NPT ensemble and no restraints. Production simulations used the NPT ensemble and hydrogen mass repartitioning to permit a 4 fs timestep. Calculations were performed as three independent repeats using different random seeds. Error bars in the gures represent the standard error of DDG across the three repeats and the uncertainty reported by the MBAR estimator.</p>
<p>The initial ligand and protein structures were taken from the setup of the FEP+ approach. The necessary atom and residue naming adjustments, as well as modications of the nonstandard amino acid residues, were made for compatibility with the <rs id="a12966022" type="software">GROMACS</rs> naming convention. Ligand parameterization used the <rs id="a12966023" type="software">General Amber Force Field</rs> <rs id="a12966024" type="bibr">66</rs> (<rs id="a12966025" type="software">GAFF</rs> v <rs id="a12966026" type="version" corresp="#a12966025">2.1</rs>) and the <rs id="a12966027" type="software">CHARMM General Force Field</rs> <rs id="a12966028" type="bibr">67</rs> ( <rs id="a12966029" type="software">CGenFF</rs> v <rs id="a12966030" type="version" corresp="#a12966029">3.0.1</rs> and v <rs id="a12966031" type="version" corresp="#a12966029">4.1</rs>). For the <rs id="a12966032" type="software">GAFF</rs> parameter assignment, the <rs id="a12966033" type="software">ACPYPE</rs> <rs id="a12966034" type="bibr">68</rs> and <rs id="a12966035" type="software">Antechamber</rs> <rs id="a12966036" type="bibr">69</rs> tools were used. The AM1-BCC 70 charge model was used in combination with the <rs id="a12966037" type="software">GAFF</rs> parameters. <rs id="a12966038" type="software">CGenFF</rs> parameters were assigned using the automated atom-typing toolset <rs id="a12966039" type="software">MATCH</rs> <rs id="a12966040" type="bibr">71</rs> and replacing the bonded-parameters with those in <rs id="a12966041" type="software">CGenFF</rs> v <rs id="a12966042" type="version" corresp="#a12966041">3.0.1</rs>. For the BACE inhibitor sets, the <rs id="a12966043" type="software">MATCH</rs> algorithm was unable to identify the appropriate atom types, therefore in these cases a web-based atom-typing and parameter assignment server 72,73 was used. For the BACE inhibitors, the <rs id="a12966044" type="software">CGenFF</rs> v<rs id="a12966045" type="version" corresp="#a12966044">4.1</rs> bonded parameters were used. Ligands containing chlorine and bromine were decorated with virtual particles carrying a small positive charge, following the rules for <rs id="a12966046" type="software">GAFF</rs> <rs id="a12966047" type="bibr">74</rs> and <rs id="a12966048" type="software">CGenFF</rs>. <rs id="a12966049" type="bibr">75</rs> Having parameterized the ligands, hybrid structures and topologies for the ligand pairs were generated using <rs id="a12966050" type="software">pmx</rs>. A mapping between the atoms of two molecules was established following a predened set of rules to ensure minimal perturbation and system stability during the simulations. <rs id="a12966051" type="software">pmx</rs> follows a sequential, dual mapping approach. In the rst step, <rs id="a12966052" type="software">pmx</rs> identies the maximum common substructure between the two molecules and proposes this as a basis for mapping. In the second step, <rs id="a12966053" type="software">pmx</rs> superimposes the molecules and suggests a mapping based on the inter-atomic distances. Finally, the mapping with more atoms identied for direct morphing between the ligands is selected. Additionally, <rs id="a12966054" type="software">pmx</rs> ensures that no ring breaking and disconnected fragments in the mapping occur. The obtained mapping is used to create hybrid structures and topologies following a single topology approach.</p>
<p>The simulation systems for the solvated ligands and ligandprotein complexes were prepared by placing the molecules in dodecahedral boxes with at least 1.5 nm distance to the box walls. The TIP3P water model 76 was used to solvate the molecules. Sodium and chloride ions were added to neutralize the systems and reach 150 mM salt concentration. Proteins were parameterized in two different force elds: Amber99sb*ILDN [77][78][79] and CHARMM36m. 80 Ion parameters by Joung &amp; Cheatham 81 were used for simulations in <rs id="a12966055" type="software">Amber</rs>/<rs id="a12966056" type="software">GAFF</rs> force eld; <rs id="a12966057" type="software">Charmm</rs>/<rs id="a12966058" type="software">CGenFF</rs> simulations were performed with the default <rs id="a12966059" type="software">Charmm</rs> ion parameters.</p>
<p>Overall performance of the non-equilibrium free energy calculations Double free energy differences (DDG) were calculated for a set of 482 ligand modications across 13 protein-ligand datasets. This large set of diverse modications allows for a reliable comparison between the investigated alchemical approaches. Fig. 1A summarizes the main ndings: in absolute terms (average unsigned error, AUE), the pmx-based non-equilibrium free energy calculations perform equivalently to the state-of-theart FEP+ approach. Predictions of both approaches, on average, deviate from experiment by less than 1 kcal mol À1 (4.184 kJ mol À1 ). The individual force elds, <rs id="a12966060" type="software">GAFF</rs> and <rs id="a12966061" type="software">CGenFF</rs>, are outperformed by FEP+ using the proprietary OPLSv3 force eld. However, remarkably, the combination of free energy estimates obtained with <rs id="a12966062" type="software">GAFF</rs> and <rs id="a12966063" type="software">CGenFF</rs> force elds (even when considering equivalent sampling time) substantially improves the accuracy. The agreement with experiment in terms of Pearson correlation is slightly better for the FEP+ approach (0.69 AE 0.03) than for the consensus force eld approach based on the non-equilibrium free energy calculations (0.63 AE 0.03). Similar to the AUE comparison, in terms of Pearson correlation, the consensus force eld approach appears to yield higher quality estimates than the individual force elds, when considering all protein-ligand datasets together.</p>
<p>The comparisons described above took into consideration all the simulations performed for each approach, i. 88 However, the simulation time used for obtaining the latter result is not reported, complicating a direct comparison. In the current work, using FEP+ with the <rs id="opslv31234" type="software">OPLSv3</rs> force eld and combining free energy estimates from three independent FEP+ runs resulted in an AUE of 3.66 AE 0.14 kJ mol À1 . The non-equilibrium free energy calculations performed comparably to FEP+ and reached an AUE of 3.70 AE 0.17 kJ mol À1 when using 60 ns per DG estimate, and 3.58 AE 0.18 kJ mol À1 when using 2 Â 60 ns. In terms of correlation, the newer OPLSv3 shows improvement over OPLSv2.1: 0.65 AE 0.04 versus 0.59 AE 0.03. The <rs id="a12966064" type="software">pmx</rs>-based calculations show slightly lower correlation of 0.55 AE 0.04. In a recent study, the Wang et al. dataset was investigated with equilibrium TI calculations using the <rs id="a12966065" type="software">Amber</rs><rs id="a12966066" type="version" corresp="#a12966065">18</rs> simulation package. <rs id="a12966067" type="bibr">89</rs> The authors reported substantially worse performance than obtained in the current work: AUE of 4.9 kJ mol À1 and correlation of 0.48, investing 74 ns per DG estimate.</p>
<p>For the dataset of 152 mutations (Fig. 1C) assembled from the literature for this study, both FEP+ and non-equilibrium calculations reach similar correlation: 0.79 AE 0.04 and 0.76 AE 0.04, respectively. Interestingly, for this subset the AUE of the FEP+ predictions is lower than that of the consensus approach by 0.57 AE 0.33 kJ mol À1 (3.2 AE 0.21 and 3.77 AE 0.25 kJ mol À1 for FEP+ and <rs id="a12966068" type="software">pmx</rs>, respectively). These observations suggest the accuracy is dependent on the particular protein-ligand system studied. It is also important to note that the number of data points varies among the datasets, ranging from 7 in the case of galectin, to 71 in the case of MCL1. This emphasizes the importance of using large datasets for reliable method comparison.</p>
<p>For all the sets depicted in Fig. 1, the <rs id="a12966069" type="software">GAFF</rs> force eld outperforms <rs id="a12966070" type="software">CGenFF</rs>. Combining the results of both into a consensus estimate consistently yields a higher, or at least equivalent, accuracy compared to the <rs id="a12966071" type="software">GAFF</rs> force eld. Increasing the simulation time invested to obtain a DDG estimate has only a marginal effect on the results, given the time scales considered (at least 60 ns per DG). We have also probed the effect of simulation length on FEP+ accuracy by running 1 ns per l window and using 3 replicas, resulting in 36 ns per DG estimate, as opposed to the standard protocol using 5 ns per l window (180 ns per DG). Also in this case, the accuracy was only marginally affected by the shorter simulations: AUE of 3.88 AE 0.15 (1 ns) and 3.66 AE 0.14 kJ mol À1 (5 ns), and correlation 0.68 AE 0.03 and 0.69 AE 0.03, respectively.</p>
<p>To further assess the sensitivity of the <rs id="a12966072" type="software">GROMACS</rs> calculations to the invested sampling time, we estimated DDG values aer discarding half of the simulation time. Such a protocol resulted in a setup using 3 replicas of 10 ns, which closely matches the 1 ns FEP+ protocol (1 ns Â 12 l-windows Â 3 replicas). The AUE of the <rs id="a12966073" type="software">GAFF</rs> calculations was 4.03 AE 0.16 kJ mol À1 and the Pearson correlation 0.59 AE 0.03. The <rs id="a12966074" type="software">CGenFF</rs> calculations had an AUE of 4.7 AE 0.19 kJ mol À1 and correlation of 0.53 AE 0.04. The modest decrease in accuracy matches well with the similar effect observed for the FEP+ calculations. It appears that even the shorter investigated sampling times are sufficient to explore the local minima in the vicinity of the starting structure to obtain a converged free energy estimate. This is corroborated by our earlier explorations of sampling strategies applied in drug resistance mutation studies, where investing 54 ns per DG value yielded converged results. 35 The scatter plots of the calculated and experimental double free energy differences provide an intuitive understanding of the ranges spanned by the datasets and the calculated values (Fig. 2). While taken separately the <rs id="a12966075" type="software">GAFF</rs> and <rs id="a12966076" type="software">CGenFF</rs> force elds produce more outliers than FEP+ with <rs id="a12345677" type="software">OPLSv3</rs> (Fig. 2 and S1 ‡), the consensus results reduce the number of outliers. The proportion of estimates falling within 1 kcal mol À1 (4.184 kJ mol À1 ) of experiment is 68 AE 2% and 66 AE 2% for the FEP+ and the pmx-based consensus force eld approach respectively. The overall range spanned by the estimated DDG values is comparable between the methods and force elds as well as similar to the distribution of experimental values (Fig. S2 ‡). The consensus non-equilibrium estimates were more accurate than FEP+ for the perturbations associated with a small DDG (Fig. S4 ‡), whereas FEP+ was more accurate for larger DDG perturbations.</p>
<p>A notable difference between the results of the methods is the magnitude of estimated errors (Fig. 2 and S3 ‡): the nonequilibrium free energy estimates have larger associated errors than those predicted by FEP+. It is important to note that error estimates for the individual DDG values comprise both the uncertainty of the estimator and the standard error of the estimates coming from the different simulation replicas. Furthermore, the consensus approach increases the errors because the <rs id="a12966077" type="software">GAFF</rs> and <rs id="a12966078" type="software">CGenFF</rs> estimates may differ from each other more than the estimates obtained with individual force elds. While this feature allows for an increased prediction accuracy, it also increases the uncertainty associated with an estimate.</p>
<p>The agreement between the free energy predictions and experiment is system dependent. Fig. 3 summarizes the AUE and Pearson correlation for every protein-ligand complex studied (Fig. S8 ‡ shows average signed errors). Together with the system-dependent accuracy, Fig. 3 again highlights the value of the consensus force eld approach. In several cases the DDG estimate between force elds varies greatly, leading to substantially different
AUEs (<rs id="a12966079" type="software">CGenFF</rs> shows larger AUEs for one of the BACE sets, TYK2, MCL1, and P38, while <rs id="a12966080" type="software">GAFF</rs> for cMET). This is also the case for the Pearson correlation. Taking the consensus of the estimated free energy differences by using a simple average of the values from the two force elds yields a result outperforming or on par with the best result from a single force eld.</p>
<p>The improved accuracy due to combination of results from different force elds may seem counterintuitive. In fact, if both force elds yield DDG estimates deviating from experiment in the same direction, the consensus approach would yield only an intermediate quality prediction falling in between the two individual force elds. Such an outcome would still be preferable in a prospective study, since relying on a single force eld might lead the investigation in a wrong direction. In the current work, however, employing a consensus approach generally resulted in an improved prediction accuracy over any of the single force elds. This is only possible because in 33% of all the calculated double free energy differences the values obtained by <rs id="a12966081" type="software">GAFF</rs> and <rs id="a12966082" type="software">CGenFF</rs> force elds were pointing in opposite directions from the experimental measurement (see also Fig. S10 ‡ for a graphical depiction of the signed deviations from experiment for both force elds plotted one against the other).</p>
<p>The variable performance of calculated DDG for individual protein-ligand complexes can be seen from the scatter plots in Fig. 4 (for the FEP+ estimates see Fig. S6 ‡). In the majority of cases, the estimates fall within 1 kcal mol À1 (4.184 kJ mol À1 ) of the experimental measurement. This indicates that the accuracy is mainly reduced by a small number of outliers. The latter observation holds for both the consensus approach based on the non-equilibrium calculations (Fig. 4) and FEP+ using the <rs id="a1234567899" type="software">OPLSv3</rs> force eld (Fig. S6 ‡). Interestingly, both approaches have difficulties with the MCL1 dataset where only half of the estimates fall within 1 kcal mol À1 (4.184 kJ mol À1 ) of the experimental measurement. 45% of the non-equilibrium estimates fell outside this range for the BACE set of Hunt et al. 64 and for the cMET set. FEP+ had comparable difficulties with the BACE set of Cumming et al. 90 and PDE2. 33 The range spanned by the DDG values also has an inuence on the prediction accuracy (Fig. S5 ‡). An illustrative example for this effect is a set of thrombin inhibitors. The experimental range of the double free energy differences is narrow. The nonequilibrium approach captured the DDG values very accurately in terms of AUE (2.23 AE 0.57 kJ mol À1 ). However, no correlation for the small differences between the ligands was observed. In contrast, FEP+ had a signicantly larger AUE of 4.51 AE 0.82 kJ mol À1 . However, it was able to achieve moderate correlation (0.45 AE 0.18). In general, FEP+ obtained higher correlations: the averaged correlation coefficient was higher for 9 out of 13 datasets (Fig. S7 ‡). In terms of AUE, on average, FEP+ was more accurate than the <rs id="a12966083" type="software">pmx</rs>-based calculations for 7 out of 13 datasets. When compared to the previous generation of the OPLS force eld (v2.1), 3 the consensus force eld approach performs better in 6 out of 8 cases in terms of AUE and 3/8 in terms of correlation. It is worth noting that the earlier FEP+ results reported by Wang et al. were more accurate for BACE and thrombin than those obtained here with the newer OPLS version.</p>
<p>Protonation effects. For one system (PTP1B), we looked in detail at the molecular determinants inuencing the free energy calculation accuracy. In particular, we investigated the effects of the protonation state of the catalytic cysteine. PTP1B is a tyrosine phosphatase that harbors a catalytic cysteine (Cys215) that can be oxidized, thus inhibiting the enzyme. 91 When in the apo state, Cys215 has been shown to be deprotonated (pK a ¼ 5.4) and make a covalent bond with the main chain nitrogen of Ser216. 92 The protonation state of Cys215 is not known for the set of PTP1B inhibitors probed here. From the crystallographic structures resolved with four of the ligands in the set, 93 the short Fig. 4 Performance of the <rs id="a12966084" type="software">pmx</rs>-based consensus force field calculations for each protein-ligand system studied. The DDG estimates are plotted against their experimentally determined values. Text in the panels: AUE is in kJ mol À1 ; "cor" is Pearson correlation; "1 kcal/mol" denotes the percentage of the estimates that fall within 1 kcal mol À1 (4.184 kJ mol À1 ) of the experimental measurement; "values" refers to the total number of perturbations per dataset.</p>
<p>We further probed whether the ligand's carboxyl group or Cys215 is more likely to be protonated. The empirical pK a predictor <rs id="a12966085" type="software">PROPKA</rs> <rs id="a12966086" type="version" corresp="#a12966085">3.1</rs> 94,95 suggested that the pK a for the carboxyl group is less than 5.0 for every ligand in the set. The low carboxyl pK a was also conrmed by the <rs id="a12966087" type="publisher" corresp="#a12966088">ChemAxon</rs>'s <rs id="a12966088" type="software" subtype="implicit">predictor</rs>. <rs id="a12966089" type="bibr">96</rs> In contrast, the pK a for Cys215 in the complexed system was predicted to be between 9.8 and 10.5, depending on the inhibitor. Taken together, these observations suggest that Cys215 ought to be protonated for the inhibitor set synthesized by Wilson et al. 93 Wang et al., 3 however, modeled a deprotonated variant of Cys215 in their free energy calculations, whilst also keeping the ligand's carboxyl group deprotonated. Although the carboxyl groups of the ligand are not modied in the alchemical simulations it is plausible that structurally diverse inhibitors may be affected differently by the two negative charges nearby. Using the Wang et al. setup with the deprotonated Cys215 we obtained similar quality free energy estimates (Fig. 6). Briey, the Cys(À1) results from Wang et al. had an AUE and correlation of 3.87 AE 0.52 kJ mol À1 and 0.64 AE 0.06, respectively, compared to 3.66 AE 0.56 kJ mol À1 and 0.61 AE 0.08 from the <rs id="a12966090" type="software">pmx</rs> consensus predictions, also with similar outliers as seen in the correlation plots. Interestingly, the FEP+ calculations performed here using <rs id="a123456799" type="software">OPLSv3</rs> showed substantially better accuracy (AUE of 2.8 AE 0.27 kJ mol À1 and correlation of 0.91 AE 0.03), suggesting the newer force eld includes updates that have an improved representation of interactions between the deprotonated thiol and carboxyl group for the investigated set of ligands. Since our empirical prediction suggests that Cys215 could be protonated we have also calculated free energy differences with this protonation state. Interestingly, upon protonation of Cys215 the quality of FEP+ <rs id="a12345678" type="software">OPLSv3</rs> prediction drops (Fig. 6): AUE 3.68 AE 0.49 kJ mol À1 , correlation 0.8 AE 0.07.</p>
<p>The <rs id="a12966091" type="software">pmx</rs> calculations using the consensus force eld approach follow a different trend. When Cys215 is deprotonated and turned into a neutral residue (by redistributing charges on the side-chain atoms), the agreement with experiment increases. This articially constructed cysteine residue should not be interpreted in physical terms (e.g. as a radical). It rather represents a convenient intermediate step between the negative deprotonated cysteine in the active site of PTP1B and the properly protonated neutral Cys215. Agreement with experiment further improves when Cys215 is protonated (Fig. 6): AUE 3.23 AE 0.42 kJ mol À1 , correlation 0.74 AE 0.06. The increased accuracy when protonating Cys215 could be an artifact of a decient parameterization of the thiolate group in <rs id="a12966092" type="software">Amber</rs> and <rs id="a12966093" type="software">CHARMM</rs> force elds. 97 On the other hand, it may also suggest that the thiol group of the cysteine residue is protonated upon binding of the ligands from the investigated set of PTP1B inhibitors.</p>
<p>The set of galectin inhibitors contains only 8 ligands connected by 7 perturbations. <rs id="a123456789" type="software">OPLSv3</rs> performed particularly well in this case: AUE of 1.2 AE 0.5 kJ mol À1 , correlation of 0.98 (Fig. 3). Both <rs id="a12966094" type="software">GAFF</rs> and <rs id="a12966095" type="software">CGenFF</rs> force elds show a lower accuracy in terms of AUE (3.0 AE 1.2 and 2.2 AE 0.4 kJ mol À1 , respectively). In terms of correlation, <rs id="a12966096" type="software">GAFF</rs> has a below-average agreement with experiment and a large associated uncertainty (0.41 AE 0.4). A closer look into the DDG estimates obtained with the <rs id="a12966097" type="software">GAFF</rs> force eld highlights a peculiar case of possible error cancellation in the free energy estimates (Fig. 7). A large AUE for the perturbation transforming a methylamino group (-NHMe) to methoxy (-OMe) suggests that the parameterization of one or both of these moieties might be imperfect. However, perturbations of these groups into more chemically similar substituents gave more accurate DDG estimates: methylamine to dimethylamine; methoxy to hydroxyl. The parameterization errors pertaining to a specic chemical group cancel out until transformations involving different chemistry (with different parameterization errors) are introduced: e.g. free energy differences within the group of ligands containing methylamine in the current case are represented correctly. Similarly, the free energy differences within the group of compounds containing methoxy and hydroxyl groups are accurately estimated. However, the free energy difference between these two sets of ligands containing different chemical groups is not captured accurately (at least not with the sampling time used in the current study).</p>
<p>Directions for force eld optimization. The consensus force eld approach provided more accurate predictions than the individual <rs id="a12966098" type="software">GAFF</rs> and <rs id="a12966099" type="software">CGenFF</rs> force elds. As already mentioned, an improvement in accuracy is only possible if the two force eld estimates are opposite with respect to the experimental result. The cMet protein-ligand dataset provides an informative example: in 14 out of 25 (56%) cases <rs id="a12966100" type="software">GAFF</rs> and <rs id="a12966101" type="software">CGenFF</rs> predictions had an error in different directions from the experimental measurement. The cMet inhibitor set contains 12 ligands with a common scaffold (Fig. 8A) and a single substitution site (Fig. 8B), except for compound 1_21, which also has a cyano group in place of a scaffold uorine atom.</p>
<p>Overall, for this system FEP+ showed an AUE of 3.2 AE 0.58 kJ mol À1 , while <rs id="a12966102" type="software">CGenFF</rs> was only slightly worse with an AUE of 3.78 AE 0.59 kJ mol À1 . Interestingly, this dataset gave the worst performance for <rs id="a12966103" type="software">GAFF</rs> among all the investigated protein-ligand complexes: AUE of 5.55 AE 0.94 kJ mol À1 .</p>
<p>A closer look at the major discrepancies between force elds reveals some peculiar trends that could be useful for further force eld ne tuning. For example, in all four transformations with compound 4200_15, <rs id="a12966104" type="software">GAFF</rs> overestimates the binding affinity of this ligand in comparison to both <rs id="a12966105" type="software">CGenFF</rs> and experiment. Similarly, compound 400_10 is consistently (3 transformations) predicted by <rs id="a12966106" type="software">GAFF</rs> to be a higher affinity binder than determined experimentally. In contrast, all 6 transformations involving ligand 5300_8 with <rs id="a12966107" type="software">GAFF</rs> suggest the inhibitor to be a far worse binder than measured experimentally. Although pinpointing the exact force eld parameters that are responsible for these inaccuracies is not trivial, the trends observed for certain chemical groups suggest likely candidates for re-parameterization. Similarly, we can envisage future work using large-scale scans of such calculated thermodynamic properties of biomolecular complexes to aid force eld development.</p>
<p>Overall, the current investigation revealed several consistent trends. The accuracy in terms of AUE was comparable for the FEP+ and <rs id="a12966108" type="software">pmx</rs>-based calculations, while the correlation was slightly higher for FEP+ when using the <rs id="a1234567891" type="software">OPLSv3</rs> force eld (Fig. 1). The <rs id="a12966109" type="software">GAFF</rs> force eld yielded higher accuracy than <rs id="a12966110" type="software">CGenFF</rs>, however, the consensus approach of averaging the results from both force elds performed better or equally well as the best performing force eld. This indicates that the errors made by the force elds in free energy estimates are in some cases cancelling out, allowing for an increased accuracy. This effect has been previously observed in the free energy estimations for amino acid mutations in protein stability and proteinligand binding studies, 27,35 as well as for nucleotide mutations in protein-DNA interactions. 29 Furthermore, the benets of the consensus approach are emphasized in the case-by-case analysis of the protein-ligand complexes studied (Fig. 3). Here, it becomes evident that in a prospective study of a particular system relying on the results of a single force eld may lead to a substantial decrease in the predictive accuracy. In fact, in the current investigation, the two force elds gave opposite results with respect to experiment for as many as 33% of the cases, while in 10% of the cases the estimates from two force elds showed a statistically signicant systematic difference. Admittedly, using a consensus approach requires additional effort in preparing the simulation system. With the currently available soware packages, 68,71 however, automation of such procedures should not pose a considerable challenge.</p>
<p>The signicant difference in standard errors obtained from repeated calculations represents an interesting difference between the FEP+ and non-equilibrium TI based free energy results. With an average standard error of 0.57 kJ mol À1 per DDG value, FEP+ provides predictions with high precision. That is, the DDG estimates from FEP+ converge to highly similar values, with little spread in the results. This might be a consequence of the enhanced sampling technique (REST 65 ) ensuring convergence of the FEP+ simulations. <rs id="a12966111" type="software">pmx</rs>-based non-equilibrium calculations, on the other hand, come with higher uncertainty: 2.36 kJ mol À1 on average for the consensus results. The larger spread of the calculated DDG values, in comparison to FEP+, suggests that the non-equilibrium calculations could still benet from an increased convergence: longer simulations or an enhanced sampling approach present a compelling direction for further investigation. Considering that both FEP+ and <rs id="a12966112" type="software">pmx</rs>-based calculations have, on average, a similar AUE of $3.7 kJ mol À1 , the high precision associated with FEP+ indicates that the method is highly precise even for those predictions that are substantially different from experiment. The <rs id="a12966113" type="software">pmx</rs>-based calculations give a larger prediction uncertainty, thus encompassing the experimental observation within the condence interval of the estimate. It remains to be explored whether increased precision of the <rs id="a12966114" type="software">pmx</rs>-based calculations (using longer simulations or an enhanced sampling technique) will have an effect on the accuracy of free energy estimates.</p>
<p>The success of combining results from <rs id="a12966115" type="software">GAFF</rs> and <rs id="a12966116" type="software">CGenFF</rs> indicates differences in the force eld parameterization. Naturally, the simplistic forms of the potential energy functions used by the classical molecular mechanics force elds cannot capture the full complexity of molecular interactions, for which a more complex representation would be required, e.g., polarizability. 100,101 Force eld parameterization based on a large number of quantum chemical calculations is helpful, as illustrated by the high accuracy achieved by FEP+ with the <rs id="a1234567892" type="software">OPLSv3</rs> force eld. However, the simplied description of the potential energy leads to unavoidable, inherent limitations. Thus, at this time, combining estimates from different force elds may be an attractive avenue to pursue. Given that parameterization of different force elds relies on different theoretical premises, combining their results may indirectly capture features of molecular interactions that are inaccessible to a single force eld. Finally, the signicant prediction differences obtained when altering the protonation state of a single amino acid sidechain highlight the sensitivity of alchemical methods to the simulation setup and force eld parameterization details. Furthermore, this example emphasizes the need for transparent and open-source force eld parameters akin to those put forward by the Open Force Field Consortium. 102</p>
<p>We thank Dr James P. Edwards for reviewing the manuscript and the <rs id="a12966117" type="software">GROMACS</rs> development team for making their soware freely available. L. P. B. was partly funded by the European Union's Horizon 2020 Research and Innovation Programme under grant agreement No 675451 (CompBioMed project). The project was also partly funded by Vlaams Agentschap Innoveren &amp; Ondernemen Project 155028. V. G. and B. L. d. G. were supported by the BioExcel CoE (http://www.bioexcel.eu), a project funded by the European Union (Contract H2020-EINFRA-2015-1-675728). M. A. was supported by a Postdoctoral Research Fellowship of the Alexander von Humboldt Foundation.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f81049903"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:16+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The age of the nodes for which time estimates were available was fixed using the <rs id="a12951942" type="software" subtype="component" corresp="#a12951943">Node Age Constraint</rs> tool in <rs id="a12951943" type="software" subtype="environment">Mesquite</rs> <rs id="a12951944" type="version" corresp="#a12951943">3.02</rs> <rs id="a12951962" type="bibr">(Maddison and Maddison 2015)</rs>. The timecalibrated supertree was inferred using the combination of <rs id="a12951946" type="software" subtype="component" corresp="#a12951948">Enforce Minimum Node Age Constraints</rs> and <rs id="a12951947" type="software" subtype="component" corresp="#a12951948">Arbitrarily Ultrametricize</rs> functions in <rs id="a12951948" type="software" subtype="environment">Mesquite</rs> <rs id="a12951949" type="version" corresp="#a12951948">3.02</rs> (<rs id="a12951950" type="bibr">Maddison and Maddison 2015</rs>).</p>
<p>The set of characters was mapped onto the tree topology. All character states in the outgroup were scored as 0, absent (i.e., plesiomorphic character states for all characters). Maximum parsimony and maximum likelihood reconstruction of ancestral character states were performed in <rs id="a12951951" type="software">Mesquite</rs> <rs id="a12951952" type="version" corresp="#a12951951">3.02</rs> <rs id="a12951964" type="bibr">(Maddison and Maddison 2015)</rs>. The Markov k-state 1 parameter model (Mk1) that assumes an equal rate of change between all character states (Lewis 2001) was used for maximum likelihood reconstruction. An asymmetric likelihood ratio test (Pagel 1999b) indicated that the asymmetric twoparameter model does not offer a significant improvement over the Mk1 model for any of the seven characters in question, thus validating the Mk1 model. Each character was mapped onto a set of topologies using the <rs id="a12951954" type="software">Trace Character History</rs> function.</p>
<p>In order to test hypotheses about temporal ordering of character state changes and coevolution of traits we used Pagel's test for correlated discrete character evolution (Pagel 1994(Pagel , 1999a) ) implemented in the <rs id="a12951955" type="software" subtype="component" corresp="#a12951956">Pagel94</rs> module in <rs id="a12951956" type="software" subtype="environment">Mesquite</rs> <rs id="a12951957" type="version" corresp="#a12951956">3.02</rs> <rs id="a12951965" type="bibr">(Maddison and Maddison 2015)</rs>. This method uses a continuous-time Markov model to infer character changes along each branch of a phylogenetic tree in order to establish the most likely temporal ordering and direction of evolutionary change and the most probable evolutionary pathway between two discrete binary characters. Evolutionary change in each character along the tree branches is modelled as a Markov process, in which the likelihood of character change is dependent on its current character state.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f33740381"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:17+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Our descriptive statistics show that younger individuals had higher levels of school attendance. To test whether there is an effect of age independent of schooling, we also ran a set of multivariate regressions. Specifically, we ran a Poisson multivariate regression for each of the variables derived from the verbal-memory task as dependent variable. An OLS model was used for the variable Learn slope. As explanatory variables we included 1) society's dummies, 2) a binary variable that captures whether the person is a man or a woman, 3) a set of dummies for the age categories, and 4) a binary variable that captures whether the person had attended school or not. For statistical analysis we used <rs id="a12951896" type="software">Stata</rs>. Data are available in S1 Dataset.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f343942348"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:47+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The type I error tolerance was set at 0.05 (95% CI) and all tests were two tailed. The entire analysis was performed using <rs id="a12894635" type="software">STATA</rs>, Version <rs id="a12894636" type="version" corresp="#a12894635">15.0</rs> ( <rs id="a12894637" type="publisher" corresp="#a12894635">Stata Corp</rs>, College Station, TX, USA).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f81531127"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:20+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Data were analyzed using <rs id="a12951824" type="software">Enhanced ChemStation</rs> (<rs id="a12951825" type="version" corresp="#a12951824">E02.02.1431</rs>) from <rs id="a12951826" type="publisher" corresp="#a12951824">Agilent Technologies</rs>. Firstly, the mass spectra recorded were compared to the mass spectral library of unknown peaks (see Tables S1-11) considering all peaks with a peak area of ≥0.2 % of the total. If matches were below 80 with the self-made library (<rs id="a12951827" type="software">ChemStation</rs> score, with 100 being the best possible match), the <rs id="a12970890" type="software">NIST MS</rs> library ( <rs id="a12951829" type="software">MS Search</rs> version <rs id="a12951830" type="version" corresp="#a12951829">2.0 g</rs>) was used for spectral comparison. In this case, matches with scores higher than 90 were accounted true.</p>
<p>The second approach for pigment identification was a modified version of the method published by Yang et al. (2014). They used a statistical comparison of average mass spectra (AMS) of vehicle top-coatings to describe hierarchical cluster similarity with reference samples of different manufacturers. Unfortunately, this kind of data processing would be only suitable to identify a "brand" rather than single components of the inks. We therefore modified this data evaluation approach using AMS to create a mass spectral library of the 36 pigments pyrolyzed in our study. The AMS of unknown samples were then compared to the AMS library using the <rs id="a12954585" type="software">NIST MS</rs> <rs id="a12951833" type="version" corresp="#a12954585">2.0</rs> program. High abundant masses from column bleed or other column noises were excluded and masses in the range of 30-400 Da were included in the search. The highest match between a certain tattoo ink and the AMS library was taken as pigment hit. By that, we were able to identify the most abundant pigments in a few seconds in 92.9 % of all tattoo inks and around 80 % in self-made mixtures (Table 2). Hence, this method would facilitate fast and easy screenings, which then can be manually re-assessed using the evaluation approach explained above.</p>
<p>We were able to prove py-GC/MS suitable for the identification of polymers and pigments used in tattoo inks. Main advantages of this method are the absence of any sample purification steps and a relatively high sensitivity in distinguishing different ingredients of multi-component inks. Py-GC/MS is applicable to a wide range of pigments, including phthalocyanines, diketopyrrolopyrrols, quinophthalones, triphendioxazines and, most importantly, azo pigments. However, in some of the commercial inks investigated some of the pigments could not be identified that have been declared on the label of the container. Occasionally pigments are only used in minute amounts to achieve a certain color shade. Hence, their concentrations might have fallen below the detection limit. Additionally, quinacridones could not be sufficiently identified and thus were a cause for false or incomplete identification. Our results are in accordance with the literature and demonstrate the unsuitability of py-GC/MS for identification of quinacridones (Ghelardi et al. 2014;Russell et al. 2011). Similarly, the polycyclic P.O.43, P.R.179 and other similar structures could not be identified in pigment mixtures. Like quinacridones, these pigments miss specific cleavage sites making identification through py-GC/MS rather difficult. Two different data evaluation approaches were applied: (1) Average mass spectra (AMS): chromatograms of tattoo inks were converted into AMS and compared to an AMS library made of the 36 pigments under consideration by using the <rs id="a12954586" type="software">NIST MS</rs> program. The best match was taken as a possible hit for pigment identification. Percentage of wrong hits was calculated by division of false identifications by number of inks; (2) Fragment comparison: all peaks at levels of ≥0.2 % of the total peak area were compared to the NIST MS library and the spectra and molecular masses of unknown pyrolysis products (Tables S1-S11); the percentage of wrong hits was calculated by division of false identifications by the total number of pigments present in the inks. Wrongly identified pigments are marked in italics, and pigments that could not be identified in either of the methods are marked in bold as "missing" The unfeasibility to identify pigments without characteristic cleavage sites is a limiting factor of the py-GC/MS approach applied. Methods such as LDI-ToF-MS (insoluble pigments) or LC-MS (soluble pigments) may serve as suitable complementary techniques for the identification of pigments in complex mixtures (Hauri 2014). In addition, FT-IR and Raman spectroscopy might help to pinpoint the most abundant pigments, but will be of limitation in mixtures (Poon et al. 2008;Timko et al. 2004).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f223540035"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:48+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The <rs id="a12970902" type="software" subtype="implicit">HMM software</rs> used is based on Chan and Lee (2013), one of whose authors programmed our <rs id="a12970901" type="software" subtype="implicit">software</rs>
in <rs id="a12951779" type="language" corresp="#a12970901">Perl</rs> for a Linux environment specifically for the current study. The model was initialized with a linear structure where state 1 can transit to state 1 or state 2, state 2 can transit to state 2. The parameters were optimized according to the forward-backward algorithm to find the best model (Rabiner, 1989). The data training with the algorithm was discontinued when the model converged, i.e. when further iteration resulted in no significant improvement of the model and the optimal stages had been probabilistically determined. After obtaining the set of parameters, the single best stage sequence was calculated with the Viterbi algorithm (Ryan &amp; Nudd, 1993).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f189127518"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T05:59+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>To measure and characterize the biggest challenges that librarians face while collaborating on systematic review projects, we used a web-based survey designed in <rs id="a12970660" type="software">Qualtrics survey software</rs> ( <rs id="a12900031" type="publisher" corresp="#a12970660">Qualtrics</rs>, Provo, UT). The thirteen-item survey consisted of screening questions and questions related to demographics, training, and challenges associated with systematic reviews, as well as strategies used to overcome those challenges. An online survey was selected as the best method for describing the characteristics of this population since it allowed us to gather more results from a wider range of participants.</p>
<p>Descriptive statistics were summarized using <rs id="a12900032" type="software">SPSS</rs>, version <rs id="a12900033" type="version" corresp="#a12900032">23</rs> (Chicago, IL). Where respondents were asked how frequently they had experienced a variety of challenges related to collaborating on systematic reviews ("Never," "Rarely," "Sometimes," "Often," "Not sure"), we collapsed responses into three variables for analysis. The options "Often" or "Sometimes" were combined to represent a common challenge. The options "Rarely" or "Never" were combined to represent a rare or nonexistent challenge. The remaining option "Not sure" could represent that the respondent was unsure of the meaning or how to respond. Openended responses were imported into Dedoose (Los Angeles, CA), and a thematic analysis was performed using a grounded theory approach to identify common themes.</p>
</text>
</tei>
  <tei>
    <teiHeader>
        <fileDesc id="f558985933"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T16:03+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text lang="en">
        <p>Background: Recently, much attention has been given to e-learning in higher education as it provides better access to learning resources online, utilising technologyregardless of learners' geographical locations and timescaleto enhance learning. It has now become part of the mainstream in education in the health sciences, including medical, dental, public health, nursing, and other allied health professionals. Despite growing evidence claiming that e-learning is as effective as traditional means of learning, there is very limited evidence available about what works, and when and how e-learning enhances teaching and learning. This systematic review aimed to identify and synthesise the factorsenablers and barriersaffecting e-learning in health sciences education (el-HSE) that have been reported in the medical literature. Methods: A systemic review of articles published on e-learning in health sciences education (el-HSE) was performed in MEDLINE, EMBASE, Allied &amp; Complementary Medicine, DH-DATA, PsycINFO, CINAHL, and Global Health, from 1980 through 2019, using 'Textword' and 'Thesaurus' search terms. All original articles fulfilling the following criteria were included: (1) e-learning was implemented in health sciences education, and (2) the investigation of the factorsenablers and barriersabout el-HSE related to learning performance or outcomes. Following the PRISMA guidelines, both relevant published and unpublished papers were searched. Data were extracted and quality appraised using 
            <rs type="software">QualSyst</rs> tools, and synthesised performing thematic analysis.
        </p>
        <p>The citations identified through the searches was imported into 
            <rs type="software" id="s1">Refworks</rs> software (
            <rs type="url" corresp="s1">https://www.refworks. com</rs>/). The literature which emerged from the databases, snowballing and hand-searching has been screened at two stages: first, a review of abstracts and titles of the retrieved literature to see whether they meet minimum inclusion criteria. Second, the full text of the included articles was reviewed and retrieved using a critical appraisal tool [35]. As Means et al. [16] argue, "the intent of the two-stage approach was to gain efficiency without risking exclusion of potentially relevant, highquality studies of online learning effects". The standard PRISMA flowchart has been used to provide the process of study selection [36] (Fig. 1).
        </p>
        <p>Methodological quality of the included studies was assessed with the '
            <rs type="software" id="s2">QualSyst</rs> developed by <rs type="creator" corresp="s2">Kmet and colleagues</rs> <ref type="bibr">[35]</ref>, and we particularly found its scoring system useful because it has clearly shown the process to be more "systematic, reproducible and quantitative means of assessing the quality" of those retrieved papers [66]. There are checklists of 14 question items for assessing quantitative and 10 questions for qualitative studies, and a score of 0-2 has been awarded to each item, with a final score calculated by summating the total score across the items and dividing them by the total possible sum (e.g. 28 for quantitative and 20 for qualitative studies) [66,67]. A cut-off of 75% as the threshold for quantitative, and 55% for qualitative papers has been set up. For mixed methods studies, specifically designed questions were employed to assess the quality [67]. Complete details regarding quality appraisals of individual studies were provided in the Additional file 2.
        </p>
        </text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f304620792"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:19+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Slurry samples originating from digesters at Fredericia and Randers WWTPs were used for DNA extraction and metagenomic sequencing (Data S1, Supplementary Methods Section 1.2). The raw metagenomic sequences were trimmed and assembled with <rs id="a12972835" type="software">CLC Genomics Workbench</rs> v. <rs id="a12972836" type="version" corresp="#a12972835">9.5.2</rs> ( <rs id="a12972837" type="publisher" corresp="#a12972835">QIAGEN Bioinformatics</rs>), generating 12 singleassemblies and 6 co-assemblies (Table S2). Details are provided in Supplementary Methods.</p>
<p>The assemblies and associated mapping data were exported as FASTA and BAM files, respectively. Script <rs id="a12972838" type="software" subtype="component" corresp="#a12972839">jgi_summarize_bam_contig_depths</rs> from the <rs id="a12972839" type="software" subtype="environment">MetaBAT2</rs> package was used to calculate coverage from the BAM files for each assembly. Metagenomic binning was applied to both single-and co-assemblies using <rs id="a12972840" type="software">MetaBAT2</rs> v. <rs id="a12972841" type="version" corresp="#a12972840">2.12.1</rs> <rs id="a12972842" type="bibr">[22]</rs>, with options -minContigLength 2000, -minContigDepth 2. This resulted in 491 bins for the digester at Fredericia WWTP and 1007 for that at Randers WWTP. The bins from each digester were dereplicated to produce medium to high quality MAGs using <rs id="a12972843" type="software">dRep</rs> v. <rs id="a12972844" type="version" corresp="#a12972843">2.2.1</rs> <rs id="a12972845" type="bibr">[23]</rs> with options dereplicate_wf -p 16 -comp 50 -con 25.</p>
<p>The completeness and contamination of each MAG were assessed based on the presence of lineage-specific, conserved, single-copy marker genes using <rs id="a12972846" type="software">CheckM</rs> v. <rs id="a12972847" type="version" corresp="#a12972846">1.0.11</rs> <rs id="a12972848" type="bibr">[24]</rs>. Objective taxonomic classifications were assigned to the MAGs according to the Genome Taxonomy DataBase (GTDB) taxonomy (release 03-RS86) using the toolkit <rs id="a12972849" type="software" subtype="environment">GTDB-Tk</rs> (v. <rs id="a12972850" type="version" corresp="#a12972849">0.1.3</rs>) with the <rs id="a12972851" type="software" subtype="component" corresp="#a12972849">classify</rs> workflow <rs id="a12972852" type="bibr">[25]</rs>. A phylogenetic genome tree of the bacterial MAGs was created and refined as detailed in Supplementary Methods.</p>
<p>The MAGs were annotated using <rs id="a12972853" type="software">Prokka</rs> (v. <rs id="a12972854" type="version" corresp="#a12972853">1.12</rs>) <rs id="a12972855" type="bibr">[26]</rs> with the bacteria or archaea database based on their taxonomic classification [25]. An e-value threshold of 10 -6 was used for prediction of coding sequences (CDSs). The CDSs and contigs of each MAG were labeled with the MAG name and merged to create the CDS and MAG reference database for further metatranscriptomic analysis. The entire CDS set of the MAGs was further analyzed using <rs id="a12972856" type="software">EnrichM</rs> (v. <rs id="a12972857" type="version" corresp="#a12972856">0.2.0</rs>) ( <rs id="a12972858" type="url" corresp="#a12972856">https://github.com/geronimp/enrichM</rs>), and annotated with Kyoto Encyclopedia of Genes and Genome (KEGG) orthologous group ids (KO) for ensuing metabolic pathway analysis [27]. KOs involved in oxidation of propionate via the methylmalonyl-CoA pathway, butyrate via the beta-oxidation pathway, and acetate via the reversed Wood-Ljungdahl pathway [28], were selected based on a list of gene families (determined by KO) which are directly associated with the corresponding KEGG metabolisms (Fig. S1, Data S2).</p>
<p>The processed mRNA reads were mapped to the CDS reference database made from the constructed MAGs using the <rs id="a12972859" type="software">CLC Genomics workbench</rs> (identity of ≥98% and ≥80% of the read length). Datasets originating from Fredericia or Randers were studied separately (Table S4, Data S3). The transcriptomic data (reads mapped per CDS) were analyzed in <rs id="a12972860" type="software" subtype="environment">R</rs> v. <rs id="a12972861" type="version" corresp="#a12972860">3.5.0</rs> <rs id="a12972862" type="bibr">[29]</rs> using the <rs id="a12972863" type="software" subtype="component">DESeq2</rs> v. <rs id="a12972864" type="version" corresp="#a12972863">1.20.0</rs> differential gene expression workflow [30]. The fold change (FC) and p value were calculated for the expression of individual CDS after each SCFA stimulation relative to the baseline before stimuli. CDSs with FC ≥ 2 and p value &lt; 0.05 or FC ≤ 0.5 and p value &lt; 0.05 were described as "upregulated" or "downregulated" genes. Prior to analysis, CDSs with less than five reads mapped were removed due to a low signal-to-noise ratio. 61 gene families determined by KO, which were related to oxidation of propionate, butyrate, and acetate (Data S2), were selected as targets to further analyze the different SCFA-oxidation metabolisms in the microbiome. The plots and heatmaps were made using the <rs id="a12972865" type="software">ggplot
2</rs> package v. <rs id="a12972866" type="version" corresp="#a12972865">3.1.0</rs> <rs id="a12972867" type="bibr">[31]</rs>.</p>
<p>Bacterial community composition was investigated based on previous data derived from amplicon sequencing the V1-3 variable regions of the bacterial 16S rRNA genes [8]. Raw sequences were processed with <rs id="a12972868" type="software">usearch</rs> v. <rs id="a12972869" type="version" corresp="#a12972868">10.0.240</rs> <rs id="a12972870" type="bibr">[32]</rs> in order to generate amplicon sequence variants (ASVs) and an ASV table. To evaluate the relative read abundance of the three novel syntrophs described in this study, nearly full-length 16S rRNA gene sequences associated with the MAGs were extracted and used as reference. ASVs were mapped to the full-length 16S rRNA sequences using <rs id="a12972871" type="software">usearch_global</rs> with a minimum identity of 94.5%, and annotated based on their percentage identity to the reference sequences with the thresholds for genera (94.5%) and species (98.7%) proposed by Yarza et al. [33]. Amplicon data were further analyzed using <rs id="a12972872" type="software">Ampvis2</rs> <rs id="a12972873" type="bibr">[34]</rs>. Both novel and previously described syntrophic bacterial genera [3,35] were specifically analyzed. For details see Supplementary Methods.</p>
<p>Three bacterial MAGs demonstrating strong positive responses to SCFA stimuli were selected for further genome curation and annotation. Genome annotation was performed in the ' <rs id="a12972874" type="software">MicroScope</rs>' annotation pipeline (v. <rs id="a12972875" type="version" corresp="#a12972874">3.12.0</rs>) <rs id="a12972876" type="bibr">[36]</rs>. Automatic annotations were validated and curated manually for the genes involved in metabolic pathways of interest with the assistance of the integrated MicroCyc [37] and KEGG [27] databases. The hydrogenases identified were further checked and classified using the <rs id="a12972877" type="software">HydDB</rs> tool <rs id="a12972878" type="bibr">[38]</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f201563981"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:18+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The purpose of this study is to examine the influences of self-identity, subjective norm and attitude on the intention to purchase luxury fashion goods. It also demonstrates how purchase intention influences on customers' intention to pay premium prices for luxury fashion goods. A theoretical model is established and assessed in this study on a sample of luxury fashion consumers in Malaysia. By means of a self-structured questionnaire, 240 valid submissions are included for data analysis with the <rs id="a12951900" type="software">Smart-PLS</rs> software <rs id="a12951901" type="version" corresp="#a12951900">3.2.0</rs>. The results support all hypotheses on the correlation of self-identity, subjective norm and attitude with purchase intention and price premium, except the correlation between self-identity and purchase intention. This is the first research paper investigating self-identity as an antecedent of purchase intention, and price premium as a consequence of purchase intention. It also justifies the behavior of luxury fashion consumers with regards to luxury fashion products. Findings from this study may help marketers develop effective strategies which would yield them a competitive advantage within the luxury fashion market.</p>
<p>This paper assessed common method variance before it proceeded to PLS-SEM analysis. According to Spector (2006), CMV may accrue in quantitative research as well as when data collection is from one source (MacKenzie &amp; Podsakoff, 2012). This research follows Harman's (1976) one-factor test and Bagozzi's method, (Bagozzi, Yi, &amp; Phillips, 1991) to identify the potential threat of CMV. The <rs id="a12951902" type="software">SPSS</rs> software is used to apply Harman's one-factor test and results indicate the absence of a CMV issue. According to Bagozzi's method, CMV is an issue when any two variables have a correlation higher than 0.9 (Bagozzi et al., 1991). Table 3 determines the highest correlation as 0.780 that is the correlation between attitude and purchase intention. Therefore, no CMV issue is present in the data used.</p>
<p>Although the SEM technique includes several statistical methods that enable researchers to improve and verify models and theories (Anderson &amp; Gerbing, 1982), a preferable technique is to test hypotheses and evaluate parameters (Fornell &amp; Larcker, 1981;Hair et al., 2011). In SEM and according to, VB-SEM and convenience-based SEM (CB-SEM) rely on a theory. Reinartz et al. (2009) argued that CB-SEM is particular with errors and sets of model parameters whereas PLS-SEM delegates scholars to question the causal relationship between latent constructs and the causal relationship between items (Rezaei, 2015). To add on, when the objective is to expand upon the extant structural theory, PLS-SEM is preferable over CB-SEM (Hair et al., 2011). Furthermore, Henseler et al. (2014) and Hair et al. (2011) suggest PLS-SEM as a preferred technique when the central purpose is accurate prediction in a complex model with many indicators and constructs. Therefore, this study considers PLS-SEM method as the most suitable approach for analysis based on the aforesaid justifications, as well as because of the complexity of the proposed model as it includes 7 paths and two dependent variables (Ringle et al., 2012). PLS-SEM analysis includes two processes which are to assess the inner model (measurement model) and to assess the outer model (structural model) (Henseler, 2010). The measurement model evaluates the relationship between indicators and validates constructs, whereas the structural model assesses the relationship between constructs (hypothesis testing) (Anderson &amp; Gerbing, 1982). In our present study, PLS-SEM analysis is evaluated by <rs id="a12951903" type="software">SmartPLS</rs> software.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f227721568"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T10:01+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Based on list of known public managers, we used the snowballing technique (Myers &amp; Newman, 2007) to recruit new respondents. We first made contact with them by telephone, informing them about the study goals; shortly thereafter, we sent them a personalized link to the software <rs id="a12966379" type="software">Q-software</rs> with specific instructions on how to do the sorting exercise online.</p>
<p>We used <rs id="a12966380" type="software">STATA</rs> software V<rs id="a12966381" type="version" corresp="#a12966380">14.2</rs> to perform the recommended analysis, extracting a total of nine factors that accounted for 69% of the total variance. In line with (McKeown &amp; Thomas, 1988), we opted for a nine-factor solution owing to theoretical and practical considerations instead of choosing factors purely based on the eigenvalue criterion (i.e. eigenvalue ≥1.00) as suggested by (Watts &amp; Stenner, 2012). All retained factors were significant at the p &lt; .01 level, complying with the guidelines defined by (Brown, 1993) and (McKeown &amp; Thomas, 1988). 1 Factor loadings, normalized weighted average statement scores (Zscores) as well as the statements most agreed and disagreed with per group, are shown in the Appendix.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f588865624"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:48+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>In the objective function presented in (1), γ is a parameter that determines the type of shrinkage applied during the estimation process, while τ is a penalty parameter that manages the degree of shrinkage. The response variable is represented by Y i ∈ R, and the values of the covariate vector are denoted as x i ∈ R p , for i ∈ {1, . . . , n}. The regression intercept is α o , and the regression coefficients associated with the covariate vector are in the form of α = (α 1 , . . . , α p ), where each α j is an element of the vector α, for j ∈ {1, . . . , p}. Note that p and n, respectively, represent the number of covariates and the sample size of the dataset. We can assume a linear regression structure represented by E(Y|X = x) = α o + x α, where "E" signifies the expected value of Y conditional to X = x. The <rs id="a12894852" type="software" subtype="component" corresp="#a12894853">glmnet</rs> package of the <rs id="a12894853" type="software" subtype="environment">R</rs> software provides various regression methods for variable selection and prediction, especially when n &lt;&lt; p. If γ = 1, the package defaults to LASSO with l 1 penalty, enabling both parameter shrinkage and variable selection. If γ = 0, ridge regression with L2 penalty is implemented. For optimal use of the elastic net, a value of 0 &lt; γ &lt; 1 is considered. The value of γ selectively shrinks certain coefficients to zero, enabling sparse selection. The model structure requires variable selection to form a subset of covariates. The elastic net approach is designed to handle the p &gt;&gt; n problem, indicating that the number of parameters significantly exceeds the sample size used in modeling. It is particularly effective when groups of highly correlated covariates are formed. The model combines variable selection with its structure to enhance accuracy. If a group of covariates is highly correlated and one variable is chosen for the sample, the entire group is automatically included in the model.</p>
<p>where L is the likelihood function of the data under the model, n is the sample size, and k is the total number of parameters in the model, with k = p + q + d + 1 if the intercept term is non-zero, and k = p + q + d if the intercept term is zero. Automatic ARIMA modeling is an approach that leverages statistical methodologies and computational capabilities to automatically determine the parameters (p, d, q) of an ARIMA model. A method used for automatic ARIMA modeling is the <rs id="a12894863" type="software" subtype="component" corresp="#a12894864">auto.arima()</rs> function implemented in the <rs id="a12894864" type="software" subtype="component" corresp="#a12894865">forecast</rs> package of the <rs id="a12894865" type="software" subtype="environment">R</rs>
software <rs id="a12953691" type="bibr">[61]</rs>, developed in [62]. This function performs a unit root test to decide on the order of differencing (d), and then selects the best (p, q) order based on the AIC. This is performed through a stepwise search or, optionally, through a more exhaustive search. The <rs id="a12894867" type="software">auto.arima()</rs> function is summarized in Algorithm 2, which offers an efficient and effective way of selecting the most suitable ARIMA model for a given time series dataset. Starting from an initial ARIMA model with (p, d, q) = (0, d, 0), the function automatically adjusts the values of p and q using a stepwise search, incrementally increasing or decreasing these parameters based on whether this improves the AIC score. This automated process effectively identifies the appropriate lag values for the autoregressive and moving average parts of the model, traditionally carried out through manual inspection of ACF and PACF plots.</p>
<p>Additionally, we evaluate the accuracy of the predictions and discuss any discrepancies or limitations encountered during the analysis. The <rs id="a12894868" type="software">auto.arima()</rs> function was used to automatically select the best parameters p, d, and q for the ARIMA model based on the training data. The result was that the ARIMA(2,2,1) model provided the best fit for the training data, meaning it minimized the forecast error better than other models with different parameters. Thus, the prediction for the observed value of the time series is based on two past values, where two levels of differencing were required to make the series stationary, and the observed error (residual) is based on a past error equal to one. The residuals from this model are depicted in Figure 2. It is important to note that nearly all lags from the ACF fall within the confidence interval. Moreover, the histogram for the residuals shows a likeness to a normal distribution.</p>
<p>We conducted a validation of the ARIMA(2,2,1) model by comparing it with eight other selected ARIMA models. The criteria used for this comparative analysis included AIC, AICc, BIC, RMSE, MAE, and MAPE. The results of this comparison are presented in Table 2. It is noteworthy that the ARIMA(2,2,1) model, despite being selected by the <rs id="a12894869" type="software">auto.arima()</rs> function, does not possess the minimum error values across all measures. Specifically, it ranks third lowest in terms of RMSE, it has the minimum MAE, and its MAPE is the third highest. However, this model is preferred because it has the lowest values for AIC, AICc, and BIC. These information criteria consider both the model's goodness of fit and complexity. Therefore, we conclude that the ARIMA(2,2,1) model provides the best balance of fit and simplicity for the data. Now, we present a comparison between our forecasts using the ARIMA(2,2,1) model and the real data observed during the six study periods outlined previously.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f81599433"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T14:20+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>As big data analytics permeates different sectors, the tools and frameworks that are needed to address domain-specific challenges will emerge. For example, modern largescale manufacturing facilities utilise sophisticated sensors and networks to record numerous measurements in the factory, such as energy consumption, environmental impact and production yield. Given the existence of such data repositories, these facilities should be in a position to leverage big data analytics. However, a number of domain-specific challenges exist, including diverse communication standards, proprietary information and automation systems, heterogeneous data structures and interfaces, as well as inflexible governance policies regarding big data and cloud integration. These challenges coupled with the lack of inherent support for industrial devices, makes it difficult for mainstream big data tools and methods (e.g. <rs id="a12899993" type="publisher" corresp="#a12899994">Apache</rs> <rs id="a12899994" type="software">Hadoop</rs>, <rs id="a12899995" type="software">Spark</rs>, etc.) to be directly applied to large-scale manufacturing facilities. Although some of the aforementioned challenges are addressed by different commercial tools, their scope is typically limited to data (e.g. energy and environmental) that is needed to feed a particular application, rather than facilitate open access to data from across the factory. To address these constraints, as well as many more, a new interdisciplinary field known as smart manufacturing has emerged. In simple terms, smart manufacturing can be considered the pursuit of data-driven manufacturing, where real-time data from sensors in the factory can be analysed to inform decision-making. More generally, smart manufacturing can be considered a specialisation of big data, whereby big data technologies and methods are extended to meet the needs of manufacturing. Other prominent technology themes in smart manufacturing include machine learning, simulation, internet of things (IoT) and cyber physical systems (CPS).</p>
<p>Pervasive real-time networks are a key aspect of smart manufacturing. However, current network topologies in industry can be complicated by the existence of multiple Virtual Private Networks (VPN) and Local Area Networks (LAN). These configurations exist to secure operations across departments and processes, but they can also restrict data access for consuming applications (e.g. equipment maintenance analytics <rs id="a12970655" type="software" subtype="implicit">software</rs>). Data accessibility can be further exacerbated where resources are under the control of external vendors. While these measures make sense from a security or management perspective, they represent a challenge to the adoption of smart manufacturing. Therefore, legacy technology configurations must be overcome to facilitate communication across networks without being encumbered by rigid governance policies and procedures. However, this requirement must be considered on a site-by-site basis to ensure security protocols are not violated.</p>
<p>Purpose The processing components are responsible for transforming time-series data to a form that is useful for analysis. The data processing components in the pipeline aim to remove the onus on ad hoc processing and aggregation routines on raw data. The basic processing illustrated for time-series data is the transformation of high residual data to different levels of granularity, such as hourly, daily, monthly and annual averages. More sophisticated data processing may include the execution of expert rules to identify early fault signals, or encoding of time-series data in a semantic format (e.g. Project <rs id="a12899994A" type="software">Haystack</rs>) to support interoperability with a particular application. Each processing component in the architecture is responsible for a single use case, such as those previously mentioned. Therefore, new requirements that cannot be met by existing components can be facilitated through the creation and deployment of a new component.</p>
<p>Figure 5 illustrates how industrial analytics applications consume data from the pipeline. The processed time-series data resides in a directory structure that gives context to the data being accessed (i.e. returntemp and settemp for AHU1). There are two applications in the simulation. The <rs id="a12899997" type="software" subtype="implicit">dashboard</rs> is a business intelligence application that implements basic descriptive analytics. It uses the data pipeline to access precompiled aggregates of time-series data to eliminate the overhead of running this routine dynamically. The simulation shows that the <rs id="a12899998" type="software" subtype="implicit">dashboard</rs> accesses the hourly, daily and monthly data for both measurements. The other data consumer is a <rs id="a12899999" type="software" subtype="implicit">predictive maintenance model</rs> that is used to identify issues in AHU's. The <rs id="a12900000" type="software" subtype="implicit">predictive model</rs> requests 15-min data for both measurements given its need for granular data. Both applications in the simulation were able to access data using a common interface without having to engage low-level industrial protocols.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f544396629"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:48+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The qualitative data collected from the interviews were transcribed, organized and analysed with the assistance of the qualitative research software package, <rs id="a12972664" type="software">QSR NVivo</rs> <rs id="a12972665" type="version" corresp="#a12972664">9</rs>. Initially, descriptive coding was performed to help with the data identification. Accordingly, student participants were coded on the basis of their demographic information such as gender (M/F), year group (Y1, Y2, Y3), or academic major (E for English major and NE for Non-English major). Likewise, information about gender (F/M) and the interview order (1, 2, 3 and so on) was used to code the data of the teacher participants.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f541098785"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:46+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Calculations were performed using the commercial software <rs id="a12893775" type="software">ANSYS Fluent</rs>. The main difficulty in solving the proposed mathematical model is that the discrete phase model integrated in the software does not support real-time updateable injection of homogeneous droplets at the nucleation site, and there is no accurate heat and mass transfer model. That is, relying on <rs id="a12893776" type="software">ANSYS Fluent</rs> alone cannot simulate the homogeneous droplets of the Euler-Lagrange-Euler method, and even cannot accurately simulate the growth and evaporation of heterogeneous droplets, let alone interaction of homogeneous and heterogeneous droplet. User-defined functions are therefore introduced, which include the exchange of heat and mass source terms between gas, droplet, and liquid films, droplet nucleation and growth models, and recording and updating of homogeneous and heterogeneous condensation nuclei information.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f82390785"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:17+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The correlations between geographic distance and cultural dissimilarity have been obtained using partial Mantel tests (Smouse, Long, &amp; Sokal, 1986), as implemented in the <rs id="a12952076" type="software" subtype="component" corresp="#a12952078">vegan</rs> <rs id="a12952084" type="bibr">(Oksanen et al., 2013)</rs> and <rs id="a12952077" type="software" subtype="component" corresp="#a12952078">ecodist</rs> <rs id="a12952085" type="bibr">(Goslee &amp; Urban, 2007)</rs> packages in <rs id="a12952078" type="software" subtype="environment">R</rs> ( <rs id="a12952079" type="publisher" corresp="#a12952078">R Core Team</rs>, 2013). Mantel tests (Mantel, 1967) are commonly used to measure the correlation between two distance matrices, where standard regression analysis cannot be used to compute the significance given that distances are not independent (i.e. removing a site would alter the entire matrix). The solution consists of comparing the observed correlation against a distribution of correlations obtained by randomly permuting the rows and the columns of one of the matrices.</p>
<p>statistics with a permutation test (1,000 iterations) using the <rs id="a12952080" type="software" subtype="component" corresp="#a12952081">pegas</rs> package in <rs id="a12952081" type="software" subtype="environment">R</rs> <rs id="a12952086" type="bibr">(Paradis, 2010)</rs>.</p>
<p>In order to visualise the relationship between the degree of population structure defined by each material culture type, geography, and temporal distance, we employed DISTATIS analysis (Abdi, Valentin, O'Toole, &amp; Edelman, 2005), using the <rs id="a12952082" type="software" subtype="component" corresp="#a12952083">DistatisR</rs> package <rs id="a12952087" type="bibr">(Beaton, Fatt, &amp; Abdi, 2013)</rs> in <rs id="a12952083" type="software" subtype="environment">R</rs>. This is a variant of multidimensional scaling that allows the simultaneous assessment of multiple distance matrices through the creation of a compromise matrix that represent the best aggregate between the original matrices. The observations are projected on a compromise space, together with vectors representing each source distance matrix. DISTATIS thus enables visual depiction of the differences and similarities between the different distance matrices for each culture, providing information additional to the partial mantel tests.</p>
</text>
</tei>
  <tei>
    <teiHeader>
        <fileDesc id="f159779413"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T16:22+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text lang="en">
        <p>Secondary colony forming efficiency assay (CFE). BMSCs obtained after lineage depletion of aBM and PCs obtained after explant removal were directly plated at a concentration of 400 or 2000 cells/cm 2 in growth media consisting of MEMα supplemented with 10% FBS, 1% penicillin-streptomycin (Life Technology, Carlsbad, California), and 10ng/ml bFGF (R&amp;D, Minneapolis, MN) for 14 days. The medium was changed every 3 days. Clones were fixed for one hour in 70% ethanol, stained with Giemsa stain (Fluka), and counted under microscope. CFE was reported in 
            <rs type="software" id="s1">GraphPad Prism</rs> v
            <rs type="version" corresp="s1">6.0a</rs>.
        </p>
        <p>Cell-growth assay. To assess cell growth in vitro, 1. dishes and cultured in growth medium. The cells were trypsinized for counting the first two days and then every two days during twelve days. Cell count was reported in 
            <rs type="software" id="s2">GraphPad Prism</rs> v
            <rs type="version" corresp="s2">6.0a</rs>.
        </p>
        <p>In vitro osteogenic and adipogenic and chondrogenic differentiations. For each differentiation protocol, BMSCs were used following lineage depletion and PCs following explant removal without further passage. For osteogenic differentiation, the cells were plated at confluence in osteogenic medium containing MEMα with 10% FBS supplemented with 0.1 μM dexamethasone, 0.2 mM L-ascorbic acid, and 10 mM glycerol 2-phosphate disodium salt hydrate (Sigma, St. Louis, MO). The medium was changed every three days during 2-5 weeks, and the cells were stained with 0.2% alizarin red S (Sigma, St. Louis, MO). For adipogenic differentiation, the confluent cells were cultured with adipogenic medium containing MEMα with 10% FBS supplemented with 10 μg/ml insulin, 100 μM indomethacin, 0.5 mM 3-isobutyl-1-methylxanthine, and 0.1 μM dexamethasone (Sigma, St. Louis, MO). The medium was changed every 3 days during 3 weeks and the cells were stained with Oil Red O solution (Sigma, St. Louis, MO). Nuclei were counterstained with Harris hematoxylin (DiaPath, Martinengo, Italy). Pictures of lipid droplets were taken under light microscopy using Leica DM IRB light microscope and 
            <rs type="software" id="s3">LAS</rs>
            <rs type="version" corresp="s3">v4.3</rs> software (
            <rs type="creator" corresp="s3">Leica Microsystems Inc</rs>, Buffalo Grove, IL). For chondrogenic differentiation, the cells were resuspended at a concentration of 5.10 5 cells in 200 μl of growth media and plated as micromass. After 2 h at 37 °C, the cells were covered with chondrogenic medium containing DMEM with 10% FBS supplemented with 0.1 μM dexamethasone, 100 μg/ml sodium pyruvate, 40 μg/ml L-proline, 50 μg/ml L-ascorbic acid, 50 mg/ml ITS, and 10 ng/ml TGFβ1. The medium was changed every 3 days during 1-2 weeks and the cells were stained with Alcian blue (Sigma, St. Louis, MO).
        </p>
        <p>Wound healing assay. Wound healing assay was performed to assess migration capacity of cells in vitro. Forty-eight hours before the assay, BMSCs obtained after lineage depletion and PCs after explant removal were directly plated in culture inserts in μ-slide 8 well ibiTreat (Biovalley) and cultured in growth media. Before starting the assay, culture inserts were removed allowing a clear separation between two migration fronts (wound). A volume of 10 μM of Cytosine β-Darabinofuranoside hydrochloride (Sigma, St. Louis, MO) was added in the medium to inhibit cell mitosis. Wound healing was recorded every 10 minutes over 50 to 72 h using videomicroscopy (Nikon Eclipse Ti-E). Data were analyzed with 
            <rs type="software" id="s4">ICY software</rs> (
            <rs type="url" corresp="s4">bioimageanalysis.org</rs>) and reported in
            <rs type="software" id="s5">GraphPad Prism</rs> v
            <rs type="version" corresp="s5">6.0a</rs>.
        </p>
        <p>Histomorphometry and cell-lineage analyses. The mice were killed at specified time points post fracture. Tibias were fixed in 4% paraformaldehyde, decalcified in 19% EDTA, and processed for histomorphometric analyses of callus, cartilage, and bone on Safranin-O (SO) and Trichrome (TC) stained sections 62,64 . Picrosirius staining was performed on adjacent sections to visualize bone and fibrous tissue. For quantitative analyses of GFP-transplanted cells in fracture calluses, GFP signal was analyzed on sections adjacent to Safranin-O and Trichrome using a Zeiss Imager D1 AX10 light microscope and 
            <rs type="software" id="s6">ZEN</rs> software (
            <rs type="creator" corresp="s6">Carl Zeiss Microscopy GmbH</rs>, Gottinger, Germany).
        </p>
        <p>Microarray analyses. BMSCs and PCs were isolated and purified from uninjured tibia (d0) (n = 4) and tibia 3 days post fracture (n = 4). The cells were harvested and total RNA was extracted using Rneasy Plus mini Kit (Qiagen). RNA quality was assessed using Agilent Model 2100 Bioanalyzer (Agilent Technologies). Gene expression analyses were performed using GeneChip Mouse 430 2.0 Array (Affymetrix). Fluorescence data were imported into 
            <rs type="software">Affymetrix Expression Console</rs> and
            <rs type="software" subtype="environment">R</rs>
            <rs type="software">Bioconductor</rs> analysis software. Data were normalized with RMA method, groups were compared by Student's t-test and the results were filtered at p-value ≤5% and fold change ≥1.2. Hierarchical clustering was performed using 
            <rs type="software">Multi-Experiment Viewer software (MeV)</rs> 65,66 . Gene Set Enrichment Analysis (GSEA) analysis was performed using all normalized probes on "curated gene set" and "GO gene set" collections of the Molecular Signatures Database v5.2 according to 67,68 . Postn-linked gene list was built using STRING database 69 (http://string-db.org/cgi/ input.pl?UserId=5bskVvnWAJdi&amp;sessionId=hMz9XrOQGQ4P&amp;input_page_ show_search=on). We used the following parameters: active interaction sources: all checked; minimum required interaction score: 0.4; maximum number of interactors to show: 1 st shell: 100; and 2 nd shell: 20. We obtained a list of 93 genes (Supplementary Table 3).
        </p>
        <p>Statistical analyses. Statistical significance was determined with two-sided Mann-Whitney test and reported in 
            <rs type="software" id="s7">GraphPad Prism</rs> v
            <rs type="version" corresp="s7">6.0a</rs>. P-values were determined as follows: * ,$ p ≤ 0.05; ** ,$$ p &lt; 0.001; ***, $$$ p &lt; 0.0005. All samples were included except for fractures that were proximal and/or distal or comminuted fractures. All analyses were performed using a blind numbering system.
        </p>
        </text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f212866466"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:53+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Patient follow-up, quality-of-life metrics, and statistical analysis Pre-treatment collection of patient data included patient demographics, baseline Response Evaluation Criteria in Solid Tumors (RECIST) criteria target tumor measurements of the treated lesion, and baseline European Organization for Research and Treatment of Cancer Core Quality of Life Questionnaire (EORTC QLQ-C30) Version 3.0 quality of life (QOL) scores. Patients had planned followup at 6, 12, and 26 weeks post-treatment. Study physicians prospectively assessed treatment response of treated-lesions using RECIST at pre-planned time points of three and six months posttreatment. Disease-free, progression-free, and overall survival were prospectively assessed at 12 and 26 weeks post-treatment and subsequently through chart review and routine clinical appointments. Acute (defined as within six months of therapy completion) Common Terminology Criteria for Adverse Events (CTCAE) v4.0 gastrointestinal toxicity was prospectively assessed at 6, 12, and 26 weeks by clinical research coordinators (CRCs) through the Washington University clinical trials office. Subsequent late toxicities occurring after the six month study period were rigorously assessed by the treating physicians through routine clinical care and retrospective chart review. CRCs also prospectively assessed patient-reported QOL scores at zero, six, and 26 weeks post-treatment. Repeated measures analysis was used to assess for change in QOL scores. Kaplan Meier analysis was used to estimate local progression-free survival. Statistical analyses were performed by the study statistician using <rs id="a12970084" type="software">SAS</rs>, Version <rs id="a12970085" type="version" corresp="#a12970084">9.2</rs> ( <rs id="a12970086" type="publisher" corresp="#a12970084">SAS Institute, Inc.</rs>, Cary, NC).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f343672724"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:18+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The great diversity of choices creates a duality in ENMs: while models are simple to fit and the required data is easily available, several decisions should be made regarding methodological steps that must be done judiciously and are not as readily available as the data. As a result, studies that rely on ENMs usually do not have the same methodological rigor as studies that focus on developing ENMs, i.e., several studies still apply (Area Under the Curve) AUC as an evaluation metric, even though it has been demonstrated for over 10 years that the metric is deeply affected by prevalence (Lobo et al., 2008) or the extent of the accessible area (Peterson et al., 2008;Barve et al., 2011). On the other side, there has been a great effort to develop alternatives for the AUC and several other methodological aspects, which have been implemented in several <rs id="a12972707" type="software" subtype="environment">R</rs> <rs id="a12972708" type="software" subtype="implicit" corresp="#a12972707">packages</rs> and
ENMs <rs id="a12972752" type="software" subtype="implicit">software</rs> <rs id="a12972709" type="bibr">(Thuiller et al., 2009;Guo &amp; Liu, 2010;Naimi &amp; Araújo, 2016;Hijmans et al., 2017;Golding et al., 2018;Kass et al., 2018;Sánchez-Tapia et al., 2018;Cobos et al., 2019)</rs>.</p>
<p>Ideally, ENMs should be fit-for-purpose, which means that fitting ENMs is a process that must be thought carefully, as there is not a single correct way to fit models (Guillera-Arroita et al., 2015;Qiao et al., 2015). Due to the great variety of methodological choices and the velocity that new alternatives arise, it may be hard to keep up with novelties within the ENMs' field. As a result, people who are not involved in the methodological developments within the field or do not have connections to developers have small participation in all the published papers (Ahmed et al., 2015). We introduce here <rs id="a12972710" type="software" subtype="component" corresp="#a12972711">ENMTML</rs>, a new <rs id="a12972711" type="software" subtype="environment">R</rs> package to fit ENMs. The main objective of this package is to put together all this methodological diversity developed within the ENM field and present it to users simply and transparently. Despite being an
R package, we also made it friendly for non-programmers and summarized the whole fitting process into a single function with several arguments that correspond to the methodological alternatives.</p>
<p>The <rs id="a12972712" type="software">ENMTML</rs> package and its processes can be divided into three major stages: preprocessing, processing, and post-processing. This division in three stages is familiar to most ENMs routines. Identifying the stage in which each methodological step will be performed may help users to understand the connections among the different methodological steps and provides an overview that assists the decision-making process (Figure 1).</p>
<p>Finally, users might opt to reduce autocorrelation in occurrence data and possible sampling bias by a thinning technique (argument thin_occ), performed using the package <rs id="a12972713" type="software">spThin</rs> <rs id="a12972714" type="bibr">(Aiello-Lammens et al., 2015)</rs>. There are three alternatives for defining the thinning distance: i) based on the distance of a Moran's I Variogram that minimizes the spatial autocorrelation;</p>
<p>A crucial decision at the moment to construct ENMs is the hypothesized accessible area, i.e., the geographical region used by a species throughout a relevant period of time (Barve et al., 2011), also known as the movement component of the BAM diagram (Soberon &amp; Peterson, 2005). Such an accessible area can be delimited based on the knowledge of species ecology, dispersal ability, geographical barriers, and ancient region were species inhabited (Soberón, 2010;Peterson et al., 2011). Nonetheless, this information is often missing for most species; therefore, different techniques act as an approximation of the accessible area. <rs id="a12972715" type="software">ENMTML</rs> account with four option to define accessible areas: i) no restriction, i.e., the entire predictors extent will be used as accessible area; ii) define an accessible area based on a buffer around occurrence data; iii) define the accessible area based on a mask, e.g., using a shapefile for biogeographical ecoregions, or; iv) accessible are defined by the user (supported formats: SHP/TIF/BIL/ASC/TXT; Table 4).</p>
<p>As one of the primary sources of ENMs/SDMs uncertainty is the method used to construct them (Watling et al., 2015;Thuiller et al., 2019), and assuming that no single methods can lead with all modeling situation (Qiao et al., 2015), our <rs id="a12972716" type="software">ENMTML</rs> package fit 13 algorithms that range different statistical techniques and type of data used to fit the models (Table 5).</p>
<p>The major source of model uncertainty is caused by the different algorithms used to fit ENMs (Diniz-Filho et al., 2009;Thuiller et al., 2019). A commonly used method to deal with this is to create an ensemble model of different algorithms (Araújo &amp; New, 2007;Marmion et al., 2009). <rs id="a12972717" type="software">ENMTML</rs> offers six ensemble methods, three based on different ways to calculate models` average and three based on PCA derived from the models. Average-based ensembles can be created using: i) a simple average of all models, ii) weighted average, in which models` suitability is weighted by how well that algorithm performed and iii) superior average, in which a simple average is calculated only for those algorithms that performed better than the average of all algorithms. PCA-based ensemble performs a principal components analysis on suitability maps and uses the first component as the final map, this can be performed: i) using all models, ii) using only the superior models, selected similarly to the superior average, and iii) principal components are calculated using only suitability values above the threshold for each algorithm, values below the threshold are set to zero (Table 9).</p>
<p>The <rs id="a12972718" type="software">ENMTML</rs> package has the option to fit models using parallel processing, which accelerates the process. However, as this is computation-intensive, we chose to leave it open for users to decide the number of computer cores allocated for fitting ENMs. If the users do not specify the number of cores, only a single core will be used.</p>
<p>There are several possible outputs for a single run of the <rs id="a12972719" type="software">ENMTML</rs> package. All the outputs produced by the fitting process are within a Result folder, which is created at the same level as the Predictors folders (Figure 2). Within the Result folder, there is a sub-folder named</p>
<p>We used the <rs id="a12972720" type="software">ENMTML</rs> package to fit current and future distribution for five virtual species.</p>
<p>All these procedures are expressed in <rs id="a12972724" type="software">R</rs> command line below:</p>
<p>We present the release of the <rs id="a12972725" type="software">ENMTML</rs> package, but we already have in mind ideas for future implementations. As the main objective of the package is to approach complex methodological developments to people that rely on ENMs but do not focus the development of new methods and are not comfortable using <rs id="a12972726" type="software">R</rs>, in the next update we expect to launch a web platform using <rs id="a12972727" type="software">Shiny</rs>. On the other hand, we also believe that <rs id="a12972728" type="software">ENMTML</rs> package might be of great use for the whole ENMs' community, as it centers on methodological developments scattered around the literature, and not always implemented in <rs id="a12972729" type="software">R</rs>, in one single location. With that in mind, we also look forward to providing further options for people who are interested in the fine-tuning of models. One of the first additions already planned is the possibility for users to change algorithms parameters. In addition, we also plan to explore indepth the ensemble field and include more ensemble alternatives and uncertainty maps.</p>
<p>There are several <rs id="a12972730" type="software" subtype="environment">R</rs> <rs id="a12972753" type="software" subtype="implicit" corresp="#a12972730">packages</rs> to fit ENMs. We performed a literature search and found seven alternatives: <rs id="a12972731" type="software">biomod</rs> <rs id="a12972732" type="bibr">(Thuiller et al., 2009)</rs>, <rs id="a12972733" type="software">ModEco</rs> <rs id="a12972734" type="bibr">(Guo &amp; Liu, 2010)</rs>, <rs id="a12972735" type="software">sdm</rs> <rs id="a12972736" type="bibr">(Naimi &amp; Araújo, 2016)</rs>, <rs id="a12972737" type="software">Model-R</rs> <rs id="a12972738" type="bibr">(Sánchez-Tapia et al., 2018)</rs>, <rs id="a12972739" type="software">Wallace</rs> <rs id="a12972740" type="bibr">(Kass et al., 2018)</rs>, <rs id="a12972741" type="software">ZOON</rs> <rs id="a12972742" type="bibr">(Golding et al., 2018)</rs>, and <rs id="a12972743" type="software">kuenm</rs> <rs id="a12972744" type="bibr">(Cobos et al., 2019)</rs>. We summarize those packages in a table, highlighting each package features and contrasting them with the features available at <rs id="a12972745" type="software">ENMTML</rs> (Table 10). Most packages focus on the development of a specific aspect of the modeling process, e.g., the package <rs id="a12972746" type="software">biomod</rs> was proposed as a platform for creating ensemble models, while the package <rs id="a12972747" type="software">kuenm</rs> is heavily focused towards accurately developing Maxent models; therefore a crucial aspect of software/package selection lies on the study objective.</p>
<p>We introduce the package <rs id="a12972748" type="software">ENMTML</rs>, which proposes to integrate complex methodological developments in the ENMs' field, published from several different sources, in a single package and make them visible for users, which are not accustomed to the methodological details of ENMs. Our secondary objective was to make the package user-friendly, even for people not comfortable with the programming environment; therefore, we summarized the whole process into one single function with arguments that must be filled by the user according to the study objectives. We covered the majority of the ENMs process, from preprocessing occurrences and predictors to post-processing suitability models into ensembles or MSDM and provided several methodological alternatives to the different modeling steps (Table 10).</p>
<p>• We present <rs id="a12972749" type="software" subtype="component" corresp="#a12972750">ENMTML</rs>, an open source <rs id="a12972750" type="software" subtype="environment">R</rs> package to fit ecological niche models (ENMs) • The package covers a wide variety of methodological aspects gathered from several studies • Complex methodological features, which were not readily available in
R, are now easily accessible to users • We condense all this complexity in a single function to make it easier for users to follow a workflow • We demonstrate an example of fitting models for four species with complex methodological choices and its interactions</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f489752071"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:00+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Figure 1 shows the four German sociology journals included in <rs id="a12900059" type="publisher" corresp="#a12900060">Thomson-Reuters</rs>' <rs id="a12900060" type="software">Web-of-Science (WoS)</rs> when mapped in terms of their "cited" patterns in relations to other sociology journals. The "citing" patterns of these same journals, however, are very different: on the "citing" side the journals are deeply integrated in sociology, which provides the knowledge base for their references. In other words, the identity of these journals is sociological, but their audience is the German-language realm, including journals in political science, education, psychology, etc. Thus, journals can show very different patterns for being cited or citing, and the same asymmetry (cited/citing) holds for document sets other than journals. For example, the oeuvre of an author or an institutionally delineated set of documents (e.g., a department) cannot be expected to match journal categories. One may be indebted ("citing") to literatures other than those to which one contributes (e.g., Leydesdorff and Probst, 2009).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f589959736"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T12:49+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>To provide an overview of the current state of academic literature, a SLR was conducted in the first semester of 2019, with a focus on publications in the field of Management, Business and other related areas such as Information Systems. In recent years, this research methodology became very popular within the field of innovation and entrepreneurship studies (e.g. Kivimaa, Boon, Hyysalo, &amp; Klerkx, 2019). A SLR "is a review of an existing body of literature that follows a transparent and reproducible methodology in searching, assessing its quality and synthesizing it, with a high level of objectivity" (Kraus, Breier, &amp; Dasí-Rodríguez, 2020, p. 4). The literature review reported in the previous section comprised different peer-reviewed and non-refereed articles, case reports, book chapters and consulting reports. Following a wellestablished approach used for studies of different types (e.g. Bouncken, Gast, Kraus, &amp; Bogers, 2015;Vallaster, Kraus, Lindahl, &amp; Nielsen, 2019), a SLR focused above a certain quality level of publications in peer-reviewed academic journals with specific criteria chosen to be conducted hereafter. A stepwise research approach has been undertaken to ascertain a broad perspective for an in-depth understanding of the context (Cook &amp; West, 2012) and linkages between DT and HC. The first step was the literature search through the <rs id="a12954450" type="software">EBSCO host</rs> databases 'Business Source Ultimate' and 'Business Source Complete' (see e.g., Mas-Tur, Kraus, Brandtner, Ewert, &amp; Kürsten, 2020). Since digitalization in HC, understood as the use of information technology for processing and managing data, information, and processes, started to become popular in the early 2000s (Agarwal et al., 2010), the 2000-2019 time frame was set. Non-refereed articles, conference papers and book chapters have been excluded from the search.</p>
<p>An initial search included the keyword combinations 'digital*' 1 AND 'healthcare' both in titles only to ensure that publications covering both core areas could be identified. This first search yielded 31 articles. With the intent to increase the breadth of the SLR, a further broader search should produce a more significant number of articles: Hence, even if many were less relevant to the present topic, the combination 'digital*' in titles AND 'healthcare' in abstracts was applied. The search was conducted according to the same search criteria as above, e.g. publication date, source type and language. This further search yielded 114 manuscripts. After excluding 15 multiple entries, the search sample for <rs id="a12954451" type="software">EBSCOhost</rs> consisted of 130 journal articles. Furthermore, to strengthen the inclusiveness of the sample of articles in this field, the databases <rs id="a12954453" type="publisher" corresp="#a12954452">Elsevier</rs> <rs id="a12954452" type="software">ScienceDirect</rs> and <rs id="a12954454" type="software">SpringerLink</rs> were also scanned according to the same keywords as mentioned above. The above search resulted in a total of 340 articles. Articles that did not fall under the conceptual criteria set earlier were then systematically excluded. In order to provide a quality threshold, we adopted the official German journal ranking "VHB-JOURQUAL 3" and considered only those articles that were released in journals ranked at a "C" level or higher ratings remained in the sample (Bouncken et al., 2015). We did not apply other quality criteria (e.g., impact factor) or journal rankings (e.g. ABS). After this check, 198 articles were excluded due to low or non-ranking statuses of the journals they were published in. The revised subsample then contained 142 high-quality articles.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f42947188"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T08:17+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Given the clustered nature of the data (students within school classes), we computed robust variance estimators for all logistic regression models. All analyses were performed using <rs id="a12901079" type="software">STATA</rs> version <rs id="a12901080" type="version" corresp="#a12901079">10</rs>. An alpha level of .05 (2-tailed) was utilized for statistical tests.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f566466868"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:39+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Model-based dimensioning was performed for coils design. Given the symmetry of the system, a single coil was modelled through Finite-Element analysis ( <rs id="a12970066" type="software">COMSOL Multiphysics</rs>® <rs id="a12970067" type="version" corresp="#a12970066">5.2</rs>) to derive magnetic field spatial distribution when varying coil driving current up to 3A. Higher currents run the risk of overheating the coils and damaging them. The magnetic field produced increased with an increase in the number of windings in the coil. Therefore, considering spatial constraints a stepped design was chosen to maximize the number of windings that can be accommodated in the coil (Figure 2). The simulation results in Figure 2B confirmed that the tapered cores were able to carry and concentrate the field along the core axis. It can also be seen that the field strength reduced drastically with distance from the core (Figure 2C), implying the need to have the cores close to the extrusion point. However, due to space constraints the only way to bring the cores closer to the extrusion point was by tapering them. Considering this fact, the tapered core extremities were placed 5 mm far from the extrusion point. Reducing the distance further might increase the magnitude of the field produced at the extrusion point, thus easing the orientation process. However, this also increases the magnitude of field gradients which tend to attract the extruded ink toward one of the coils. Therefore, this distance was chosen as a compromise between the two effects. Magnetic field strength increased linearly with the current in the coils and no saturation was observed with driving currents up to 3A (Figure 2D). Each coil was also tested for its response time, as a slow response time will introduce a lag between the produced field and the required field. A fast response time ensures that the particles are oriented in the right direction at the right printing coordinates. The experimental dynamic response of a single coil is shown in Figure 2E. A step input with a step of 1 A current was supplied to each coil and the resulting magnetic field was recorded. It can be observed that as the step current is given, the field also increased and stabilized to a higher value within a few milliseconds, which is a good response time when considering the extrusion and curing dynamics (Supporting Information) and the printing speed (25 mm min -1 ).</p>
<p>The Electromagnetic Coils: The three electromagnetic coils which were orthogonal to each other were placed around the extruder such that the core of each coil was 5 mm away from the extrusion point. There were 480 windings of 1 mm diameter insulated copper wire in total in each coil, which were arranged in a stepped manner as can be seen in Figure 1B. The core was tapered and made of a soft magnetic material (Vacoflux-50, Vacuumschmelze, Germany). An orientation field of 20 mT was used unless otherwise mentioned. The currents in the coils were driven using Servo drives (Elmo Gold Solo Twitter, Israel) which were in turn controlled using <rs id="a12970068" type="software">LabVIEW</rs> (V <rs id="a12970069" type="version" corresp="#a12970068">20.0f1</rs>, <rs id="a12970070" type="publisher" corresp="#a12970068">National Instruments</rs>, USA).</p>
<p>Measurement of Magnetization: The printed structures were characterized in a Vibrating Sample Magnetometer VSM (Microsense, EZ9, USA) to quantify their magnetization Printing: A Dual Extrusion 3D Bioprinter (Omega, 3Dynamic, UK) was used, albeit only one extruder was utilized which uses 17HS4417 motors (ACT MOTOR, China). The 3D printer was in turn controlled either with <rs id="a12970071" type="software">Repetier Host</rs> (V <rs id="a12970072" type="version" corresp="#a12970071">2.1.6</rs>, <rs id="a12970073" type="publisher" corresp="#a12970071">Hot-World GmbH &amp; Co. KG</rs>, Germany) or with <rs id="a12970074" type="software">LabVIEW</rs> (V <rs id="a12970075" type="version" corresp="#a12970074">20.0f1</rs>, <rs id="a12970076" type="publisher" corresp="#a12970074">National Instruments</rs>, USA). Commercially available 20 ml Leur Lock syringes (NORM-JECT®, HenkeSassWolf, Germany) and 1.2 mm nozzle (7018097, SmoothFlow tapered Tips, Nordson, USA) were used for extrusion. The printing speed was 25 mm min -1 which is the lowest speed achievable with this printer. Having a low speed is important to give the particles enough time to reorient themselves in the desired direction.</p>
</text>
</tei>
  <tei>
    <teiHeader>
        <fileDesc id="f82871221"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T16:17+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text lang="en">
        <p>The HTKS was used to assess children's behavioral selfregulation and requires cognitive flexibility, working memory, and inhibitory control (McClelland and Cameron, 2012). There are a total of 30 test items with scores of 0(incorrect), 1(selfcorrect), or 2(correct) for each item. A self-correct is defined as any motion to the incorrect response, but self-correcting and ending with the correct action. Scores range from 0 to 60 where higher scores indicate higher levels of behavioral self-regulation. The task takes approximately 5-7 min with strong inter-rater reliability (κ = 0.90; Cameron Ponitz et al., 2009;McClelland and Cameron, 2012). There are two parallel forms of the HTKS: A and B, which were given randomly in an alternating order of assessments over the four time points of the longitudinal study. Form A starts with head/toes and Form B starts with knees/shoulders. No significant differences have been found between the two versions of the task McClelland et al., 2007a;Cameron Ponitz et al., 2009;Wanless et al., 2011a;Bowles et al., submitted. The measure now incorporates three sections, the HTT (1 section of "opposites"), the HTKS (2 sections, two sets of "opposites") and the HTKS-Extended (3 sections, adding a final rule switch). The task is available in a number of languages, is reliable, and significantly predicts academic outcomes in diverse samples (McClelland et al., 2007a,b;Wanless et al., 2011a;McClelland and Cameron, 2012;von Suchodoletz et al., 2013). Validity information for the current sample is presented in the Results below. Cronbach's alphas were computed in 
            <rs type="software" id="s1">Mplus</rs>
            <rs type="version" corresp="s1">7</rs> using polychoric correlations, which are appropriate for categorical data. The HTKS in the current sample had Cronbach's alphas of 0.92, 0.94, 0.94, and 0.94 across the four waves of the study.
        </p>
        <p>To assess inter-rater reliability in the current study, a random subsample of children (n = 28) was videotaped while being administered the HTKS task. Videotapes were later viewed and scored by an assessor who had not administered the original HTKS task to the child. We used double-coded HTKS sum scores analyzed with the default weighted kappa option in 
            <rs type="software">Stata</rs> (i.e., 1.00, 0.50, 0.00). The correlation between the doublecoded HTKS scores was strong (r = 0.88, p &lt; 0.001). Results showed high inter-rater agreement (92.29%), with a weighted Cohen's kappa of 0.79 (p &lt; 0.001) indicating very strong interrater reliability for the HTKS task (<ref type="bibr">Landis and Koch, 1977</ref>). To measure test-retest stability of the HTKS task in the current sample, Pearson's correlation coefficients for fall and spring HTKS scores were examined in prekindergarten and kindergarten (see Table 2). The average length of time between fall and spring HTKS task assessments was 5.64 months in prekindergarten (SD = 0.57, range = 4.17-7.16) and 5.84 months in kindergarten (SD = 0.81, range = 3.38-7.46). Results showed good test-retest stability with strong positive correlations between fall and spring HTKS total scores in both prekindergarten (r = 0.60, p &lt; 0.001) and kindergarten (r = 0.74, p &lt; 0.001).
        </p>
        <p>All research questions were addressed using 
            <rs type="software" id="s2">Stata</rs>
            <rs type="version" corresp="s2">13.1</rs> (
            <rs type="creator" corresp="s2">StataCorp</rs>, 2013). For construct validity, we first analyzed correlations between the HTKS and the four EF measures (the Day-Night Stroop, the DCCS, Simon Says, and the Woodcock-Johnson Working Memory subtest) for each wave. Then, we looked at multilevel models predicting HTKS scores with the four EF measures at each wave, controlling for child age, parent education, gender, Head Start status, and English Language Learner status. The ICCs for the HTKS across the four waves of data were: 0.12, 0.22, 0.15, and 0.10.
        </p>
        <p>For predictive validity, we used multilevel models with generalized structural equation modeling in 
            <rs type="software" id="s3">Stata</rs>
            <rs type="version" corresp="s3">13.1</rs>, adjusting for the nested nature of the data (children within classrooms) and used a full information maximum likelihood estimator. For each random effects model, the models incorporated two waves of data, roughly 6 months apart during the same academic year (e.g., prekindergarten or kindergarten). In these models, the spring achievement variable was regressed on fall achievement, a single EF measure of interest, child age, parent education, gender, Head Start status, and English Language Learner status. The ICCs for the outcome achievement measures in the spring of prekindergarten (ICCs = 0.14-0.23) and kindergarten (ICCs = 0.22-0.27) suggested multilevel models were appropriate, and thus, all predictive models adjusted for this nesting.
        </p>
        <p>Fixed effects analyses were estimated in 
            <rs type="software" id="s4">Stata</rs>
            <rs type="version" corresp="s4">13.1</rs>, with standard errors adjusted for clustering. In the fixed effects analyses, all four waves of data were analyzed simultaneously, such that all available data for each child from fall of prekindergarten to spring of kindergarten was modeled. In fixed effects analyses, associations of intra-individual change on predictors (i.e., EF) and outcomes (i.e., achievement) are of interest, thus no timeinvariant covariates are included (as they were in the random effects model). Other than the effect of time, no time-varying covariates were used in these models (all time-invariant variables, measured and unmeasured, are incorporated in the estimate of the unit on the outcome).
        </p>
        </text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f327077861"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:21+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>An optimized dense convolutional neural network (CNN) architecture (DenseNet) for corn leaf disease recognition and classification is proposed in this paper. Corn is one of the most cultivated grain throughout the world. Corn crops are highly susceptible to certain leaf diseases such as corn common rust, corn gray leaf spot, and northern corn leaf blight are very common. Symptoms of these leaf diseases are not differentiable in their nascent stages. Hence, the current research presents a solution through deep learning so that crop health can be monitored and, it will lead to an increase in the quantity as well as the quality of crop production. The proposed optimized DenseNet model has achieved an accuracy of 98.06%. Besides, it uses significantly lesser parameters as compared to the various existing CNN such as <rs id="a12966194" type="software">EfficientNet</rs>, <rs id="a12966195" type="software">VGG19Net</rs>, <rs id="a12966196" type="software">NASNet</rs>, and <rs id="a12966197" type="software">Xception Net</rs>. The performance of the optimized DenseNet model has been contrasted with the current CNN architectures by considering two (time and accuracy) quality measures. This study indicates that the performance of the optimized DenseNet model is close to that of the established CNN architectures with far fewer parameters and computation time.</p>
<p>An improved <rs id="a12966198" type="software">GoogLeNet</rs> and Cifar 10 model was proposed in [7] for classification of 8 corn leaf diseases (southern leaf blight, brown spot, rust, round spot, dwarf mosiac, curvularia leaf spot, gray leaf spot, northern leaf blight). Total 500 images were collected for 9 classes (8 classes of diseased corn leaves and one for healthy leaves). Data augmentation technique was deployed on the images. The proposed <rs id="a12966199" type="software">GoogLeNet</rs> architecture had 22 layers and lesser parameters than VGG and <rs id="a12966200" type="software">Alexnet</rs> model. Cifar 10 model was also optimized by adding more layers and using ReLU function. The performance of the models was evaluated on the corn leaf dataset. The precision of <rs id="a12966201" type="software">GoogLeNet</rs> and Cifar 10 was 98.9% and 98.8%, respectively. Durga and Anuradha [8] used SVM and ANN algorithm for leaf disease classification in tomato and corn plants. Image dataset included 200 pictures with a collection of healthy leaves and diseased leaves like northern leaf blight, common rust, bacterial spot, tomato mosaic virus, etc. They used the following steps to identify the diseases: the RGB picture was converted to the grayscale picture and the image was then segmented by calculating the intensity gradient at each pixel. For feature extraction, HOG (histogram of oriented gradients) procedure was used. The extracted features were fed to the SVM and ANN classifier models. For corn crops, SVM gave an accuracy of 70-75% and ANN gave an accuracy of 55-65%.</p>
<p>Bhatt and Sarangai [9] developed a system for the identification of corn leaf diseases using CNN architectures (<rs id="a12966202" type="software">VGG16</rs>, <rs id="a12966203" type="software">inception</rs>-v<rs id="a12966204" type="version" corresp="#a12966203">2</rs>, <rs id="a12966205" type="software">ResNet50</rs>, <rs id="a12966206" type="software">mobileNet</rs>-v<rs id="a12966207" type="version" corresp="#a12966206">1</rs>) and applied a combination of adaptive boosting with decision tree-based classifier to distinguish between diseases that appeared to be similar. Four categories of image data included healthy leaves, common rust, leaf blight, and leaf spot. Pictures of each class were taken from PlantVillage dataset. The pictures were resized for image preprocessing according to the necessity of the CNN model used. Features derived from the CNN models were fed to the classifiers (softmax, support vector machine and random forest). It was observed that <rs id="a12966208" type="software">inception</rs>-v<rs id="a12966209" type="version" corresp="#a12966208">2</rs> gave the highest precision with random forest. From the confusion matrix of each classifier, the author noted that leaf blight and leaf spot classes were difficult to differentiate. Therefore, to increase the classification accuracy between them, the Adaptive boosting technique was applied to the best performing model (based on <rs id="a12966210" type="software">Inception</rs>-v<rs id="a12966211" type="version" corresp="#a12966210">2</rs> and RF). Finally, the model reached an accuracy of ~98%.</p>
<p>• We propose an optimized DenseNet architecture for corn leaf disease recognition and classification. Further, we have trained four other established CNN models such as <rs id="a12966212" type="software">VGGNet</rs>, <rs id="a12966213" type="software">XceptionNet</rs>, <rs id="a12966214" type="software">EfficientNet</rs>, <rs id="a12966215" type="software">NASNet</rs> for corn leaf disease recognition and classification.</p>
<p>It is noted that the proposed CNN architecture takes fewer parameters and is computationally cost effective. Simulation results reveal that the proposed network has 77,612 parameters and shows 98.06% accuracy as compared to the existing CNN models such as <rs id="a12966216" type="software">EfficientNet</rs>, <rs id="a12966217" type="software">VGG19Net</rs>, <rs id="a12966218" type="software">NASNet</rs>, and <rs id="a12966219" type="software">XceptionNet</rs>. Further, these results indicate that the accuracy of the proposed model is close to existing CNN architectures, but with significantly fewer parameters.</p>
<p><rs id="a12966220" type="software">LeNet</rs> <rs id="a12966221" type="bibr">[15]</rs> architecture started the history of CNN. The interest in CNN began with <rs id="a12966222" type="software">AlexNet</rs> <rs id="a12966223" type="bibr">[10]</rs> and <rs id="a12966224" type="software">ZFNet</rs> <rs id="a12966225" type="bibr">[16]</rs> and it has grown exponentially ever since. Some popular architectures that have been considered in this study are discussed below. weight layers) and <rs id="a12966226" type="software">VGG19</rs> (with 19 weight layers). Large receptive fields in <rs id="a12966227" type="software">VGGNet</rs> have been substituted with consecutive layers of 3x3 convolutions with ReLU in between. The convolutional stride was fixed to 1 pixel. The padding of convolution layer input was maintained as 1 pixel and max-pooling was done with a stride of 2 over a 2 × 2-pixel window [17]. The pile of convolutional layers was joined with 3 fully connected layers and one softmax layer. <rs id="a12966228" type="software">VGGNet</rs> has 144 million parameters, of which approximately 124 million are used in the last 3 Fully Connected layers. Therefore, if the fully connected layers could be eliminated, the architecture efficiency could be enhanced. This step was taken in subsequent architectures replacing the first Fully Connected layer with a node layer using a method called average pooling [18]. The architecture of <rs id="a12966229" type="software">VGGNet</rs> (<rs id="a12966230" type="software">VGG16</rs>) is shown in Figure 1[36]. The main hallmark of the inception network was to utilize computing resources inside the network. It eliminates all the fully connected layers and uses average pooling, therefore, the system has only 5 million parameters [18]. Inception module is the building block of inception network, which captures parallel paths with distinct receptive field dimensions and operations in the stack of feature maps [14]. After the tremendous success of the inception network, <rs id="a12966231" type="software">GoogLeNet</rs> (<rs id="a12966232" type="software">Inception</rs> V<rs id="a12966233" type="version" corresp="#a12966232">1</rs>) was modified to <rs id="a12966234" type="software">Inception</rs>V<rs id="a12966235" type="version" corresp="#a12966234">2</rs>, <rs id="a12966236" type="software">Inception</rs> V<rs id="a12966237" type="version" corresp="#a12966236">3</rs>, and <rs id="a12966238" type="software">Inception-ResNet</rs>.</p>
<p>Inspired by inception network, Chollet [19] The need for computing resources grows as the dataset increases. Zoph and V. Le [22] proposed a method to search for an architectural building block on a small dataset and transferred the block to a larger dataset. <rs id="a12966239" type="software">NASNET</rs> achieved an 82.7 percent top-1 and 96.2 percent top-5 state-of-the-art accuracy on <rs id="a12966240" type="software">ImageNet</rs>. Two types of convolutional layers (or cells) are required to build <rs id="a12966241" type="software">NASNet</rs>: (1) convolutional layers that return same size feature maps (Normal Cells), and (2) convolutional layers which return a feature map with its height and width decreased by a factor of two (Reduction Cells). The overall architecture is predefined in [22] as shown in Figure 3. [20] proposed <rs id="a12966242" type="software">EfficientNet</rs>, which showed high precision value and, found more flexible as compared to ConvNets [21]. Authors [20] had addressed the challenges (developed at a fixed resource budget, and then scaled up to gain better accuracy) of the ConvNets by suggesting new scaling method which showed the tendency to uniformly scales all dimensions (depth/width/resolution) of the CNN. The effectiveness of the proposed scaling method was demonstrated by implementing it for scaling on MobileNets and ResNet.</p>
<p><rs id="a12966243" type="software">EfficientNet-B7</rs> revealed impressive results as it achieved state-of-the-art result of 84.4% top-1/97-1% top-5 accuracy on <rs id="a12966244" type="software">ImageNet</rs> with 8.4x smaller and 6.1x quicker compared to finest current ConNets [21]. The feature-maps of all the previous layers are used as inputs for each layer, and their featuremaps are used in all subsequent layers as inputs. DenseNets solves the issue of vanishing gradient [34], strengthen feature propagation, encourages reuse of features, and significantly reduces the number of parameters [24]. Each lth layer has l inputs, composed of the feature-maps of all preceding convolutional blocks. It passes its feature-maps to all Ll subsequent layers. This introduced L(L+1)/2 connections in an L-layer network [24]. This architecture has a dense connectivity pattern, therefore called a Dense Convolutional neural network.</p>
<p>• t: Initial time step • Vd w : first moment vector for weights • Vd b : first moment vector for bias • Sd w : second moment vector for weights Most of the computer vision applications use CNN. Two major difficulties with CNN are hardware for excellent performance and high-power consumption of this hardware [12]. Therefore, high-performance hardware like Colaboratory's GPU is required. Using the accelerated runtime of <rs id="a12966245" type="software">Colaboratory</rs> to train CNNs is 2.93 times faster on average than using all
Linux server physical cores <rs id="a12966247" type="bibr">[25]</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f196573605"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:49+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Eye gaze and mouse-click responses were recorded and exported through <rs id="a12951680" type="software">SMI Experiment Suite</rs> software. Mouse-click accuracy in experimental items was close to ceiling (98.4%) and thus not further analysed. Eye gaze data were classified as fixations, saccades and blinks using the software's default settings. Data were binned into 20-ms samples for further analysis. Trials with a very low proportion of fixations overall were removed (4.2% of data). 4 Participants with fewer than 15 (out of 20) experimental trials remaining after this procedure were excluded from further analysis (n=2). One additional participant was excluded for consistently fixating the Theme during the intersentential pause, thus rendering Source/Goal analysis impossible.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f390093682"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:44+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>We estimated the effectiveness of NPIs with a Bayesian hierarchical model. We used case and death data from each country to infer the number of new infections at each point in time, which is itself used to infer the (instantaneous) reproduction number R t over time. NPI effects were then estimated by relating the daily reproduction numbers to the active NPIs, across all days and countries. This relatively simple, data-driven approach allowed us to sidestep assumptions about contact patterns and intensity, infectiousness of different age groups, and so forth that are typically required in modeling studies. This approach also allowed us to directly model many sources of uncertainty, such as uncertain epidemiological parameters, differences in NPI effectiveness between countries, unknown changes in testing and infection fatality rates, and the effect of unobserved influences on R t . The <rs id="a12894722" type="software" subtype="implicit">code</rs> is available online <rs id="a12953679" type="bibr">(14)</rs>.</p>
<p>In this section, we give a short summary of the model (Fig. 5). The detailed model description is given in supplementary text section A. Briefly, our model uses case and death data from each country to "backward" infer the number of new infections at each point in time, which is itself used to infer the reproduction numbers. NPI effects are then estimated by relating the daily reproduction numbers to the active NPIs, across all days and countries. This relatively simple, data-driven approach allowed us to sidestep assumptions about contact patterns and intensity, infectiousness of different age groups, and so forth that are typically required in modeling studies. <rs id="a12894724" type="software" subtype="implicit">Code</rs> is available online <rs id="a12953680" type="bibr">(14)</rs>.</p>
<p>Data and materials availability: All data and <rs id="a12894726" type="software" subtype="implicit">code</rs> are available in the paper or publicly online at <rs id="a12953681" type="bibr">(14)</rs>. This work is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. To view a copy of this license, visit https:// creativecommons.org/licenses/by/4.0/. This license does not apply to figures/photos/artwork or other content included in the article that is credited to a third party; obtain authorization from the rights holder before using such material.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f478813187"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T12:56+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Membranes 2020, 10, x FOR PEER REVIEW 2 of 18 fossil fuel supply, along with issues concerning the implementation of traditional fossil fuels on human health. There is an immediate need to use green alternative and sustainable energy to replace existing non-renewable fossil fuels. It is noted that there is an increase in renewable energy generation produced globally. Based on literature review, renewable power capacity of approximately 1560 GW was utilized at the end of 2013, nearly double the 895 GW recorded at the beginning of 2004 [2]. Nevertheless, renewable power plants were reported to have many disadvantages. One of the disadvantages is that renewable power plants are typically located far from the demand site, which causes difficulty in transporting renewable energy. With current centralized power generation and distribution networks, increasing distributed renewable power plants, such as photovoltaic arrays and wind farms, results in a major effect on grid stability. Hence, the curtailment method was applied to resolve these expensive problems and further escalating issues. Other than the storing energy technique, fuel cell technology is one of the recent technologies that provides a fast solution to the above-mentioned problems. Fuel cells have potential in various applications, such as portable power, stationary electricity generation, vehicle propulsion and in large electrical plants [3,4]. The category of fuel cells is dependent on many elements, for example, conditions during operation (pressure, humidity, temperature), fuel cell structure (application system and scale), and the complexion of the fuel cell's polymer electrolyte [5]. DuPont Company produced a cation-exchange membrane, also known as Nafion ® in the middle of 1960s with a backbone of polytetrafluoroethylene, perfluorinated vinyl ether suspended side chains eliminated by ionic sulfonate groups [6]. Its properties of excellent chemical and thermal strengths, as well as its high-proton-causing Nafion ® , are now being used commercially. The structure of the Nafion membrane consists the cluster channel that is labeled as the first unit for its component. The 4-nm structure of the Nafion is linked together with the water structure that having the diameter of 1 nm that are equally discrete within the hydrophobic backbones is imagined in Figure 1 [7]. Researchers attempts to obtain a robust polymer electrolyte membrane with properties of the high conductivity of protons, little water or fuel crossover, high chemical and thermal stability, and excellent mechanical characteristics [8]. Therefore, to overcome the disadvantages of Nafion ® and to create brand-new membrane materials of better or similar quality for the application of fuel cells, scientists are manufacturing feasible PEMs via the polymeric materials functionalization [7]. Researchers attempts to obtain a robust polymer electrolyte membrane with properties of the high conductivity of protons, little water or fuel crossover, high chemical and thermal stability, and excellent mechanical characteristics [8]. Therefore, to overcome the disadvantages of Nafion ® and to create brand-new membrane materials of better or similar quality for the application of fuel cells, scientists are manufacturing feasible PEMs via the polymeric materials functionalization [7]. Previous studies reported the sulfonated poly (arylene ether sulfone) (SPAES) fabrication and alteration via functionalization in modifying membrane morphology to enhance the features of fuel cells, such as the conductivity of protons, the permeability of methanol and water absorption [9]. Moreover, the data acquired from <rs id="a12900019" type="software">SCOPUS</rs> ® , peer-reviewed literature's citation database and the largest abstract show that there are currently increasing interests in SPAES for fuel cells used [10].</p>
<p>Previous studies reported the sulfonated poly (arylene ether sulfone) (SPAES) fabrication and alteration via functionalization in modifying membrane morphology to enhance the features of fuel cells, such as the conductivity of protons, the permeability of methanol and water absorption [9]. Moreover, the data acquired from <rs id="a12900020" type="software">SCOPUS</rs> ® , peer-reviewed literature's citation database and the largest abstract show that there are currently increasing interests in SPAES for fuel cells used [10].</p>
<p>The fundamentals theory and the practical operation of a PEMFC involve various mathematical models presented in this section. Peng and groups had constructed the equivalent modeling of membrane hydration dynamic inside PEMFC in order to minimize the membrane micro-flooding. From the results, it was found that the implementation of the studied model able to improve the maximum net power boost can be estimated as being up to 3.74%, which is essential for the optimal operation of the integrated PEMFC system to achieve a higher system efficiency [70]. In another study by Salimi et al., the neural network modeling is found able to increase the power output of the PEMFC systems [71].Through the designated model named an artificial neural network (ANN), the operating performance increased up to 28.9%. A comprehensive stack model is developed based on the integration of a 1 + 1 dimensional multiphase stack sub-model and a flow distribution sub-model has been developed [72]. The purpose the constructed model is to study the flow distributions as well as reactions, phase changes, and transport processes inside the PEMFC. From the obtained data analysis, the uniform flow assumption not only overestimates the stack output performance but also underestimates the fuel cell voltage variations. Besides, neglecting the non-uniform flow distribution may lead to higher predictions of the overall stack temperature and lower predictions of the temperature variations among different fuel cells. In other approaches by Chugh and colleagues, the low temperature of PEMFC performance is deeply studied via the mathematical modeling, which is <rs id="a12900021" type="software">MATLAB</rs>. The model predicts an increase in PEMFC performance with an increase in operating temperature, pressure and reactant humidity. An increase in stack operating temperature from 50 to 80 • C was seen to increase the stack voltage by 25%, because of lowering the activation potential and ohmic resistance. However, a further increase in operating temperature results in membrane dehydration. Similarly, a 30% increase in stack output power was observed upon increasing the operating pressure from 0 to 100 kPag. The further increase in pressure to 200 kPag showed a 60% increase in the output power [73]. In PEMFC, the water transport behavior in the gas diffusion layer (GDL) and bipolar plate (BPP) affected by the nonuniform compression on the GDL. Thus, Xu et al. studied these effects via the constructed model to obtain the relationship between the GDL deformation and assembly clamping force based on the energy method [74]. From the proposed model, the results show that drainage pressure increases monotonically with the assembly clamping force. In addition, thin GDL is conducive to improving drainage capacity. However, due to the combined effect of through-plane and in-plane mass transport in GDL, the maximum pressure first decreases and then increases with the thickness of GDL. GDL with a thickness of 0.2 mm is regarded as the best size to guarantee good water transport for the baseline case.</p>
<p>Iwata et al. presented their SOFC model, which discussed the relationship between the profile of high temperature and the current density [101]. The studies revealed that temperature depends on power density. Numerical methods were used to analyze the coupled flow processes, mass/heat transfer, electrochemistry and chemical reaction. A tubular SOFC thermal transport model was developed by Haynes and Wepfer, and they stated that the primary heat transfer mechanism between the cell and the air supply tube was radiation [102]. Larrain et al. implemented a parameter estimation approach to investigate parameters for a simple kinetic and thermal model used for small SOFC (20 cm 2 of anode-supported electrolyte with an active area of 1 cm 2 ) [103]. Khaleel et al. incorporated <rs id="a12900022" type="software">MARC</rs>, a commercial finite element analysis code, with an electrochemical (EC) module formed in-house to simulate SOFCs of the planar type [104]. This EC module measures the distribution of heat generation, current density as well as oxidant and fuel concentration, including <rs id="a12900023" type="software">MARC</rs>'s temperature profile. <rs id="a12900024" type="software">MARC</rs> conducts thermal and flow evaluation according to the boundary and initial of flow and thermal conditions, as well as the heat generation measured by the EC module. The operating conditions of a rectangular planar SOFC with an integrated air preheater were examined by Costamagna and co-workers [105].</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f82857191"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:17+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Songs of all nine fathers were recorded. Mature female-directed song was recorded from 15 sons at the age of 108 ± 14 days to ensure ecological realism and relevance with regard to song production. Songs from three additional sons was recorded at the age of 142-153 days. For all recordings, males were placed in a sound proof room overnight in a 46 * 44 * 36 cm cage. The following morning, an adult female zebra finch was placed in an identical cage next to the male's cage and recording began. If the male did not sing within 60 min, further recordings were attempted after a day back with the colony, until we obtained at least 10 song motifs in a single recording. The number of song bouts and the total number of syllables annotated for each individual is included in Table 1. All recordings used a Sennheiser shotgun microphone attached to a Canon MiniDV ZR930 camcorder on Fujifilm DVCassette miniDVs. MiniDV tapes were digitized with a JVC Super VHS ET Professional deck at 44.1 KHz. Uncompressed sound files were created using <rs id="a12951783" type="software">Soundtrack Pro</rs> <rs id="a12951784" type="version" corresp="#a12951783">6</rs> and were saved as separate wav files.</p>
<p>A song bout consists of some repetitions of a single note (introductory notes; see Price, 1979) followed by one or more song motifs (Price, 1979;Sossinka and Böhner, 1980). Song bouts in our data set were defined as strings of syllables in which all silent intervals were shorter than 500 ms, and every uninterrupted sound was defined as a separate syllable (cf. Williams, 2004). A song bout typically included 2-10 repetitions of the introductory note and 1-8 repetitions of a motif (Price, 1979). Because the present research investigates probabilistic dependencies among syllables combined into stable sequences or motifs, we denoted each syllable type in each individual's song by a letter (Price, 1979;Eales, 1985). Every song in the recordings was then broken down into these constituent syllables and transcribed as a sequence of letters using the <rs id="a12951785" type="software">Syrinx</rs> software ( <rs id="a12951786" type="publisher" subtype="person" corresp="#a12951785">John Burt</rs>, <rs id="a12951787" type="url" corresp="#a12951785">www.syrinxpc.com</rs>). See Figure 2 for an example of the full corpus of songs from a single individual.</p>
<p>The second of the two additional measures of distance between graphs, CNAFeat, is based on a family of graph features used in computational network analysis (CNA); the particular features we considered have been used for characterizing brain dynamics and are part of the <rs id="a12951788" type="software">Brain Connectivity Toolbox</rs> (<rs id="a12951789" type="software">BCT</rs>; <rs id="a12951790" type="bibr">Rubinov and Sporns, 2010</rs>). Because of the diverse nature of these features, some of which are global (pertain to the entire graph) and others local (per-vertex), we employed the Mahalanobis distance, which weights individual dimensions by their variance. The composition of the graph feature vectors (ordered lists of features) that we looked at is as follows (for definitions of each measure see Rubinov and Sporns, 2010): transitivity (global); clustering coefficient (per vertex); modularity index (global) and module membership (per vertex); betweenness centrality (per vertex); 3-vertex motif intensities for the 13 classical motifs (per vertex); and 4-vertex motif intensities for the 199 classical motifs (per vertex).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f322978318"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:21+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>A 20-fold dilution of each sample was prepared including 10 ppb of the elements In and Sc as internal standards. XSeries II ICP-MS (Thermo Fischer Scientific, Bremen, Germany) together with an ESI SC2 autosampler (Elemental Service &amp; Instruments, Mainz, Germany) were used for sample analysis. Sample analysis was carried out in triplicate with 100 sweeps each. Resolution was set to 0.02 amu and the dwell time for all elements was 10 ms. Measurements were carried out with collision cell in either -3.0 V mode (In, Sc, Cr, Fe, Ni, Cu, Cd) or 0.0 V mode (Sc, Al). H 2 /He (7% v/v) was used as the collision gas with 5 ml/min flow rate. Data were processed with <rs id="a12951791" type="software">PlasmaLab</rs> <rs id="a12951792" type="version" corresp="#a12951791">2.5.11.321</rs> ( <rs id="a12951793" type="publisher" corresp="#a12951791">Thermo Scientific</rs>, Bremen, Germany).</p>
<p>In total 8 skin and 8 lymph node samples of tattooed donors as well as 2 skin and 2 lymph node samples of non-tattooed donors were analyzed. Samples between 50-200 µg were lysed using 1 mg/ml collagenase from Clostridium histolyticum Type IA (Sigma Aldrich, Munich, Germany) with an incubation time of at least 24 hours at 37 °C. Lysates were heat-inactivated at 90-95 °C for at least 12 hours. Precipitated pigment particles were washed twice with PBS. Centrifugation was carried out with 500× g for 10 min. Precipitates were applied as thin films to a ground steel target plate with a plastic pipette tip and measured using an UltrafleXtreme MALDI-ToF/ToF (Bruker Daltonik, Bremen, Germany). Spectra were obtained by averaging 3000 individual spectra, with a laser rate of 1000 Hz in positive reflector mode. The instrument was calibrated prior to each measurement with an external ProteoMass ™ MALDI Calibration Kit (Sigma Aldrich, Munich, Germany). Data were processed using the <rs id="a12951794" type="software">flexControl</rs> <rs id="a12951795" type="version" corresp="#a12951794">3.4</rs> and <rs id="a12951796" type="software">flexAnalysis</rs> <rs id="a12951797" type="version" corresp="#a12951796">3.4</rs> software ( <rs id="a12951798" type="publisher" corresp="#a12951796">Bruker Daltonik</rs>, Bremen, Germany).</p>
<p>The <rs id="a12951799" type="software">OMNIC</rs> software was used to transform spectra from maps of skin and lymph node samples to second derivatives using Savitsky-Golay of second polynomial order with 21 smoothing points 45,46 . <rs id="a12951800" type="software">Unscrambler X</rs> software (Version <rs id="a12951801" type="version" corresp="#a12951800">10.3</rs>, <rs id="a12951802" type="publisher" corresp="#a12951800">CAMO Software</rs>, Oslo, Norway) was used for further statistical analysis. Principal component analysis (PCA) was performed on the mean-centered data using the spectral regions from: 1800 to 1350 cm -1 (related to proteins) and 3200 to 2800 cm -1 (related to lipids) 47,48 . PCA was performed separately for skin and lymph node samples. Score plots and loading plots obtained by PCA analysis as well as mean values from the regions of interest were used for data interpretation.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f477935931"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:29+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>To realize quasi-BIC resonances in this metasurface, we have carried out numerical simulations using the commercial electromagnetic solver <rs id="a12970077" type="software">CST Microwave Studio Suite</rs>, optimizing all the relevant parameters defined in Figure 1a. We found that a crescent metasurface with P x = P y = 405 nm, R 1 = R 2 = 180 nm, W = 60 nm can realize a quasi-BIC resonance at 747 nm as indicated by the green curve (scaling factor S = 1.0) in Figure 1c. By a straightforward scaling of the factor S on these optimized parameters, linear tuning of the quasi-BIC resonances across a large part of the visible spectrum from 610 to 838 nm can be realized (Figure 1c). Our crescent design also provides localized near-field enhancement surrounding the surface (Figure 1b, inset). Figure 1d shows the experimentally measured transmittance spectra using the same geometrical parameters used in the simulation. A confocal microscope integrated with a spectrometer in transmission mode with collimated light illumination is used to characterize the optical properties (see Figure S2, Supporting Information). The height h of the crescent meta-atoms considered in Figure 1c,d is 250 nm. Both the experimental observations and the simulation results indicate a linear red shift of the quasi-BIC resonance with increasing the scaling parameter S from 0.7 to 1.2, which is consistent with previous work. [23] As expected, we also observe a broadening of the FWHM when the resonant wavelength is blue-shifted below 650 nm in both simulations and experiments, which we attribute to the silicon interband loss in that wavelength region (&lt; 660 nm). Figure S3, Supporting Information compares the Q factor (defined as Q = λ 0 /FWHM, where is λ 0 the resonance position) as a function of S between the simulation and experiment. With increasing S, the Q factor improves as expected, due to the reduced intrinsic absorption loss of silicon. The Q factor of the quasi-BIC can be tuned precisely by breaking the in-plane symmetry of the underlying crescent resonators, [26] which is investigated in the following section.</p>
<p>Numerical Simulations: The optical behavior of the all-dielectric silicon metasurface consisting of crescent meta-atoms was simulated by frequency-domain finite-element method Maxwell solver from <rs id="a12970078" type="software">CST STUDIO SUITE</rs> <rs id="a12970079" type="version" corresp="#a12970078">2020</rs>. To improve the agreement between simulation and experiment, measured values of the silicon permittivity from ellipsometry were considered in the simulation. For the illumination, a plane wave at normal incidence to the metasurface with linear polarization in the x-direction (along the axis of crescent meta-atom width, indicated in Figure 1a) was considered. A 2D periodicity defined the unit cell in the x-and y-axis with P x = P y = 405 nm. The crescent width was W = R 1 /3 (R 1 = 180 nm). The refractive index of fused silica serving as supporting substrate was taken as 1.5.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f82834914"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:49+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>After selecting the target turns, we extracted them into individual sound files using <rs id="a12951763" type="software">Praat</rs> <rs id="a12951774" type="bibr">(Boersma and Weenink, 2012)</rs> and created four different versions of each stimulus. These versions were as follows.</p>
<p>Scrambled-Word-Order. We randomly changed the order of the words within the target turn using <rs id="a12951764" type="software">Praat</rs>. The pauses between the words in the original were assigned to the subsequent word. The resulting stimuli therefore had the same duration as the Natural-Turn stimuli. In this condition there was no sequential wordorder information to base the anticipation on, but there were still words present. Thus, the predictability of a word on the basis of its preceding words is switched off, i.e., the cloze probability (Taylor, 1953) of the words in the resulting turns was very low.</p>
<p>Noise. The Noise condition was created using a <rs id="a12951765" type="software" subtype="environment">Praat</rs> script that convolved the speech stimulus of the natural turn with white noise. The resulting sample of constant noise had the same duration and frequency spectrum as the original fragment. This condition served as a comparative baseline from which all linguistic information that could be used for anticipation was removed. The only way to be certain that the turn has ended in this condition is to react to the turn end. This condition measured the response distributions when the participants had no choice but to react to the end of the turn. In order to control for subjective loudness between conditions and stimuli we adjusted the loudness of all stimuli to a reference sone value.</p>
<p>For the presentation of the stimuli we used the <rs id="a12951767" type="software">E-Prime</rs> software package <rs id="a12951775" type="bibr">(Schneider et al., 2012a,b)</rs>, which also allowed us to record the time from stimulus onset to button press.</p>
<p>Conventional significance tests are designed to reject the null hypothesis without fault in the limit of infinite sample size. This is characterized by vanishing p-values and unbounded t-values. In contrast, if the null hypothesis is true and infinite sample sizes are considered the p-values are not converging to any limit value. Correspondingly, under the null hypothesis, all p-values are all equally likely (Rouder et al., 2009). Hence, it is not possible to claim evidence favoring a null hypothesis using conventional significance tests. We therefore also performed a Bayesian analysis (Jeffreys, 1961;Kass and Raftery, 1995) for the Advance-Knowledge and the Natural-Turn condition by comparing them using a Bayesian paired t-test (Rouder et al., 2009). To be consistent with Morey and Rouder (2011) and Rouder et al. (2012) we used a Cauchy prior with scale parameter √ 2 for the standardized effect size in combination with a Jeffreys prior on the variance. The analysis was performed using the <rs id="a12951768" type="software" subtype="component" corresp="#a12951770">BayesFactor</rs> package <rs id="a12951776" type="bibr">(Morey et al., 2014)</rs> for <rs id="a12951770" type="software" subtype="environment">R</rs> ( <rs id="a12951771" type="publisher" corresp="#a12951770">R Development Core Team</rs>, 2009). An overview of a common textual interpretation of Bayes Factor values is presented in Table 3. The Bayesian paired t-test using item means for the variable BIAS revealed that the null hypothesis, stating that Advance-Knowledge and Natural-Turn condition are equal in anticipation accuracy, is twelve times more likely than the alternative hypothesis that these two conditions differ in button press accuracy (BF = 0.08). This provides "strong" evidence for the null hypothesis.</p>
<p>In Eq. ( 1) P Ant (t) and P Reac (t) denote the probability that a response at time t was an anticipation or reaction, respectively. These probabilities were computed in <rs id="a12951772" type="software">R</rs> ( <rs id="a12951773" type="publisher" corresp="#a12951772">R Development Core Team</rs>,
2009) using the density distributions (cosine kernel and 2.5 Sheather and Jones (1991) bandwidth) from the Advanced-Knowledge condition of Experiment 1 and the Noise condition of Experiment 2. To account for noise in the data leading to possibly infinite RAP values we used a cutoff value of 10 -4 in the factor calculations. Due to the log-scale of the RAP ratio negative values corresponds to a higher probability of reaction whereas a positive value indicates that anticipation is more likely.</p>
</text>
</tei>
  <tei>
    <teiHeader>
        <fileDesc id="f81823659"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T16:22+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text lang="en">
        <p>Xenograft tumor sections were de-paraffined and hydrated from xylene, 100% ethanol, 95% ethanol, 85% ethanol and 70% ethanol to distilled water. For Masson's trichrome staining, slides were re-fixed with Bouin's solution at 60°C for 60 minutes. Slides were washed in running tap water for 5 minutes and stained in Weigert's working hematoxyin for 10 minutes. Then they were washed in running tap water for 5 minutes and stained in Biebrich scarlet-acid fuchsin solution for 5 minutes. Slides were rinsed in distilled water and differentiated in phosphomolybdic-phosphotungstic acid solution for 10 minutes, transferred to aniline blue solution and stain for 5 minutes. Slides were rinsed in distilled water and images were taken with a Nikon microscope. The percentage of collagen was quantified by calculating the ratio of blue staining (collagen) area in the total area of the tumor section using 
            <rs type="software">Imagescope</rs> analysis software <ref type="bibr">[33]</ref>.
        </p>
        <p>Cells grown on plastic were lysed in situ in 2% SDS in PBS buffer containing phosphatase and protease inhibitor cocktails (Calbiochem). Protein concentration was measured using DC™ protein assay (Bio-Rad). Control and shP4HA2 cells were trypsinized and counted; equal amounts of conditional medium (normalized to cell number) were precipitated by pre-cooled acetone. Equal amounts of protein lysates and cell conditional medium were subjected to SDS gel electrophoresis, immunoblotted, and detected with an ECL system (Pierce). Western blotting results were quantified using 
            <rs type="software">AlphaInnotech</rs> analysis software.
        </p>
        <p>Analysis of P4HA2 mRNA levels in normal and malignant tissues was performed in the TCGA breast cancer dataset that was downloaded from Oncomine. The association between mRNA levels of P4HA2 and collagen genes was evaluated by the Spearman correlation analysis. All experiments were repeated at least twice. Results are reported as mean ± S.E.M; the significance of difference was assessed by independent Student's t-test. P &lt; 0.05 represents statistical significance and P &lt; 0.01 represents sufficiently statistical significance. All reported P values were 2-tailed. Statistical analysis was conducted with 
            <rs type="software" id="s1">SigmaPlot</rs> (
            <rs type="creator" corresp="s1">Systat Software, Inc</rs>.) and
            <rs type="software" id="s2">SAS</rs> (version
            <rs type="version" corresp="s2">9.2</rs>;
            <rs type="creator" corresp="s2">SAS Institute Inc</rs>.).
        </p>
        </text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f326518267"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:22+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The period from the late 1990s through to around 2007 was dominated by modeling language issues. This was a time where the Unified Modeling Language (UML) was undergoing considerable changes to its semantics, infrastructure and superstructure, and there was a very substantial body of research considering precise semantics of such modeling languages, as well as the metamodeling process [6][7][8][9]. The key use case for modeling was code generation, as embodied by research on model-to-text transformation languages [10] and standards, 3 and the popularity of code generators that were offered "out of the box" in modeling tools, such as the Kennedy Carter (now Abstract Solutions) 4 or <rs type="software" id="a2">Artisan</rs> (now <rs type="software" id="a1">PTC Integrity Modeler</rs>) <rs type="bibr" corresp="#a1">5</rs> tools.</p>
<p>More recently, various research roadmaps [4,5,11] identified a variety of significant research challenges, many of which have seen substantial research effort. The key issues that were identified in these previously published roadmaps include the following -Language engineering (e.g., [12]) principles, practices, and patterns for specifying abstract and concrete syntax, as well as semantics. Research challenges related to understanding the language engineering process were also identified. -Language workbenches (popularized in 2005)-i.e., tools for defining and composing domain-specific languages and their IDEs: the fundamental research against this challenge led to the development of modern language workbenches such as <rs type="publisher" corresp="#a12967661">JetBrains</rs> <rs id="a12967661" type="software">MPS</rs>, <rs id="a12967662" type="bibr">10</rs> <rs id="a12967663" type="software">Xtext</rs>, <rs id="a12967664" type="bibr">11</rs> <rs id="a12967692" type="software">Kermeta</rs>, <rs id="a12967666" type="bibr">12</rs> <rs id="a12967693" type="software">Racket</rs> <rs id="a12967668" type="bibr">13</rs> and <rs id="a12967669" type="software">Spoofax</rs>. <rs id="a12967670" type="bibr">14</rs> -Model management-processes and tasks for manipulating and analyzing models: the fundamental research in this area led to theoretical results (e.g., identification of different model management tasks, such as model merging and comparison) as well as technical contributions (e.g., model management platforms such as <rs id="a12967671" type="software">AtlanMod</rs> <rs id="a12967672" type="bibr">15</rs> and <rs id="a12967673" type="software">Epsilon</rs> <rs id="a12967674" type="bibr">16</rs> ). -Model analysis the challenge of techniques for analyzing models (e.g., for performance or correctness [13]), along with principles relate to understanding what makes a good model. -Models at runtime the use of models to manage and understand systems after they have been deployed and as they execute behavior [14]. Substantial research has taken place regarding this challenging to identify techniques 8 https://www.omg.org/spec/OCL/About-OCL/.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f480166618"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:43+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>For the rim weighting, initially (first stage) the sample was weighted to LTLA counts and age by sex groups only, adjusting the age and sex groups to ensure that the final weighted estimates were as close as possible to the population profile. Then, using the first stage weights as starting weights, the rim weighting was adjusted for all four measures, with the adjustment factor between the first and second stage weights trimmed at the 1st and 99th percentiles to dampen the extreme weights and improve efficiency. The final weights were calculated as the first stage weights multiplied by the trimmed adjustment factor for the second stage, with confidence intervals for weighted prevalence estimates calculated using the "<rs id="a12966970" type="software" subtype="component" corresp="#a12966971">survey</rs>" package in <rs id="a12966971" type="software" subtype="environment">R</rs> <rs id="a12966980" type="bibr">(41)</rs>.</p>
<p>Statistical analyses were carried out in <rs id="a12966973" type="software">R</rs> <rs id="a12966981" type="bibr">(42)</rs>. To investigate the potential confounding effects of covariates on prevalence estimates we performed logistic regression on swab positivity as the outcome and: sex, age, region, employment type, ethnicity, household size and neighborhood deprivation as explanatory variables. We adjusted for age and sex, and mutually adjusted for the other covariates to obtain odds ratio estimates and 95% confidence intervals. We decided not to adjust for multiple testing to facilitate direct comparisons with other publications where only comparison-wise error rate (CER) has been controlled for (43). We estimated adjusted VE as 1 -odds ratio where the odds ratio was obtained from comparing vaccinated and unvaccinated individuals in a logistic regression model with swab positivity as outcome and with adjustment for age and sex, and age, sex, IMD quintile and ethnicity.</p>
<p>RT-PCR positive swab samples where there was sufficient sample volume and with N gene Ct values &lt; 32 were sent frozen from the laboratory to the Quadram Institute, Norwich, UK for viral genome sequencing. Amplification of viral RNA used the ARTIC protocol (48) and sequencing libraries were prepared using CoronaHiT (49). Analysis of sequencing data used the ARTIC bioinformatic pipeline (50) with lineages assigned using <rs id="a12966976" type="software">PangoLEARN</rs> <rs id="a12966982" type="bibr">(51)</rs>.</p>
<p>Access to REACT-1 individual-level data is restricted to protect participants' anonymity. Summary statistics, descriptive tables and <rs id="a12966978" type="software" subtype="implicit">code</rs> from the current REACT-1 study are available at <rs id="a12966979" type="url" corresp="#a12966978">https://github.com/mrc-ide/reactidd</rs>. REACT-1 Study Materials are available for each round at www.imperial.ac.uk/medicine/research-andimpact/groups/react-study/react-1-study-materials/. Public involvement A Public Advisory Panel provides input into the design, conduct and dissemination of the REACT research program.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f195283552"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T09:30+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref> 
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>With individual-level data, TSLS can be implemented with multiple exposure variables, regressing each exposure on the full set of SNPs to yield genetically predicted estimates for X 1 and X 2 . The outcome Y can then be regressed on these predicted estimates for X 1 and X 2 jointly to obtain consistent estimates of b 1 and b 2 . This can be conducted by simply using the <rs id="a12874946" type="software" subtype="component" corresp="#a12874947">ivreg2</rs> command in <rs id="a12874947" type="software" subtype="environment">Stata</rs> or <rs id="a12874948" type="software" subtype="component" corresp="#a12874949">ivpack</rs> in <rs id="a12874949" type="software" subtype="environment">R</rs>.</p>
<p>Joint strength can be assessed using the Sanderson-Windmeijer conditional F-statistic, 33 F c , that is available as part of <rs id="a12874950" type="software" subtype="component" corresp="#a12874951">ivreg2</rs> in <rs id="a12874951" type="software" subtype="environment">Stata</rs>. F c is calculated in the following manner:</p>
<p>then regress the residuals on the full set of instruments; the Sargan test is then the sample size times the R 2 of this regression; • evaluating with the Sargan statistic with respect to a v 2 distribution with degrees of freedom equal to the number of instruments minus the number of predicted exposure variables (i.e. the null hypothesis that all of the instruments are valid). 4 This test is available as part of the <rs id="a12874952" type="software" subtype="component" corresp="#a12874953">ivreg2</rs> command in <rs id="a12874953" type="software" subtype="environment">Stata</rs> and the <rs id="a12874954" type="software" subtype="component" corresp="#a12874955">ivpack</rs> package in <rs id="a12874955" type="software" subtype="environment">R</rs>. In order to conduct this test, the model must be over-identified, i.e. there must be more instruments than exposure variables (so that the degrees of freedom of the v 2 test is positive). 35 This 'global' test does not give any indication as to which of the genetic instruments are invalid if the test rejects the null. However, alternative methods of estimation can be used to estimate the causal effects as long as at least 50% of the SNPs do not have a pleiotropic effect on the outcome. 36,37 The two-sample summary data setting Assessment of instrument validity and strength is apparently yet to be described in the two-sample summary data setting that is relevant to the majority of contemporary MR studies, and consequently it is not implemented in any standard software. We therefore describe the necessary procedures in fine detail so that they can be confidently implemented by others.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f74371942"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T08:17+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>More well known are the small-sample corrections to SEs with associated sample size adjustments that have been developed for the REML estimator (Kenward andRoger, 1997, 2009), and which are available in <rs id="a12951610" type="software" subtype="environment">SAS</rs>'s <rs id="a12951611" type="software" subtype="component" corresp="#a12951610">PROC MIXED</rs> and <rs id="a12951612" type="software" subtype="environment">R</rs>'s <rs id="a12951613" type="software" subtype="component" corresp="#a12951612">ASREML</rs> package. On the one hand, Monte Carlo analysis has shown that these methods work well. On the other hand, the methods provide better inference only for fixed effects in linear mixed models and, as we have argued, applied social science researchers are often interested in estimates of variance component parameters and also in non-linear models.</p>
<p>Our simulation results are based on two-level linear and logit models. In this article, we focus on a 'basic' specification with random intercepts corresponding to equation (1). The regressors include a constant (intercept), individual-level fixed effects, a country-level fixed effect, and a random country intercept. (The model also includes an individual-specific error term.) In common with most social science applications, we assume that the random effects are uncorrelated with each other. To make the models more concrete, we refer to the outcome variables for the linear and non-linear models as 'hours' (of work) and (labour force) 'participation', respectively. We shall also refer briefly to our Monte Carlo analysis of 'extended' linear and logit models that include the same regressors but add two cross-level interactions, and two random slopes. Further details of the results for these models and <rs id="a12951618" type="software" subtype="environment">Stata</rs> <rs id="a12951614" type="software" subtype="implicit" corresp="#a12951618">code</rs> for running all of the simulations are provided in the Supplementary Material (sections 7 and 9).</p>
<p>Estimation and simulation were undertaken using <rs id="a12951524" type="software">Stata</rs> ( <rs id="a12951529" type="publisher" corresp="#a12951524">StataCorp</rs>, 2011). The linear ('hours') models were fitted by ML using the <rs id="a12951615" type="software">xtmixed</rs> command's REML estimator. The logit ('participation') models were fitted by ML using the <rs id="a12951616" type="software">xtmelogit</rs> command's adaptive Gaussian quadrature procedure with seven integration points (the default). Doubling the number of integration points to 14 led to virtually identical estimates.</p>
<p>The 95 per cent CIs for relative bias statistics (and Root Mean Squared Error) The CIs for each relative parameter bias statistics are calculated using the 'empirical' SE, which is the standard deviation of the estimated statistic calculated from the R replications (Burton et al., 2006: p. 4286). The wider is the CI, the greater is the variability of the estimate. As pointed out by Burton et al. (2006), When judging the performance of different methods, there is a trade-off between the amount of bias and the variability. Some argue that having less bias is more crucial than producing a valid estimate of sampling Notes: See main text for explanation of the models and regressors. For detailed discussion of how the regressors were simulated, see section 4 of the Supplementary Material (and the <rs id="a12951619" type="software" subtype="environment">Stata</rs> <rs id="a12951617" type="software" subtype="implicit" corresp="#a12951619">code</rs> in section 9). The RE are: an individual-specific error e ic $ N(0, r 2 e ); a random country intercept u c $ N(0, r 2 u ). variance . . . However, methods that result in an unbiased estimate with large variability or conversely a biased estimate with little variability may be considered of little practical use. (Burton et al., 2006: p. 4286.) We demonstrate below that the combination of lack of bias but large variability is a feature of country-level fixed effects estimates from multi-country data sets. Researchers sometimes use composite measures of estimator accuracy that combine summaries of bias and variability, the most common of which is the Root Mean Squared Error (RMSE) statistic associated with each parameter (the square root of the sum of absolute bias squared and the empirical SE squared). We have also calculated RMSE statistics for our basic model simulations, and they yield conclusions about accuracy consistent with the discussion below (see section 6 of the Supplementary Material).</p>
<p>A second strategy is to use methods that are more robust to small numbers of countries, as mentioned in section 3. These include small sample corrections, such as those available in <rs id="a12951527" type="software">SAS</rs> and <rs id="a12951530" type="software">R</rs> for linear models, and bootstrapping. However, we note that some of these techniques require specialized knowledge and are not routinely available in the software packages most commonly used by social scientists, and they are not applicable to all parameters of interest.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f479512301"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:21+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Motivation: <rs id="a12965778" type="software">fastsimcoal2</rs> extends <rs id="a12965779" type="software">fastsimcoal</rs>, a continuous time coalescent-based genetic simulation program, by enabling the estimation of demographic parameters under very complex scenarios from the site frequency spectrum under a maximum-likelihood framework. Results: Other improvements include multi-threading, handling of population inbreeding, extended input file syntax facilitating the description of complex demographic scenarios, and more efficient simulations of sparsely structured populations and of large chromosomes. Availability and implementation: <rs id="a12965780" type="software">fastsimcoal2</rs> is freely available on <rs id="a12965781" type="url" corresp="#a12965780">http://cmpg.unibe.ch/software/fastsimcoal2/</rs>. It includes console versions for <rs id="a12965782" type="software">Linux</rs>, <rs id="a12965783" type="software">Windows</rs> and <rs id="a12965784" type="software">MacOS</rs>, additional <rs id="a12965785" type="software" subtype="implicit">scripts</rs> for the analysis and visualization of simulated and estimated scenarios, as well as a detailed documentation and ready-to-use examples.</p>
<p>Coalescent theory (Kingman, 1982) has provided a very efficient framework to simulate the diversity of neutrally evolving loci (Hudson, 1990;Kelleher and Lohse, 2020;Marjoram and Wall, 2006). These simulations have been used extensively to check the validity of theoretical derivations, and to make predictions of the effect of complex demographic processes on the genomic diversity of populations. Due to their versatility, they have also been used in Approximate Bayesian Computations (ABC, Beaumont et al., 2002) to estimate parameters under very complex models and to perform model testing (Beaumont, 2019;Currat et al., 2019;Mondal et al., 2019;Sanchez et al., 2020;Wegmann et al., 2010). Several faster alternatives to ABC have been developed in the last ten years to estimate demographic parameters under relatively complex scenarios (Albers and McVean, 2020;Gutenkunst et al., 2009;Steinru ¨cken et al., 2019;Weissman and Hallatschek, 2017), many of them fitting the Site Frequency Spectrum (SFS) using exact derivations or approximations (e.g. Excoffier et al., 2013;Gutenkunst et al., 2009;Kamm et al., 2020;Liu and Fu, 2020). It has been shown that the expected SFS could be robustly estimated using coalescent simulations (Excoffier et al., 2013). A clear advantage of SFS-based methods is that the computing time is independent of the length of the analyzed genome. SFS-based methods, however, ignore information on linkage between sites, an information that is used in Hidden Markov Models-based approaches (e.g. Li and Durbin, 2011;Schiffels and Wang, 2020;Speidel et al., 2019;Terhorst et al., 2017) or those based on the
Ancestral Recombination Graph (e.g. Gronau et al., 2011;Kelleher et al., 2019). In this paper, we describe the latest implementation of <rs id="a12965786" type="software">fastsimcoal 2</rs>, a coalescent-based program that can estimate parameters from SFS under very complex demographic scenarios including continuous arbitrary size changes, gene flow, admixture events, bottlenecks, populations splitting, population growth, inbreeding, serial sampling and spatially structured populations. Compared to its initial release one decade ago <rs id="a12965823" type="bibr" corresp="#a12965788">(Excoffier and Foll, 2011)</rs>, <rs id="a12965788" type="software">fastsimcoal</rs> has been extended in several ways described below.</p>
<p>2 Novelties implemented in <rs id="a12965789" type="software">fastsimcoal2</rs> <rs id="a12965790" type="software">fastsimcoal</rs> became <rs id="a12965791" type="software">fastsimcoal2</rs> (abbreviated <rs id="a12965792" type="software">fsc2</rs> in the following) with the implementation of demographic and mutation parameters inference from the SFS (Excoffier et al., 2013). While <rs id="a12965793" type="software">fsc2</rs> might not have a clear edge over other coalescent simulators of genomic diversity, like e.g. <rs id="a12965794" type="software">msprime</rs> <rs id="a12965824" type="bibr" corresp="#a12965794">(Kelleher and Lohse, 2020)</rs>, its innovation is rather in its built-in ability to perform parameter inference under complex evolutionary scenarios, and the most recent developments have therefore focused on this aspect.</p>
<p>For parameter inference, coalescent simulations are used to estimate the expected SFS following Nielsen (2000), and a multinomial likelihood (Adams and Hudson, 2004) is maximized using a conditional expectation maximization algorithm (Meng and Rubin, 1993) to estimate the parameters, one at a time over several optimization cycles. This approach has been shown to be very robust (Excoffier et al., 2013) and can, in principle, be applied to an arbitrarily large number of populations, whereas approaches based on analytically derived SFS can only handle a few populations [e.g. 3 populations in @a@i, (Gutenkunst et al., 2009) or 4-5 in <rs id="a12965796" type="software">dadi</rs>. <rs id="a1" type="software">CUDA</rs> <rs id="a12965825" type="bibr" corresp="#a1">(Gutenkunst, 2021)</rs>], or do not deal with continuous gene flow [e.g. in <rs id="a12965798" type="software">momi2</rs> <rs id="a12965826" type="bibr">(Kamm et al., 2020)</rs>]. The trade-off for this robustness and versatility is that computing time, which is independent of genome size, will however increase linearly with the number of sampled genomes, but it remains reasonable given the speed improvements mentioned below. <rs id="a12965800" type="software">fsc2</rs> also gives the possibility to optimize the likelihood of the model considering all sites (monomorphic and polymorphic), polymorphic positions only or a mixed approach where optimization is first performed using likelihoods based on all sites, and then only considering polymorphic sites after a given number of cycles (-l command line option). Finally, it is now possible to ignore singleton sites (--nosingleton option), which might be useful when considering ancient DNA or low coverage data where some genotyping errors might have arisen. Note that while <rs id="a12965801" type="software">fsc2</rs> is using the SMC' approximation (Marjoram and Wall, 2006) of the Sequential Markov Coalescent (McVean and Cardin, 2005) for simulating diversity at linked sites, the SFS estimation is based on the simulation of independent coalescent gene trees.</p>
<p>As compared to the first (but unpublished) version of <rs id="a12965802" type="software">fastsimcoal2 (fsc21)</rs>, several speed improvements have been performed. First, multi-threading has been introduced using the <rs id="a12965803" type="software">openMP</rs> framework ( <rs id="a12965804" type="url" corresp="#a12965803">https://gcc.gnu.org/onlinedocs/libgomp/</rs>), allowing one to distribute independent simulations over several threads (-c option). Second, <rs id="a12965805" type="software" subtype="component" corresp="#a12965807">icsilog</rs> <rs id="a12965827" type="bibr">(Vinyals and Friedland, 2008)</rs>, a fast approximation of the log function (used to generate exponentially distributed coalescent times) is now used in <rs id="a12965807" type="software" subtype="environment">fsc2</rs>, the precision of which can be specified by the user (--logprecision option). Full precision is used by default (logprecision 23), but computing speed can be improved by 10-25% by slightly lowering the precision (e.g. --logprecision 18)(see Fig. 1B-D). We have also optimized the simulation of large recombining chromosomes, obtaining a &gt; 5Â gain for the simulation of 1 Gblong chromosomes (Fig. 1A). Finally, we have optimized the simulations of samples drawn from large, subdivided populations (e.g. in a 2D stepping-stone), also leading to a drastic speed gain (6Â-60Â) for such simulations (see Fig. 1C andD).</p>
<p>Command line options now include the possibility to define initial parameter values for parameter estimation (--initvalues), which is useful when performing bootstrap confidence interval estimations. Finally, the 1D or 2D folded SFS can be computed in different populations using the --foldedSFS option, which simply folds the unfolded SFS irrespective of what is the overall minor allele among all populations, so as to provide compatibility with <rs id="a12965808" type="software">ANGSD</rs> <rs id="a12965828" type="bibr">(Korneliussen et al., 2014)</rs> folded SFS.</p>
<p>Several <rs id="a12965830" type="software" subtype="implicit">tools</rs> have been developed to facilitate the use of <rs id="a12965811" type="software">fsc2</rs> and the analysis of the outputs it produces (see <rs id="a12965812" type="url" corresp="#a12965830">http://cmpg.unibe.ch/soft ware/fastsimcoal2/additionalScripts.html</rs>). They include a shiny application and several <rs id="a12965813" type="language" corresp="#a12965816">bash</rs>, <rs id="a12965814" type="language" corresp="#a12965816">R</rs> and <rs id="a12965815" type="language" corresp="#a12965816">python</rs> <rs id="a12965816" type="software" subtype="implicit">scripts</rs> to 9i) prepare input files from VCFs, (ii) resample individuals in genomic blocks of arbitrary size for block bootstrap analyses, (iii) generate parametric bootstrap replicates, (iv) convert multidimensional SFS into a series of 1D and 2D SFS to visually compare observed and expected SFS, (v) identify the least well fitted SFS entries under a given model and (vi) visually inspect an evolutionary scenario embedded in an input (.par) file.</p>
<p><rs id="a12965817" type="software">fsc2</rs> is a very versatile coalescent simulator able to handle evolutionary scenarios of arbitrary complexity. It can also be used to estimate demographic parameters under similarly complex scenarios from the site frequency spectrum, in a very consistent way. It can now also be used to analyze geographically structured populations in a faster way than some spatially explicit simulators [e.g. <rs id="a12965818" type="software">SPLATCHE</rs><rs id="a12965819" type="version" corresp="#a12965818">3</rs> <rs id="a12965829" type="bibr">(Currat et al., 2019)</rs>], even though input files can still be very large as they can require an explicit definition of big migration matrices. The syntax of input file has been improved to build complex scenarios in a simpler and consistent way, eliminating the need of defining rules to establish a hierarchy among parameters. <rs id="a12965821" type="software">fsc2</rs> is restricted to the simulation of neutral markers, but it can have a wide range of applications from the simulation of whole recombining genomes with complex architectures, to the estimation of parameters in models including many populations exchanging arbitrary and changing numbers of migrants over time, or model comparisons via likelihood-ratio tests or AIC. It has been applied to a variety of organisms including humans (e.g. Malaspinas et al., 2016;Pouyet et al., 2018), animals (Armstrong et al., 2021;de Manuel et al., 2016;Marques et al., 2019Marques et al., , 2018;;Meier et al., 2017), plants (Gonza ´lez-Martı ´nez et al., 2017;Lu et al., 2019) or microbes (Montano et al., 2015;Va ´zquez-Rosas-Landa et al., 2020), and it can deal with ancient DNA samples and establish their relationships with modern samples (e.g. Sikora et al., 2019Sikora et al., , 2017)).</p>
<p><rs id="a12965822" type="software">fastsimcoal2</rs></p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f131081127"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T09:30+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>All pictures were processed by software <rs id="a12883968" type="software">FluoView</rs> ( <rs id="a12883969" type="publisher" corresp="#a12883968">Olympus</rs>).</p>
<p>Raw reads were trimmed and quality control was performed using <rs id="a12883970" type="software">TrimGalore!</rs> ( <rs id="a12883971" type="publisher" corresp="#a12883970">Babraham Bioinformatics</rs>) with default settings. Trimmed reads were mapped to the human genome version hg19 using <rs id="a12883972" type="software">TopHat
2</rs>
v <rs id="a12883973" type="version" corresp="#a12883972">2.0.14</rs> (<rs id="a12883974" type="bibr">47</rs>). CLIP-seq reads were mapped both with and without the transcripts annotation (Illumina iGenomes v. GRCh37). Both mappings were combined and mapping positions were selected based on minimal number of mismatches. CLIP-seq reads were categorized according to priority table (1. rRNA, 2. tRNA, 3. snoRNA, 4. snRNA, 5. miRNA, 6. mRNA exon, 7. mRNA intron, 8. mRNA unknown, 9. lincRNA, 10. repeat 11. antisense mRNA, 12. unannotated).</p>
<p>Differential gene and exon expression analysis was performed using <rs id="a12883975" type="software" subtype="environment">R</rs> packages <rs id="a12883976" type="software" subtype="component" corresp="#a12883975">DESeq2</rs> (<rs id="a12883977" type="bibr">48</rs>) and <rs id="a12883978" type="software" subtype="component" corresp="#a12883975">DEXSeq</rs> respectively according to documentation (<rs id="a12883979" type="bibr">49</rs>). Genes/exons with an adjusted P-value &lt; 0.05 were considered significantly DE. Top up/downregulated exons were selected based on ordered adjusted P-values. Numbers of exons were acquired by custom <rs id="a12883980" type="software" subtype="implicit" corresp="#a12883981">scripts</rs> from the <rs id="a12883981" type="software" subtype="environment">Ensembl</rs> annotation (GRCh37.75). <rs id="a12883982" type="software">MISO</rs> analysis was performed according to documentation (<rs id="a12883983" type="bibr">50</rs>). The top 200 inclusion/exclusion events were selected based on their respective Bayes factor. Splicing events with Bayes factor &gt; 10 were considered significant. Metagene analysis was performed with <rs id="a12883984" type="software" subtype="environment">CGAT</rs> tool <rs id="a12883985" type="software" subtype="component" corresp="#a12883984">bam2geneprofile.py</rs> (<rs id="a12883986" type="bibr">51</rs>) or custom <rs id="a12883987" type="software" subtype="implicit">scripts</rs>. GO analysis was performed using <rs id="a12883988" type="software" subtype="environment">R</rs> package <rs id="a12883989" type="software" subtype="component" corresp="#a12883988">GOSeq</rs> (<rs id="a12883990" type="bibr">52</rs>). RPKM calculations in features were done using <rs id="a12883991" type="software">bedtools</rs> (<rs id="a12883992" type="bibr">53</rs>). CLIPseq/RNA-seq coverage plots were calculated using <rs id="a12883993" type="software" subtype="component" corresp="#a12883994">cover-ageBed</rs> from the <rs id="a12883994" type="software" subtype="environment">bedtools</rs> toolset and visualized using custom <rs id="a12883995" type="software" subtype="implicit" corresp="#a12883996">scripts</rs> in <rs id="a12883996" type="software" subtype="environment">R</rs>. Gene models were generated using <rs id="a12883997" type="software" subtype="environment">R</rs> package <rs id="a12883998" type="software" subtype="component" corresp="#a12883997">GenomeGraphs</rs> (<rs id="a12883999" type="bibr">54</rs>).</p>
<p>Alternative poly(A) (APA) site usage was estimated using <rs id="a12884000" type="software">DaPars</rs> (<rs id="a12884001" type="bibr">55</rs>). APA events with adjusted P-value &lt; 0.05 were considered significant. For motif analysis, binding sites were selected based on the identification of crosslinking-induced deletions in the CLIP reads. Positions with &gt;5 reads coverage and at least 5% of deletion frequency were selected. K-mers were counted by ZOOPS (Zero or once per transcript) counting. Significance of motif enrichment was calculated by Fisher's exact test for each replicate separately, and P-values were combined by Fisher's method.</p>
<p>The constructs were transiently expressed in 293T WT and FTO KO cells, briefly, 1.5 g of the reporter construct DNA was transfected to 80% confluent 293T cells in 6-well culture plates using TURBOFECT following the manufacturer's instructions (Fermentas). The cells were harvested 24 h after transfection and total RNA was isolated with TriPure reagent according to the manufacturer's instructions. The isolated RNA was treated with DNAse (Turbo DNAse, Fermentas) and 2 g of RNA was used for reverse transcription (RT) with random hexamers and SuperScript III enzyme (ThermoFisher scientific). To assess the splicing pattern, primers annealing to the rat insulin exons 2 and 3 were used (see the Supplementary Table 1 for the sequences). RT-PCR products were resolved on agarose gels and signals were quantified by <rs id="a128840233" type="software">ImageJ</rs> (<rs id="a128840244" type="bibr">57</rs>).</p>
<p>To characterize the RNA targets bound by FTO, we performed cross-linking and immunoprecipitation (CLIP-seq) (46) of FTO in the human embryonic kidney HEK293 Flp-In T-REx cell line (293T). To ensure high specificity of the immunoprecipitation (IP), we prepared a stable 293T cell line with inducible expression of FLAG-tagged full-length FTO and optimized the immunoprecipitation (IP) conditions and RNAse I fragmentation (Supplementary Figure S1A andB). We performed three replicates of CLIP-seq followed by high-throughput sequencing on the Illumina platform, together with three replicates of RNA sequencing of the same cell line (Input). The CLIP-seq libraries yielded 33 922 806, 40 223 089 and 96 201 502 reads, respectively. After adapter trimming and collapsing of duplicate reads, we were able to map more than 70% of the reads in all three samples to the human genome version hg19 using <rs id="a12884002" type="software">tophat2</rs> (<rs id="a12884003" type="bibr">47</rs>) (762 879, 1 633 537 and 4 669 684 uniquely mapped, de-duplicated reads in three replicates). The distribution of CLIP reads in RNA classes was reproducible and most of the reads were mapped to mRNAs (42%), followed by ri-bosomal RNAs (rRNAs) (26%), and small nuclear RNAs (snRNAs) (4%) (Figure 1A, Supplementary Figure S1C).</p>
<p>Next, we asked whether mRNA binding by FTO or its demethylation activity could affect gene expression. We targeted exon 2 of FTO using a CRISPR-Cas9 system and generated a 293T FTO -/-cell line (FTO KO) with abolished expression of both alleles of the FTO gene (Figure 3A). FTO KO cells displayed slower growth rate compared to the 293T control cell line (Supplementary Figure S3A). To investigate changes in gene expression, we performed 2 × 125 bp paired-end whole transcriptome sequencing (RNA-seq) of cDNA libraries prepared from rRNA-depleted total RNA with high sequencing depth. Gene expression was estimated using the <rs id="a12884004" type="software" subtype="environment">R</rs> package <rs id="a12884005" type="software" subtype="component" corresp="#a12884004">DESeq2</rs> (<rs id="a12884006" type="bibr">48</rs>). We observed differential expression (DE) of more than 0.5log 2 fold in 846 protein coding genes (FDR &lt; 0.05), of which 677 were downregulated in FTO KO cells, and 169 were upregulated (Fig-</p>
<p>FTO was previously linked to regulation of alternative splicing of a single gene RUNX1T1, which has an important role in adipogenesis (19,21). Furthermore, our CLIP-seq and RNA-seq data point towards a function of FTO in nuclear mRNA processing. To examine the role of FTO in alternative splicing globally we carried out exon expres- sion analysis of FTO KO RNA-seq data with <rs id="a12884007" type="software">DEXSeq</rs> (<rs id="a12884008" type="bibr">49</rs>) and AS analysis with <rs id="a12884009" type="software">MISO</rs> (<rs id="a12884010" type="bibr">50</rs>). Both approaches revealed multiple differential splicing events relative to the control cell line (Table 1). Interestingly, these were predominantly exon-skipping events in FTO KO cells (957/1362 <rs id="a12884011" type="software">DEXSeq</rs>, 156/204 <rs id="a12884012" type="software">MISO</rs>; Table 1). We observed around 20% overlap of genes with significantly changed exon expression found by <rs id="a12884013" type="software">DEXSeq</rs> and significant difference of PSI estimated by <rs id="a12884014" type="software">MISO</rs>. A detailed view of a selected alternative splicing event (BRD8 gene) showing RNAseq read coverage across splice junction is represented in Figure 4A. Based on these analyses we selected six alternative splicing events and we were able to validate five of them using semi-quantitative PCR with primers annealing to flanking exons (Figure 4B).</p>
<p>To address whether FTO binding correlates with exon inclusion, we compared FTO CLIP-seq coverage around top 200 most significantly excluded alternative exons to the total set of 339881 cassette exons present in <rs id="a12884015" type="software">MISO</rs> splicing events database. Because <rs id="a12884016" type="software">MISO</rs> does not support replicates we pooled the three replicate libraries together and also performed the analyses separately for each replicate. Alternative splicing events were scored and selected from the pooled analysis. Individual AS events selected for validation were then examined in each replicate separately and reproducibility was tested by the t-test. We found the highest enrichment of FTO binding in both exons and introns adjacent to the excluded exons (Figure 4D,E). There was also a slight enrichment of FTO binding in the entire transcripts that contained excluded exons (Figure 4F). Conversely, we found that FTO binding was not significantly increased around exons that were more included in FTO KO (Supplementary Figure S4B). The CLIPseq results showed a strong enrich- ment of binding in the flanking introns of the BRD8 AS exon 20/21. Therefore, we used the reporter construct (Figure 4C) to try to narrow down the FTO target sequence. We selected a 53 nt long region in the downstream intron, that showed the highest FTO coverage and prepared mutant reporters lacking the whole region or replaced by a heterologous sequence (Supplementary Figure S4A). Interestingly, this 53 nt region contains two putative DRACH motifs. The mutant reporters showed higher exon inclusion than the wt form in FTO KO cells (Supplementary Figure S4A). A minor increase in exon inclusion from mutant constructs was also observed in control 293T cells, which is consistent with the model that absence of m6A marks inhibits exon inclusion in this AS event (Supplementary Figure S4A). In summary the minigene experiments supports our finding that the selected region is involved in AS of BRD8 exon 20/21.</p>
<p>On top of the changes in AS, the <rs id="a12884017" type="software">DEXSeq</rs> analysis revealed that last exons underwent differential expression more frequently than expected by chance (Table 1; P = 1.62 × 10 -233 , Fisher's exact test). Differentially expressed last exons were almost exclusively upregulated (860 upregulated vs 32 downregulated; FDR &lt; 0.05; Table 1, Figure 5A, B, P &lt; 2.2 × 10 -16 Mann-Whitney U-test), suggesting that FTO might play a role in regulation of expression and/or processing of mRNAs 3 ends. To gain further insights into the profile of expression change at the 3 mRNA termini, we performed a metagene analysis of 3818 transcript isoforms, which overlap with 892 DE 3 terminal exons in FTO KO. We observed a strong increase in RNA-seq coverage in regions ranging from stop codons and covering the whole 3 UTR in FTO KO compared to control cells (Supplementary Figure S5A).</p>
<p>To examine the significance of the last exon expression in FTO KO cells, we investigated whether the change in RNAseq coverage in FTO KO might be associated with the usage of alternative polyA sites (APA) using <rs id="a12884018" type="software">daPars</rs> (<rs id="a12884019" type="bibr">55</rs>) (Figure 5A). Consistently with <rs id="a12884020" type="software">DEXseq</rs> analysis, FTO KO cells showed pronounced higher usage of distal APAs comparing to the control cells (Figure 5D). To further support the involvement of APA in FTO KO cells, we plotted the ratio of FTO KO/WT RNAseq coverage around the annotated proximal polyA sites (58). We found that the change in coverage was strongly associated with the most proximal polyA site in the last exon (Supplementary Figure S6A), but not with the most proximal polyA site upstream of the last exon (Supplementary Figure S6B). This indicated, that FTO promotes usage of proximal APAs in a subset of genes. Therefore we tried to look whether these positions are bona fide cleavage sites. For that, we used the PAR-CLIP results of several 3 end processing factors, that were obtained from T293 cells (59) and plotted their binding around APA sites predicted by <rs id="a12884021" type="software">daPars</rs> (767 sites). We observed a discrete peak of binding of the Cleavage and Stimulatory Factor CSTF64 and Cleavage Factor I 68 at FTO-linked APAs (Supplementary Figure S7A). Altogether, these data support the direct role of FTO in selection of APA sites in human cell culture model.</p>
<p>Raw and processed CLIP-seq and RNA-seq data can be found in <rs id="a12884022" type="software">GEO</rs> database under accession code GSE79577.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f429414521"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T16:03+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Proteome Array Profiling Reveals Potentially Pathogenic Autoantibodies in MIS-C Several studies have proposed autoantibodies as an element of the immunopathology of Kawasaki disease (Sakurai, 2019). An external trigger, likely in the form of a virus, is indicated by epidemics of Kawasaki disease, and direct evidence of antiviral antibody responses, particularly targeting viral particles in inclusion bodies in the respiratory epithelium, have been reported (Rowley et al., 2008). Autoantibodies targeting endothelial cells can activate IL-6 production by such cells in vitro (Grunebaum et al., 2002), but a possible role of autoantibodies in MIS-C is unknown. To search for such autoantibodies in MIS-C, we screened serum samples from children with MIS-C (n = 12), Kawasaki disease (n = 28), SARS-CoV-2+ infection (n = 5), as well as healthy control children (n = 11). We probed serum samples against 9,341 human antigens from 7,669 unique human proteins using protein arrays (Zhu et al., 2001) (ProtoArray v5.1, PAH05251020, ThermoFisher, Waltham, MA). As expected, the autoantibody signals were low for the vast majority of antigenic targets (Landegren et al., 2016; Figure 7A). All autoantibody binding intensities across all samples tested are provided in Table S3. We ranked autoantibody targets using foldchange calculations between the MIS-C group and each of the other groups of samples and looked for enriched Gene Ontology (GO)-terms among the targets using <rs type="software" id="sB9999">gene set enrichment analysis</rs> (GSEA) (<ref type="bibr">Subramanian et al., 2005</ref>). There were 26 GO-terms that were enriched in MIS-C samples when compared to all other groups (Figure 7B), and these involved lymphocyte activation processes, phosphorylation signaling pathways, and heart development (Figure 7C). The latter was interesting given that myocarditis and impaired cardiac function are hallmarks of MIS-C clinical presentation. We studied individual autoantibody targets within this GO-term (Figure 7D) and identified endoglin, a glycoprotein expressed by endothelial cells and necessary for structural integrity of arteries to be differentially regulated among groups of samples (Figure 7E). Loss of endoglin leads to the disease, hereditary hemorrhagic telangiectasia, a disease characterized by multisystemic vascular dysplasia (McAllister et al., 1994). Several, but not all, of the MIS-C patients had elevated levels of autoantibodies targeting endoglin above the average levels seen in healthy controls with a couple of exceptions (Figure 7E). A subset of Kawasaki disease patients also had elevated levels of autoantibodies to endoglin (Figure 7E). Endoglin protein expression is seen predominantly in the vascular endothelium, with the heart muscle having the highest mRNA expression of S2.</p>
<p>Blood samples from KD and SARS-CoV-2+ patients were collected at the time of diagnosis and with regards to KD, always before Intravenous immunoglobulin administration. After ficoll, PBMCs and plasma samples were stored in liquid nitrogen or at À20 C, respectively, in Nunc Cryotubes (Merk KGaA, Darmstadt, Germany). Flow cytometry was performed for 30 patients, and isolated PBMCs were stained with LIVE/DEAD Fixable Near-IR Dead Cell Stain Kit (for 633 or 635 nm excitation, ThermoFisher, Waltham, Massachusetts, US) for 15 minutes at 4 C. Then, the cells were washed in wash buffer (phosphate-buffer saline with 1% bovine serum albumin) and stained for 30 minutes at 4 C with anti-hCD3 PE-CF594, anti-hCD4 BV510, anti-hCD25 PE, anti-hCD45RO-PerCP-Cy 5.5, anti-hCD27 V450, anti-hCD57 APC, anti-hCD185(CXCR5) BV605 (all from BD Biosciences, Milan, Italy), anti-hCD127 PE-Cy7, anti-hPD1 BV711 (all from Biolegend, San Diego, CA). Data acquired by CytoFLEX cytometer (Beckman Coulter, Milan, Italy) were analyzed by <rs type="software" id="sB">FlowJo</rs> software v.<rs type="version" corresp="#sB">10</rs> (<rs type="creator" corresp="sB">Treestar Software</rs>, Ashland, Oregon, USA).</p>
<p>Plasma protein analyses in Figure 2 were performed using samples from children with MIS-C (n = 13), children with Kawasaki (n = 28), adult CoV-2+ non-ICU (n = 7 subjects, 14 time points total), and adult CoV-2+ ICU (n = 10 subjects, 43 time points total). In order to compare Adult CoV-2+ cases with Kawasaki and MIS-C children, bridge normalization was used due to available bridge samples between assays regarding children. Bridge normalization was done with the function olink_normalization() from <rs type="software">OlinkAnalyze</rs> <rs type="software" subtype="environment">R</rs> package. A 30% LOD cutoff was used to filter out proteins for all Olink plates. Then both plates were filtered for matching proteins across. The normalized combined plate with children cases as well as the plate containing the Adult CoV-2+ cases were scaled within each set to unit variance (z-score) before merging. For the PCA and PC contribution plots, both were done using the library <rs type="software">factoextra</rs> and only the top 20 contributions were displayed in the contribution plot. Plasma protein analyses in Figure 4 were performed using samples from healthy children (n = 12), children with SARS-CoV-2 without hyperinflammation (n = 41), children with MIS-C (n = 21, pretreatment n = 11, post-treatment n = 7), and children with Kawasaki (n = 28). The four proteins (IL10, IL6, CCL11, IL5) that overlapped between Olink Immune Response and Inflammation panels were averaged. Proteins with &gt; 30% measurements below the threshold of detection were filtered out, resulting in 120 and 133 plasma proteins used for PCA in Figures 2 and4 respectively. Because Olink data from Figure 4 was run in two batches, bridge normalization was applied using function olink_normalization from <rs type="software" subtype="environment">R</rs> package <rs type="software">Olin-kAnalyze</rs>, followed by batch correction using the function removeBatchEffect from <rs type="software" subtype="environment">R</rs> package <rs type="software">limma</rs>. The analyzed Olink parameters for Figures 2 and4, and their mean NPX and standard deviations are provided (Tables S1 andS2). For analyses in Figure 5, seven paired pre/post treatment MIS-C samples were compared.</p>
<p>For each sample, sequencing reads were first mapped to the original library sequences with <rs type="software">Bowtie</rs>, and the number of reads of each peptide in the original library were counted with <rs type="software">SAMtools</rs>. A zero-inflated generalized Poisson distribution was applied to fit the frequency of peptides, and Àlog 10 ðpÞ was calculated for each peptide as the probability of enrichment. Peptides with Àlog 10 ðpÞ larger than 2.3 in both technical replicates were considered to be significantly enriched. Among the significantly enriched peptides, those appear in at least 3 of the beads samples were removed as nonspecific bindings. We also filtered out the peptides that were enriched in only one sample. The remaining enriched peptides were used to calculate the virus scores. To remove the hits caused by crossreactive antibodies, we first sorted the virus by their total number of enriched peptides in descending order. For each virus in this order, we iterated through all the enriched peptides and removed those that shared a sequence of more than 7 amino acids with any previously observed peptides in any virus of the same sample. The remaining enriched peptides were considered specific and their number is the virus score for the virus. Afterward, we filtered out the outlier viruses with Virus Scores &gt; 1 in only sample and negative Virus Scores in all others. We also filtered out viruses with Virus Scores &lt; 2 in all samples.</p>
<p>All analyses of autoantibody reactivities were performed using background-subtracted mean signal values of protein duplicates of the human IgG channel. Samples were quantile-normalized, and technical duplicates were averaged. For comparisons of autoantibody target reactivity between MIS-C and other groups (healthy control, CoV-2+ and Kawasaki), differential expression was run using deseq function of the <rs type="software" subtype="environment">R</rs> package <rs type="software">DESeq2</rs>. Ranked lists were obtained for each comparison to MIS-C (contrasts) and used for GSEA. <rs type="software">GSEA</rs> was run on the ranked list of unique targets using the gseGO() function of the <rs type="software">clusterProfiler</rs> <rs type="software" subtype="environment">R</rs> package.</p>
<p>Statistical comparisons between two groups on flow cytometric frequencies were performed with t test if both distributions were approximately normal or, conversely, with the Wilcoxon nonparametric test. In both cases, the normality of the distributions was tested with the D'Agostino-Pearson test. P values less than 0.05 were considered to be statistically significant. MOFA was performed with the <rs type="software" id="sC">MOFA+</rs> package (Argelaguet et al., 2019) (version <rs type="version" corresp="sC">1.0</rs>) in the <rs type="software" subtype="environment">R</rs> statistical environment. Data analysis was performed using <rs type="software" subtype="environment" id="sD">R</rs> (version <rs type="version" corresp="sD">3.6.2</rs>).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f286770478"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:44+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The analyses were carried out using Markov chain Monte Carlo simulations in <rs id="a12893800" type="software">WinBUGS</rs> version <rs id="a12893801" type="version" corresp="#a12893800">1.4.3</rs>. Vague prior distributions were assumed for all parameters (see appendix for more details). We modelled continuous and binary data simultaneously, assuming a mixture of normal and binomial likelihoods but modelling the underlying bias on the same scale. This method required re-expressing standardised mean differences as odds ratios. 15 To reduce risk of spurious findings, we defined a lower threshold of at least 10 meta-analyses for conducting an analysis.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f525043437"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:49+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>After a phonological analysis of the Narmouthis ostraca, I explored how the findings relate to the use of Greek in Egypt in general, using the resources provided by The <rs id="a12951681" type="software">Papyrological Navigator</rs>,<rs id="a12951682" type="bibr">3</rs> an open--access Internet platform currently holding ca. 70,000 Greek texts. I also sought parallel evidence from the treatment of Greek loanwords in Coptic. I gathered ca. 450 nonstandard variants of Greek loanwords in Coptic from the database of DDGLC (Database and Dictionary of Greek Loanwords in Coptic, FU Berlin), displaying the phonological phenomena described above. Coptic is the Egyptian language form relevant for this study for two reasons. First, it dates to ca. 2 nd century CE (i.e. the same period as O.Narm.: 1 st -- 3 rd c. CE). Second, it is the first form of Egyptian to employ vowel graphemes in writing. Comparisons between the phonological systems of Greek and Egyptian could not have been made in any sensible manner with the pre--Coptic stages of Egyptian, for obvious reasons. I present here the preliminary results further explored in Dahlgren (in prep.).</p>
<p>One aspect of the nonstandard language use often seen in Greek texts coming from Egypt is especially multicausal and opaque, namely the eventual merging of the Greek near--close front unrounded (&lt;ē&gt; /e̝ /), close--mid front rounded (&lt;oi&gt; /ø/) and close front rounded (&lt;y&gt; /y/) vowels into a single unrounded close front vowel, resulting in several graphemes being pronounced as [i]. This is a language--internal development for which an abundance of evidence is found already from the Classical period and from everywhere in the Greek--speaking world. Spelling variants based on this, however, were more regular in Egypt than elsewhere, as Coptic had only three front vowels (/i, e, ɛ/) to 6 The Trismegistos Text Irregularities Database -a phoneme--based search engine linked to the nonstandard attestations in the <rs id="a12951684" type="software">Papyrological Navigator (PN)</rs> -gives 938 instances of &lt;d&gt; being replaced by &lt;t&gt; and 492 of &lt;t&gt; being replaced by &lt;d&gt;, to give an example of the amount of similar nonstandard forms. &lt;ou&gt; /u/ for &lt;y&gt; /y/ is rarer, with 121 attestations, while there are 188 instances of &lt;y&gt; /y/ for &lt;ou&gt; /u/. This is the amount of nonstandard /d, t/ and /y, u/ etc. usages out of ca. 70,000 Greek texts, which gives some indication of the overall quantity of nonstandard spellings. Obtaining statistical information on the whole standard--nonstandard ratio is difficult and time--consuming even on smaller samples because only nonstandard forms are marked in the search engines, word forms in the <rs id="a12951685" type="software">PN</rs> and spelling graphemes in Trismegistos, so standard forms and phonemes would have to counted manually from the thousands of texts. In addition, editors of the texts in <rs id="a12951687" type="software">PN</rs> have differing principles for what they consider nonstandard features. A new phoneme--based search engine is under development, however which will increase the searchable number of nonstandard attestations. match the four of Greek (/i, y, e̝ , e/) (Gignac 1991, 187). With frequent fluctuation between &lt;ē, e, i&gt; it becomes evident that the qualities of /e/ and /i/varied significantly in many Greek texts written by Egyptians. One possible reason for this could be that, typically for Afroasiatic languages, vowel quality came to be conditioned by the articulatory characteristics of the adjacent consonant in Coptic phonology.</p>
<p>All searches have been performed in DDbDP (Duke Database of Documentary Papyri) in the <rs id="a12951688" type="software">Papyrological Navigator</rs>. Consequently, the text corpora are referred to by the abbreviations used in DDbDP.</p>
<p>The <rs id="a12951690" type="software">papyrological Navigator</rs> is an open--access network platform that hosts ca. 60,000 Greek texts and can be found at this web address: http://papyri.info. The searches for this article were performed in DDbDP. The Trismegistos platform can be found at this web address: http://www.trismegistos.org/textirregularities/texirr_type_list.php</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f567567942"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:21+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Genomic analyses are sensitive to contamination in public databases caused by incorrectly labeled reference sequences. Here, we describe <rs id="a12972886" type="software">Conterminator</rs>, an efficient method to detect and remove incorrectly labeled sequences by an exhaustive all-against-all sequence comparison. Our analysis reports contamination of 2,161,746, 114,035, and 14,148 sequences in the RefSeq, GenBank, and NR databases, respectively, spanning the whole range from draft to "complete" model organism genomes. Our method scales linearly with input size and can process 3.3 TB in 12 days on a 32-core computer. <rs id="a12972887" type="software">Conterminator</rs> can help ensure the quality of reference databases. Source <rs id="a12972888" type="software" subtype="implicit">code</rs> (GPLv3): https://github.com/martin-steinegger/conterminator</p>
<p>Fig. 1 How contamination occurs and how <rs id="a12972889" type="software">Conterminator</rs> detects it. a DNA extraction from an organism (red) is imperfect and often introduces contamination by other species (violet). DNA sequencing then generates short reads that are assembled into longer contigs. Contaminated DNA is typically assembled into separate, small contigs, but sometimes is erroneously included in the same contigs as DNA from the source organism. Contigs may also be linked by scaffolding, which can produce scaffolds containing a mixture of different species. Final assemblies are submitted to GenBank, and higher-quality assemblies are entered in RefSeq. b <rs id="a12972890" type="software">Conterminator</rs> detects contamination in proteins and nucleotide sequences across kingdoms, e.g., bacterial contaminants in plant genomes. The following describes the nucleotide contamination detection workflow. (1) We take taxonomically labeled input sequences and cut them into non-overlapping segments of length 1000 and extract a subset of k-mers. (2) We group the k-mers by sorting them and compute ungapped alignments between the first and all succeeding sequences per group. (3) We extract each region of the first sequence that has an alignment to other kingdoms that is longer than 100 nucleotides with a sequence identity greater than 90 %. We perform an exhaustive alignment of the input sequence segments against the multi-kingdom regions. (4) We reconstruct contig lengths within scaffolds by searching for the scaffold breakpoints (indicated by N characters in the DNA sequence) on the left and right side from the alignment start and end position. We predict that contamination is present if an alignment hits a contig that is shorter than 20 kb that aligns to a different kingdom with an contig length longer than 20 kb</p>
<p>To combat the contamination issue, NCBI (the home of GenBank) applies two filtering protocols for detection of contaminated fragments. First, <rs id="a12972891" type="software">VecScreen</rs> <rs id="a12972892" type="bibr">[7]</rs> is used to detect synthetic sequences (vectors, adapters, linkers, primers, etc.), and second, <rs id="a12972893" type="software">BLAST</rs> <rs id="a12972894" type="bibr">[8]</rs> alignments against common contaminants identify a broader array of contaminating sequences. Despite these filters, contamination still occurs, and its detection remains challenging [9,10].</p>
<p>Because humans are always present in sequencing labs, Homo sapiens continues to be a major source of contamination for genome projects. Contaminating pieces of human DNA occasionally remain in published genomes [11] despite automated searches. A recent study, for example, showed that thousands of human DNA fragments can be found in draft bacterial genomes and that many of these have been erroneously translated and annotated as proteins [10]. However, many other species [12][13][14][15][16] also cause contamination. Systematic approaches to detect contamination are limited by computational costs of comparing every submitted genome against all other known genomes. For example, a <rs id="a12972895" type="software">BLAST</rs> all-against-all comparison of the RefSeq database [17], which has a size of 1.5 Tb, would take ≈ 30,000 CPU years. Faster alignment methods such as <rs id="a12972896" type="software">Minimap2</rs> <rs id="a12972897" type="bibr">[18]</rs> or <rs id="a12972898" type="software">Bowtie2</rs> <rs id="a12972899" type="bibr">[19]</rs> will take less time, but will still suffer from the quadratic complexity of this comparison. Other fast methods such as <rs id="a12972900" type="software">Mash</rs> <rs id="a12972901" type="bibr">[20]</rs> and <rs id="a12972902" type="software">sourmash</rs> <rs id="a12972903" type="bibr">[21]</rs> can compare genomes more quickly, but are not suited for finding small contaminating sequencing within a larger genome.</p>
<p>We present <rs id="a12972904" type="software">Conterminator</rs> (Fig. 1b), a fast method for detecting contamination in nucleotide and protein databases by computing local alignments across taxonomic kingdoms. It utilizes the linear-time all-against-all comparison algorithm from <rs id="a12972905" type="software">Linclust</rs> <rs id="a12972906" type="bibr">[22]</rs> followed by exhaustive alignments using <rs id="a12972907" type="software">MMseqs2</rs> <rs id="a12972908" type="bibr">[23]</rs>. This enables us to process huge nucleotide and protein sequence sets on a single server. We applied this method to quantify the current state of contamination in the nucleotide databases Genbank [1] and RefSeq [17], and in the comprehensive NR protein database [1].</p>
<p>Figure 2 summarizes the contamination found by <rs id="a12972909" type="software">Conterminator</rs> in RefSeq (Fig. 2a,b) and GenBank (Fig. 2c,d). Processing the 1.5 and 3.3 TB in RefSeq and GenBank took 5 and 12 days on a single 32-core machine with 2 TB of main memory. <rs id="a12972910" type="software">Conterminator</rs> reported 114,035 and 2,161,746 contaminated sequences affecting 2767 and 6795 species in Ref-Seq and GenBank, respectively. Identifiers of the contaminated sequences are available in Additional files 1 and 2: Listing S1. In GenBank, over 95 % of contamination occurred in eukaryotic genomes. Eukaryotic genomes tend to be much more fragmented due to their larger genome sizes and higher repetitive content (as compared to prokaryotes), and many of the smaller contigs in eukaryotic genome assemblies suffer from contamination.</p>
<p>Our method also detected a bacterial contaminant in the C. elegans reference genome, in chromosome X (GenBank accession NC_003284.9). A segment spanning positions 5907856-5912087 of the C. elegans sequence aligns perfectly to multiple strains of E. coli (Fig. 3b). To check whether this might be a false positive reported by our method, we downloaded the raw Illumina reads used for a more-recent assembly of the same C. elegans strain (SRR003808 and SRR003809) and aligned them against the chromosome X assembly (NC_003284.9) using <rs id="a12972911" type="software">Bowtie2</rs> <rs id="a12972912" type="bibr">[19]</rs>. Only six reads (30 bp each) aligned in this region. In contrast, the average coverage over the rest of the chromosome was ∼ 99.8. This indicates that the E. coli sequence was indeed a contaminant. To corroborate the contamination further, we looked at a recent assembly of C. elegans that used a combination of long and short reads [25]. We aligned their assembly of chromosome X (GenBank accession UNSB01000006.1 ¸) against the current reference and found that in this newer assembly, the E. coli region is not present. This strongly suggests that the C. elegans reference genome contains a ∼ 4-kb insertion of E. coli contamination.</p>
<p>We detected that 19.4 % of the contaminated RefSeq contigs contain protein annotations and encode a total of 47,943 proteins. A previous study [10] reported 3437 spurious bacterial proteins that originate from human repeats that have contaminated bacterial genome assemblies. We aligned these sequences against our set using <rs id="a12972913" type="software">MMseqs2</rs>, enforcing a 80 % alignment coverage of the shorter sequence (--comp-bias-corr 0 -mask 0 --cov-mode 5 -c 0.8), and discovered that our set contains 62 % of the previously reported proteins.</p>
<p>We clustered the proteins using <rs id="a12972914" type="software">MMseqs2</rs> at a 95 % sequence identity, enforcing a bi-directional coverage of 95 % (cluster --min-seq-id 0.95 -c 0.95). This resulted in 3339 clusters that covered 12,494 sequences. The remaining sequences were singleton clusters. The largest cluster consists of 185 bacterial proteins, all of which are located on contigs shorter than 1 kb, and the proteins are widely spread among multiple phyla in the bacterial kingdom. Despite the long evolutionary distance, 166 of the sequences are 100 % identical to each other and the remaining are at least 95 % identical, suggesting that all of them represent contaminants (see Fig. 4). All short bacterial contigs containing the 185 proteins align to multiple positions in the domestic sheep Ovis aries genome; the fragments align to chromosome 15 (NC_040266.1) with a sequence identity greater than 94 % with nearly complete coverage.</p>
<p><rs id="a12972915" type="software">Conterminator</rs> can be used to analyze protein sequences. It clusters proteins [22] at 95 % sequence identity, while requiring at least 95 % sequence overlap. It reports clusters containing multiple kingdoms, using the same kingdom definition as for the nucleotide comparison. We predict that the kingdom with fewer members in the cluster is contaminated, e.g., if a cluster contains 100 proteins, and 99 represent animals while 1 represents bacteria, then the bacterial protein likely originates from a contaminated genome.</p>
<p>With the rapid and ongoing increase in the number of novel genomes sequenced every year, the number and variety of contaminating sequences continue to increase as well, presenting challenges for alignment-based methods to detect contamination. <rs id="a12972916" type="software">Conterminator</rs>'s efficiency means that it can be used routinely to detect new contamination, even on the largest databases.</p>
<p><rs id="a12972917" type="software">Conterminator</rs> detects cross-kingdom alignments and predicts contamination. It builds upon existing modules of <rs id="a12972918" type="software">MMseqs2</rs> <rs id="a12972919" type="bibr">[23]</rs>, which it extends for use in contamination detection.</p>
<p><rs id="a12972920" type="software">Conterminator</rs> identifies regions in genome sequences that align to genomes from other kingdoms with a minimum length of at least 100 nucleotides and a sequence identity threshold of at least 90 %. With very few exceptions, DNA sequences from different kingdoms should not be aligned at all, and sequences that match at this level of identity are strong candidates for contaminants. Exceptions to this rule include recent horizontal gene transfer events, but these are very rare.</p>
<p>Because modern sequence databases are very large, we cannot use a naive all-against-all alignment, which would entail a quadratic number of comparisons. Therefore, we used a similar strategy to <rs id="a12972921" type="software">Linclust</rs> <rs id="a12972922" type="bibr">[22]</rs> to reduce the computational cost to a linear number of comparisons. We reworked the algorithm to support nucleotide sequences, since <rs id="a12972923" type="software">Linclust</rs> was originally built to cluster protein sequences.</p>
<p><rs id="a12972924" type="software">Conterminator</rs> first cuts all sequences into fragments of length 1000 and records their start positions. For each fragment, we extract m canonical k-mers (default m = 100) of length 24 with the lowest hash value and write them into an array. (We use the hash function defined in [22]; see Supplementary Figure 5.) We store the k-mer in 8 bytes, with the most significant bit indicating whether the k-mer is reversed, sequence identifiers (4 bytes), its length ( 2 bytes), and its position j in the genomic sequence ( 2 bytes). We sort the array by k-mer, length, and sequence identifiers. For each k-mer group, we assign all sequences to the longest sequence with the lowest sequence identifier c by overwriting their k-mer with the identifier of c and their position with the diagonal ij respecting the strand directionality. We sort the array again with the previous criteria so that all sequences with same assignment are in a consecutive block. We write each block's central sequence identifier, assigned sequence, strand, and diagonal to hard disk while only keeping the diagonal with the most k-mer matches per sequence.</p>
<p>We perform a one-dimensional dynamic programming ungapped alignment (using the <rs id="a12972925" type="software" subtype="environment">MMseqs2</rs> command "<rs id="a12972926" type="software" subtype="component" corresp="#a12972925">rescorediagonal --rescore-mode 2</rs>") on each diagonal. We assign matches a score of 2 and mismatches a score of -3 bits and compute an E value using ALP [29]. We compute the sequence identity by dividing the number of identical positions by the number of aligned positions. We filter out all hits that are shorter than 100 bases, or that have a sequence identity below 90 %, or that have an E value above 10 -3 . We compute the alignment start positions by adding the start position of the fragment to the alignment coordinates (<rs id="a12972927" type="software" subtype="environment">MMseqs2</rs> command "<rs id="a12972928" type="software" subtype="component" corresp="#a12972927">offsetalignment</rs>").</p>
<p>Our detection method might miss alignments because it does not extract all k-mers and because it uses 24-mers rather than shorter k-mers. After the previous steps, we perform an exhaustive alignment of the sequence fragments against the extracted potential contamination sequences (and their respective reverse complements) using <rs id="a12972929" type="software">MMseqs2</rs>. The search is performed by using the two modules prefilter and rescorediagonal. The prefilter program masks out low complexity regions and short tandem repeats in the potential contaminants using <rs id="a12972930" type="software">tantan</rs> <rs id="a12972931" type="bibr">[31]</rs> and detects all consecutive double 15mer hits. We rescore the detected diagonals again with the rescorediagonal module, enforcing a minimal alignment length of 100 and a minimal sequence identity of √ 0.9. The square root of the sequence identity ensures that no pair of sequences is greater than 90 % different from each other.</p>
<p>A large majority of contamination occurs as small contigs (see Fig. 2 in Breitweiser et al. [10]). <rs id="a12972932" type="software">Conterminator</rs> uses this property to help it identify contamination based on the length distribution of sequences from each kingdom. By default, it only calls a sequence a contaminant if the sequence is shorter than 20 kb and if it aligns to a sequence in another kingdom that is longer than 20 kb. Note that in the rare cases where a contaminating sequence is longer than 20 kb, our method will fail to identify it. However, this prevents us from labeling recent horizontal gene transfer events as contamination.</p>
<p><rs id="a12972933" type="software">Conterminator</rs> can also detect protein sequence contamination using cross-kingdom analysis. It clusters proteins using <rs id="a12972934" type="software">Linclust</rs> <rs id="a12972935" type="bibr">[22]</rs> with a bidirectional length overlap of 95 % and a sequence identity of 95 % (--min-seq-id 0.95 -c 0.95 --cov-mode 0 -a). It reports every cluster with cross-kingdom members. For each contaminated cluster, it counts how often each kingdom occurs and reports the least abundant kingdom as <rs id="a12972936" type="software">MMseqs2</rs> <rs id="a12972937" type="bibr">[23]</rs> 333546</p>
<p><rs id="a12972938" type="software">KrakenUniq</rs> <rs id="a12972939" type="bibr">[32]</rs> 5c0019</p>
<p><rs id="a12972940" type="software">Pavian</rs> <rs id="a12972941" type="bibr">[33]</rs> 81d784</p>
<p><rs id="a12972942" type="software">Jalview</rs> <rs id="a12972943" type="bibr">[34]</rs> 2.11.0</p>
<p>The versions for <rs id="a12972944" type="software">MMseqs2</rs>, <rs id="a12972945" type="software">KrakenUniq</rs>, and <rs id="a12972946" type="software">Pavian</rs> are the first 6 characters of the git commit. For databases, we list the date at which the data was downloaded the one that is contaminated. It also reports kingdoms with equal abundance; however, in those cases, it cannot predict the contaminated entry. Using only abundance without concern for length may lead to incorrect directionality calls. For example, human repeats cause contamination in multiple bacterial genomes [2]. In this case, abundance-based directionality prediction would wrongly call the human genome to be contaminated.</p>
<p>We created the Sankey plots using the <rs id="a12972947" type="software" subtype="component" corresp="#a12972948">krakenuniq-report</rs> tool from <rs id="a12972948" type="software" subtype="environment">KrakenUniq</rs> <rs id="a12972949" type="bibr">[32]</rs> to create a Kraken-style report from our predicted contaminations. The visualization was done using <rs id="a12972950" type="software">Pavian</rs> <rs id="a12972951" type="bibr">[33]</rs> extracted as SVG and colored by <rs id="a12972965" type="software">Inkscape</rs>. The multiple alignment was created by <rs id="a12972952" type="software" subtype="environment">MMseqs2</rs> <rs id="a12972953" type="software" subtype="component" corresp="#a12972952">result2msa</rs> and visualized using <rs id="a12972954" type="software">Jalview</rs> <rs id="a12972955" type="bibr">[34]</rs>.</p>
<p><rs id="a12972956" type="software">Conterminator</rs> <rs id="a12972957" type="bibr">[35]</rs> is implemented in <rs id="a12972958" type="language" corresp="#a12972956">C++</rs> and its open source licensed as GPLv3 and available at <rs id="a12972959" type="url" corresp="#a12972956">https://github.com/ martin-steinegger/conterminator</rs>. The version to reproduce the results is available under https://doi.org/10.5281/ zenodo.3750825 [36]. Commands to rerun the analysis of RefSeq and NR are in Additional file 3: Listing S1. The list of contamination for GenBank (genbank.gz), NR (nr.gz)), and RefSeq (refseq.gz) are available at: ftp://ftp.ccb.jhu.edu/pub/data/conterminator/ and https://figshare.com/projects/Conterminator/77346 [37].</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f327178461"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:32+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Since their origin, DNNs have been developed in a large number and a very diverse types of models in order to achieve high accuracy. The first DNN model to become famous was LeNet [28], a Convolutional Neural Network (CNN) that was used to recognize handwritten digits. The real boom for CNNs, which are the most widely used for object detection and recognition, came in 2012 when <rs type="software" id="a1">AlexNet</rs> <rs type="bibr">[31]</rs> won the ILSVRC competition [32] by outperforming the earlier methods. Since then, also thanks to the increased availability of computational hardware and memory resources, the DNN models have become more and more complex and precise. Table 1 outlines a timeline of the models that have become more popular, describing the innovations introduced compared to previous models.</p>
<p>Among the different available technologies, CPU cores are the least used for DNNs inference and training. CPUs have the advantage of being easily programmable to perform any kind of task. Still, their throughput is limited by the small number of cores and, therefore, by the small number of operations executable in parallel. Figure 7 compares the number of cores of CPUs and GPUs. The Intel Xeon Platinum 9222, a high-end processor used in servers with price over USD 10,000, has a number of floating-point operations per second per Watt (FLOPS/W) similar to the FLOPS/W of the 2014 Nvidia GT 740 GPU with price below USD 100 (∼12GFLOPS/W). High-end GPUs, with FLOPS/W in the order of TERAs, significantly surpass any CPU. However, some attempts have been recently made to accelerate DNNs deployment (inference in particular) on CPUs. At instruction level, Intel introduced DL Boost, a set of features that include AVX-512 Vector Neural Network Instructions (AVX-512-VNNI), part of AVX-512 Instructions [49], to accelerate CNNs algorithms, and Brain floating-point format (bfloat16) [50]. Brain floating-point format is a 16-bit format that uses a floating radix point and has a dynamic range similar to that of the 32-bit IEEE 754 single-precision floating-point format. bfloat16 is also supported by <rs id="a12965709" type="software">ARMv</rs> <rs id="a12965710" type="version" corresp="#a12965709">8.6-A</rs> and is included in <rs id="a12965730" type="software">AMD's
ROCm</rs> libraries. For what concerns the ML libraries, <rs id="a12965712" type="publisher" corresp="#a12965713">Intel</rs> has created <rs id="a12965713" type="software">BigDL</rs> <rs id="a12965731" type="bibr">[51]</rs>, a distributed deep learning library for DNNs algorithms acceleration on CPU clusters. There is also an Intel distribution of <rs id="a12965715" type="software">Caffe</rs> <rs id="a12965732" type="bibr">[52]</rs>, a popular deep learning framework, targeting Intel Xeon processors. Comparison of the number of CPU cores and GPUs. CPUs and GPUs models have been selected for different targets, e.g., personal computers or servers, and different price ranges. For the CPUs, the gray and black lines correspond to the minimum and the maximum cores of a family, respectively. For the GPUs, the black lines represent the number of CUDA cores, and the gray line represents the Tensor cores present in the Nvidia Tesla V100 only.</p>
<p>GPUs are the current workhorses for DNNs' inference and especially training. They contain up to thousands of cores (see Figure 7) to work efficiently on highly-parallel algorithms. Matrix multiplications, the core operations of DNNs, belong to this class of parallel algorithms. Among the GPUs' producers, Nvidia can be considered the winner of the AI challenge. In fact, the most popular DL frameworks, such as <rs id="a12965717" type="software">TensorFlow</rs> <rs id="a12965733" type="bibr">[53]</rs>, <rs id="a12965719" type="software">PyTorch</rs> <rs id="a12965734" type="bibr">[54]</rs>, or <rs id="a12965721" type="software">Caffe</rs> <rs id="a12965735" type="bibr">[52]</rs>, support execution on Nvidia GPUs through the <rs id="a12965723" type="publisher" corresp="#a12965724">Nvidia</rs> <rs id="a12965724" type="software">cuDNN</rs> library <rs id="a12965736" type="bibr">[55]</rs>, a GPU-accelerated library of primitives for DNNs with highly-optimized implementations of standard layers. DL frameworks allow to describe very complex neural networks in a few lines of code and run them on GPUs without needing to know GPU programming. <rs id="a12965726" type="software" subtype="component" corresp="#a12965727">cuDNN</rs> is part of <rs id="a12965727" type="software" subtype="environment">CUDA-X</rs> AI <rs id="a12965737" type="bibr">[56]</rs>, a collection of <rs id="a12965729" type="publisher" corresp="#a12965727">Nvidia</rs>'s GPU acceleration libraries that accelerate DL and ML.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f478783691"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:17+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Background: Mortality from COVID-19 shows a strong relationship with age and pre-existing medical conditions, as does mortality from other causes. We aimed to investigate how specific factors are differentially associated with COVID-19 mortality as compared to mortality from causes other than COVID-19. Methods: Working on behalf of NHS England, we carried out a cohort study within the <rs id="a12972808" type="software">OpenSAFELY</rs> platform. Primary care data from England were linked to national death registrations. We included all adults (aged 18 years) in the database on 1 st February 2020 and with &gt;1 year of continuous prior registration; the cutoff date for deaths was 9 th November 2020. Associations between individual-level characteristics and COVID-19 and non-COVID deaths, classified according to the presence of a COVID-19 code as the underlying cause of death on the death certificate, were estimated by fitting age-and sex-adjusted logistic models for these two outcomes. Findings: 17,456,515 individuals were included. 17,063 died from COVID-19 and 134,316 from other causes. Most factors associated with COVID-19 death were similarly associated with non-COVID death, but the magnitudes of association differed. Older age was more strongly associated with COVID-19 death than non-COVID death (e.g. ] respectively for 80 vs 50-59 years), as was male sex, deprivation, obesity, and some comorbidities. Smoking, history of cancer and chronic liver disease had stronger associations with non-COVID than COVID-19 death. All non-white ethnic groups had higher odds than white of COVID-19 death (OR for Black: 2.20 [1.96-2.47], South Asian: 2.33 [2.16-2.52]), but lower odds than white of non-COVID death (Black: 0.88 [0.83-0.94], South Asian: 0.78 [0.75-0.81]). Interpretation: Similar associations of most individual-level factors with COVID-19 and non-COVID death suggest that COVID-19 largely multiplies existing risks faced by patients, with some notable exceptions. Identifying the unique factors contributing to the excess COVID-19 mortality risk among non-white groups is a priority to inform efforts to reduce deaths from COVID-19.</p>
<p>To our knowledge, no study to date has directly compared factors associated with COVID-19 versus non-COVID deaths in the same cohort. We aimed to address this by conducting parallel analyses of COVID-19 and non-COVID death outcomes using population-based data from England within the <rs id="a12972809" type="software">OpenSAFELY</rs> platform.</p>
<p>A retrospective cohort study was carried out within <rs id="a12972810" type="software">OpenSAFELY</rs>, a new data analytics platform in England created to address urgent COVID-19 related questions, which has been described previously [4]. We used routinely-collected electronic data from primary care practices using <rs id="a12972811" type="publisher" corresp="#a12972812">TPP</rs> <rs id="a12972812" type="software">SystmOne</rs> software, covering 2816 practices and approximately 40% of the population in England, linked to Office of National Statistics (ONS) death registrations. We included all adults (aged 18 years or over) alive and under follow-up on 1 st February 2020, and with at least one year of continuous GP registration prior to this date, to ensure that baseline data could be adequately captured. We excluded people with missing age, sex, or index of multiple deprivation, since these are likely to indicate poor data quality. For a secondary analysis of deaths prior to the pandemic, a second cohort was extracted comprising all adults alive and under follow-up on 1 st February 2019 and with at least one year of GP registration prior to that date (hereafter referred to as the "2019 cohort"). Finally, we compared directly those that died due to COVID-19 and those that died from other causes to assess associations between individual level factors and cause of death (analogous to a case-control analysis).</p>
<p>Information on all clinical covariates was obtained by searching <rs id="a12972813" type="publisher" corresp="#a12972814">TPP</rs> <rs id="a12972814" type="software">SystmOne</rs> records prior to 1 st February 2020 (or prior to 1 st February 2019, for the 2019 secondary analysis cohort) for specific coded data, based on a subset of SNOMED-CT mapped to Read version 3 codes. All codelists, along with detailed information on their compilation are available at https://codelists.opensafely.org for inspection and re-use by the wider research community.</p>
<p>NHS England is the data controller; TPP is the data processor; and the key researchers on <rs id="a12972815" type="software">OpenSAFELY</rs> are acting on behalf of NHS England. <rs id="a12972816" type="software">OpenSAFELY</rs> is hosted within the TPP environment which is accredited to the ISO 27001 information security standard and is NHS IG Toolkit compliant; [15,16] patient data are pseudonymised for analysis and linkage using industry standard cryptographic hashing techniques; all pseudonymised datasets transmitted for linkage onto <rs id="a12972817" type="software">OpenSAFELY</rs> are encrypted; access to the platform is via a virtual private network (VPN) connection, restricted to a small group of researchers who hold contracts with NHS England and only access the platform to initiate database queries and statistical models. All database activity is logged. No patient-level data leave the platform; only aggregate statistical outputs leave the platform environment following best practice for anonymisation of results such as statistical disclosure control for low cell counts [17]. The <rs id="a12972818" type="software">OpenSAFELY</rs> platform adheres to the data protection principles of the UK Data Protection Act 2018 and the EU General Data Protection Regulation (GDPR) 2016. In March 2020, the Secretary of State for Health and Social Care used powers under the UK Health Service (Control of Patient Information) Regulations 2002 (COPI) to require organisations to process confidential patient information for the purposes of protecting public health, providing healthcare services to the public and monitoring and managing the COVID-19 outbreak and incidents of exposure [18]. Taken together, these provide the legal bases to link patient datasets on the <rs id="a12972819" type="software">OpenSAFELY</rs> platform. This study was approved by the Health Research Authority (REC reference 20/LO/0651) and by the LSHTM Ethics Board (ref 21863).</p>
<p>Although individual level factors were generally similarly associated with COVID-19 and non-COVID death, some of the observed differences were striking, including the discrepant effects of ethnicity on the two outcomes. A lower overall mortality risk in Black, South Asian and other minority ethnic groups has been observed before in an analysis of linked death registration data from 2001-2013 in Scotland, with suggested reasons including self-selection of healthy individuals among migrants, and healthier lifestyles and behaviours among these groups [19]. A study using pre-pandemic data from UK Biobank also observed a reduced risk of mortality in non-white ethnic groups that was consistent for both infectious and non-infectious deaths; [10] UK Biobank participants are not representative of the broader UK population, with evidence of a healthy volunteer selection bias, [20] and it is possible that this bias may have operated more strongly in non-white ethnic groups. Nevertheless the evidence from both the internal comparison in the present study, and related data from other studies, suggests that the higher risk of poor COVID-19 outcomes reflects unique features of the pandemic rather than a generalised higher risk of death in non-white groups. Reasons might include a high likelihood of working in at-risk occupations with high exposure risk, such as health and social care, hospitality and public transportation; and a high likelihood of living in large, high-density or multigenerational households, which might act individually or in combination to increase the risk of infection, and thus the overall risk of COVID-19 death, particularly if a high exposure risk in younger people leads to increased infection in older people via households and community settings [21]. Changes between wave 1 and 2 in the patterns of results for ethnicity are consistent with data recently published by ONS, [22] as well as a detailed analysis of <rs id="a12972820" type="software">OpenSAFELY</rs> focussing on associations between ethnicity and COVID-19 outcomes, [5] and suggest that ethnic differences in risk of COVID-19 death may be largely driven by exposure risk, which is more likely to have changed rapidly over time than susceptibility to severe disease.</p>
<p>BG conceived the <rs id="a12972821" type="software">OpenSAFELY</rs> platform and the approach. BG and LS lead the project overall and are guarantors. KB and SJWE led on study design. KB did the analysis and wrote the first draft. SB led on software development. AM led on information governance. Other contributions were À data curation -CB, JP, JC, SH, SB, DE, PI and CEM; disease category conceptualisation and codelists -KB CTR BM CB JC CEM AJW HIM IJD HJC JP; statistical analysis code: KB, EW; ethical approvals -HJC EW LS BG; software -SB, DE, PI, AJW, WJH, CEM, CB, FH, JC; writing (reviewing and editing) -KB, CTR, LT, BM, AS, AM, RME, CEM, IJD, SJWE, DS, LS, BG. All authors were involved in design and conceptual development and reviewed and approved the final manuscript.</p>
<p>All data were linked, stored and analysed securely within the <rs id="a12972822" type="software">OpenSAFELY</rs> platform <rs id="a12972823" type="url" corresp="#a12972822">https://opensafely.org/</rs>. Data include pseudonymized data such as coded diagnoses, medications and physiological parameters. No free text data are included. All <rs id="a12972824" type="software" subtype="implicit">code</rs> is shared openly for review and re-use under MIT open license (<rs id="a12972825" type="url" corresp="#a12972824">https://github. com/opensafely/covid-vs-noncovid-deaths-research</rs>). Detailed pseudonymised patient data are potentially re-identifiable and therefore not shared. We rapidly delivered the <rs id="a12972826" type="software">OpenSAFELY</rs> data analysis platform without prior funding to deliver timely analyses on urgent research questions in the context of the global Covid-19 health emergency: now that the platform is established we are developing a formal process for external users to request access in collaboration with NHS England; details of this process will be published on Open-SAFELY.org.</p>
<p>Patients were not formally involved in developing this specific study design that was developed rapidly in the context of a global health emergency. We have developed a publicly available website <rs id="a12972827" type="url" corresp="#a12972828">https://opensafely.org/</rs> through which we invite any patient or member of the public to contact us regarding this study or the broader <rs id="a12972828" type="software">OpenSAFELY</rs> project.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f394996474"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:21+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>One exception investigated the impact of sunglasses and masks on emotion categorization [70]. In this study, sunglasses were added onto a validated set of emotional faces using <rs id="a12971417" type="software" subtype="implicit">image editing software</rs>. The face mask condition, however, consisted of a non-realistic grey ellipse being added to the mouth region of the faces. Although occluding some of the same region as a realistic face mask, the ellipse did not cover the nose in most of the images presented. The authors [70] found that adults classified each emotional expression (happy, sad, surprise, fear and anger) less accurately when sunglasses were added to the images than when the images were unaltered. Accuracy was reduced further by masks than by sunglasses when all emotional expressions were combined, but the reduction in accuracy for each expression was not reported for the mask condition.</p>
<p>Four images of 12 celebrities were taken from the Internet in conditions (i) reference image (unconcealed), (ii) comparison image (unconcealed), (iii) sunglasses image, and (iv) mask image (figure 1). All images were gathered via <rs id="a2" type="publisher" corresp="#a12965832">Google</rs> <rs id="a12965832" type="software">Image</rs> search following the procedures used in previous research (e.g. [7,83]), with the only constraints being that the image should be good quality (i.e. not blurry), and show the face in a mostly front-facing view. There were no constraints in terms of facial expression displayed. All images were cropped to show head and shoulders at 380 × 570 pixels. The reference image was chosen as the more front-facing, neutral expression of the two unconcealed images, so as to approximate a passport-style image. A different identity 'foil' face image was selected for each identity to serve as the reference image in non-match trials. The foil identities were chosen to match the same verbal description as the target identity, e.g. 'young woman, blonde hair'. In all trials, the unconcealed sunglasses mask</p>
<p>Participants completed the experiment online using the <rs id="a12965833" type="software">Qualtrics</rs> platform. Online tests of cognitive processing have become increasingly popular, and have been found to yield high-quality data that is indistinguishable from that collected in the laboratory [84][85][86]. In the familiar face matching task, participants were instructed that they would view pairs of face images and that their task was to decide whether the images were of the same person or two different people. Each face pair was presented side-by-side with the text 'Do you think these images depict the same or different people?' below the images. The response options presented below the text were 'Same' and 'Different'. The images and text remained on the screen until participants responded and clicked 'next' to see the next trial. On each trial, participants were also instructed to provide a confidence judgement for their same/different identity response for each trial-provided by the use of a sliding scale from 0 to 100. The confidence data are not presented in this paper. A practise trial with images not used in this experiment ensured that participants understood the paradigm. Identities were randomly assigned to conditions between participants, and each participant saw each identity only once, resulting in 12 trials. Participants saw two trials in each concealment condition (unconcealed, sunglasses, mask) for each trial type (match, non-match). The small number of trials in this experiment reflects both the difficulty of finding celebrities who would be familiar to most of our participants, and then finding images of those celebrities wearing face masks and sunglasses. At the end of the experiment, participants reviewed a list of names of celebrities and were asked to select all names for whom they would recognize the face. Participants received their accuracy score upon completion of all three experiments.</p>
<p>Images of 18 identities (nine female) displaying angry, disgust, fear, happy, neutral, sad and surprise expressions were selected from the NimStim face database [92] for their high emotional validity. Examples of sunglasses and face masks were sourced from the Internet, and added onto the images using <rs id="a3" type="publisher" corresp="#a12965836">Adobe</rs> <rs id="a12965836" type="software">Photoshop</rs> (figure 6). We chose to add sunglasses and masks to existing stimuli here (as opposed to using images of faces actually wearing sunglasses and masks as in Experiments 1 and 2) in order to ensure that the intensity and validity of the underlying expression was consistent across concealment conditions.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f562290806"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:45+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The <rs id="a12894730" type="software" subtype="implicit">software
code</rs> is available at
GitHub: <rs id="a12894732" type="url" corresp="#a12894730">https://github.com/davidechicco/R-squared_ versus_other_regression_rates</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f388546806"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:45+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The Euroimmun ELISA was performed using a Stratec Gemini automated microplate processor as previously described [28]. Raw optical density (OD) readings were adjusted by calculating the ratio of the OD of the control or participant sample divided by the OD of the assay calibrator. A ratio 1.1 was used as the threshold for a positive result as per manufacturer's instructions [1]. A ratio of 11 was used as the upper threshold of the dynamic range, as the assay saturated above this point. The Roche ECLIA was performed using the Roche cobas Ò e801 immunoassay analyser analyzer [24]. Results are expressed as a cut-off index (COI), calculated by the analyser <rs id="a12970910" type="software" subtype="implicit">software</rs> as the electrochemiluminescence signal obtained from the patient sample divided by the lot-specific cut-off value [2]. A COI1 was used as the threshold for a positive result as per manufacturer's instructions. Across their dynamic range, the semi-quantitative indices of both assays approximate to a linear relationship with antibody levels (Supplementary Fig. 1). We have previously reported quantitation of pseudovirus neutralising antibody (nAb) titres in 70 seropositive HCW from this cohort at 16À18 weeks of follow up. Briefly, lentiviral vectors pseudotyped with SARS-CoV-2 spike protein, encoding firefly luciferase were produced in HEK-293 T cells and used to infect Huh7 cells in the presence of serial dilutions of serum as previously described [29].</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f81268435"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T08:15+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>After finding and merging statistically associated statements, we ran a structural equation modeling (SEM) using <rs id="a12951541" type="software">STATA</rs> to control for interrelationships between explanatory variables. Most TAM analyses were carried out using SEM to measure the causal relationships between constructs (Arpaci, 2016;Moqbel, 2012;Oum &amp; Han, 2011;Pinho &amp; Soares, 2011;Rauniar et al., 2014).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f586144948"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:41+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>disparities that exist in various implementation platforms. For example, the is set to one in the <rs id="a12894666" type="software">MATLAB</rs> simulation</p>
<p>The mainstream simulation platform <rs id="a12894667" type="software">MATLAB</rs> <rs id="a12894668" type="version" corresp="#a12894667">R2015b</rs> is adopted to verify the correctness and effectiveness of proposed medical cryptosystem in this paper, which is installed in the
Windows
11 operating system with an i7-13700KF central processor. Besides, the parameters to be specified by user in the encryption phase are configured as follows. The compression rate is 0.25. The bit depth of medical image data is 16. with the category of medical image. In decryption phase, the Orthogonal Newton Smoothing norm (ONSL0) method is 0  employed to precisely reconstruct the sparse coefficient matrix from compressed data. Last but not the least, the medical images for simulation are downloaded from
the <rs id="a12894669" type="software">Open Access Biomedical Image Search Engine (OABISE</rs>
: <rs id="a12894670" type="url" corresp="#a12894669">https://openi.nlm. nih.gov/</rs>).</p>
<p>First, the resolution of four medical images downloaded from the <rs id="a12894671" type="software">OABISE</rs> are reshaped to . The purpose of this From the qualitative point of view, the plain medical images are synchronously compressed and encrypted into the unrecognizable images without any semantic features. Moreover, the histograms of encrypted images tend to be flat, effectively masking the statistical properties of their corresponding plain images. In other respects, the decrypted images recovered from cipher images have no obvious visual discrepancy with the matching plain medical images. Numerically, the values of and all exceed 35.50 dec PSNR dec SSIM dB and 0.85, respectively. In general, our proposed medical cryptosystem can balance its security and compressibility well.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f561876338"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:17+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Code availability. The <rs id="a12952073" type="software" subtype="implicit">simulation code</rs> used in this study has been made available at
GitHub (<rs id="a12952075" type="url" corresp="#a12952073">https://github.com/yagmurerten/migration2017</rs>).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f577748335"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:22+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>To understand how the treatments (PrevPFM v.s. NeverPFM and N0 v.s. N135) influence total maize performance and their relations with soil properties, redundancy analysis (RDA) was conducted based on the data of crop performance (stem thick, height, aboveground biomass, belowground biomass, total root length, root surface area, chlorophyll, root P, and APase) and soil properties (pH, soil moisture, Olsen-P, bulk density, soil porosity, water holding capacity and AcP). Monte Carlo permutations were used to test significance of relationships between selected soil factors and plant growth (P &lt; 0.05), and then tests the significance of the difference between each soil factor and plant growth through the <rs id="a12972829" type="software" subtype="component" corresp="#a12972830">envfit</rs> function in <rs id="a12972830" type="software" subtype="environment">vegan</rs> package. RDA was performed using <rs id="a12972831" type="software">R</rs>. <rs id="a12972832" type="version" corresp="#a12972831">4.1.3</rs>. The other statistics analyses were conducted using <rs id="a12972833" type="software">SPSS</rs> version <rs id="a12972834" type="version" corresp="#a12972833">22.0</rs>. All reported differences are significant at P &lt;</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f343955591"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:47+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Viral genomes were sequenced using Illumina-based unbiased metagenomic short-read sequencing, followed by reference-guided assembly using <rs id="a12895157" type="software">viral-ngs</rs> <rs id="a12895158" type="version" corresp="#a12895157">2.0.21</rs> software <rs id="a12953669" type="bibr">( 19 )</rs> with the Wuhan-Hu-1 sequence (NC_045512.2) as the reference (Materials and Methods). We generated 778 high-quality SARS-CoV-2 assemblies (&gt;98% complete) from 772 individuals, and an additional 72 high-quality partial genomes (&gt;80% complete) (Fig. 1A). Genome recovery and coverage were strongly correlated with viral abundance and clinical diagnostic test results (Fig. S2 andS3). Genomes were separated from one another by a median of 6 single nucleotide polymorphisms (SNPs) (interquartile range 4-9 SNPs; range 0-85 SNPs) (Fig. S4A-B). As expected during rapid population expansion, most alleles were rare, as assessed by a strongly negative Tajima's D statistic throughout the genome (Fig. S4C).</p>
<p>Allele frequency was estimated as the proportion of derived / (derived + ancestral) versions of the allele. A 95% confidence interval was estimated for the proportion using the binomial distribution. The frequency of the iSNV for MA_MGH_00427 was calculated from 2 libraries; 50 reads contained the derived T allele and 146 reads contained the ancestral G allele based on the aligned reads from the <rs id="a12895160" type="software">viral-ngs</rs> pipeline (as described above).</p>
<p>Haplotype networks were visualized using the software tool <rs id="a12895161" type="software">PopART</rs> v <rs id="a12895162" type="version" corresp="#a12895161">1.7</rs> <rs id="a12953668" type="bibr">( 48 )</rs> . The assembled sequences were aligned against NC_045512.2 and the first 268bp at the 5' end and 230bp at the 3' end (UTR regions) were removed from the alignment. A nexus-format input file for</p>
<p> <rs id="a12895164" type="software">PopART</rs> was created using a <rs id="a12895165" type="language" corresp="#a12895164">Python</rs> to consolidate sequence information with metadata classifications. This <rs id="a12895166" type="software" subtype="implicit">script</rs> is available at <rs id="a12895167" type="url" corresp="#a12895166">http://www.github.com/broadinstitute/</rs>[repository]. A TCS network of the sequences ( 49) was constructed in <rs id="a12895168" type="software">PopART</rs>. Regions where any sequence had ambiguous bases were masked. For the construction of haplotype networks in Figure 4, one sample, MA_MGH_00090, was removed to prevent masking of the G3892T variant. For the displayed haplotype networks, the area of the circle corresponds to how many verbatim-identical sequences (after masking) bin together as the same haplotype. The hash marks on the edges indicate the SNP distance between sequence haplotypes (1 mark=1 SNP apart). Gene graphs were constructed using pairwise distance matrices computed on aligned SARS-CoV-2 genomes and clustered using the <rs id="a12895169" type="software" subtype="environment">R</rs> package <rs id="a12895170" type="software" subtype="component" corresp="#a12895169">adegenet</rs> <rs id="a12953667" type="bibr">( 50 )</rs> .</p>
<p>For this analysis, the main SNF cluster was restricted to samples collected before April 15, 2020, and the conference cluster to samples collected before March 8, 2020. We assumed that the number of transmissions was the minimum possible (one fewer than the number of samples in the cluster). The p-value for the comparison between the clusters assessed the probability that the observed numbers of mutations were produced by Poisson processes with the same value of λ, using the <rs id="a12895172" type="software" subtype="environment">R</rs> function <rs id="a12895173" type="software" subtype="component" corresp="#a12895174">poisson.test</rs> (in the <rs id="a12895174" type="software" subtype="component" corresp="#a12895172">stats</rs> package v <rs id="a12895175" type="version" corresp="#a12895174">3.6.2</rs>). For the expected number of mutations, we assumed that substitutions occur predominantly during the transmission bottleneck and calculated the expected rate based on a generation time of 5.0 days ( 51) and a mutation rate of 1.0 x 10 -3 /bp/year (Fig. S5C), which together yield an expectation of 0.41 substitutions/transmission.</p>
<p>We used <rs id="a12895176" type="software">R</rs> <rs id="a12953666" type="bibr">( 42 )</rs> , <rs id="a12895178" type="software">Bioconductor</rs> <rs id="a12953665" type="bibr">( 52 )</rs> , <rs id="a12895180" type="software">ggplot2</rs>, <rs id="a12895181" type="software">tidyverse</rs> <rs id="a12953664" type="bibr">( 53 )</rs> , and <rs id="a12895183" type="software">ggtree</rs> <rs id="a12953663" type="bibr">( 54)</rs> to clean and plot data and trees, and <rs id="a12895185" type="software">choroplethr</rs> to draw maps.</p>
<p>We conducted all analyses using <rs id="a12895186" type="software" subtype="environment" corresp="#a12895188">viral-ngs</rs> <rs id="a12895187" type="version" corresp="#a12895186">2.0.21</rs> on the <rs id="a12895188" type="software" subtype="environment">Terra</rs> platform ( <rs id="a12895189" type="url" corresp="#a12895188">app.terra.bio</rs> ). All of the workflows named below are publicly available via the <rs id="a12895190" type="software">Dockstore Tool Repository
Service</rs> ( <rs id="a12895191" type="url" corresp="#a12895190">dockstore.org/organizations/BroadInstitute/collections/pgs</rs> ). We demultiplexed individual libraries using the demux_only workflow for each lane of each flowcell, removed reads mapping to the human genome and to other known technical contaminants (e.g. sequencing adapters) using deplete_only (with bwaDbs=["gs://pathogen-public-dbs/v0/hg19.bwa_idx.tar.zst"] and blastDbs=["gs://pathogen-public-dbs/v0/GRCh37.68_ncRNA.fasta.zst", "gs://pathogen-public-dbs/v0/hybsel_probe_adapters.fasta"]), and performed reference-based assembly using assemble_refbased (once per sample, with all sequencing replicates merged in the read_unmapped_bams input and with a reference_fasta taken from https://www.ncbi.nlm.nih.gov/nuccore/NC_045512.2?report=fasta ). We ran assemble_refbased on 1970 read set inputs spanning 1535 distinct samples (inclusive of controls). We used the following stringent criteria to excluded any sample where i) fewer than 50,000 cleaned reads were obtained; ii) the proportion of reads mapping to the internal control (IC) sequence (ERCC spike-in) was &gt;3 standard deviations from the mean observed for that IC</p>
<p>We constructed phylogenetic maximum likelihood (ML) and time trees with associated visualizations using the <rs id="a12895192" type="software">Augur</rs> pipeline ( augur_with_assemblies ). We used SARS-CoV-2-specific procedures taken from github.com/nextstrain/ncov , specifically setting the clock rate to 0.0008 +/-0.0004, rooting the tree using the reference genome, and using the nextstrain site-masking and clade-definition files. In addition to our 772 genomes from unique individuals from Massachusetts, we included a global comparison set of 4,011 genomes subsampled from a download from the <rs id="a12895193" type="software">GISAID</rs> database on 15 June, 2020. These 4,011 genomes contain at most 50 representatives from each state or province in North America plus at most 50 representatives from each country outside of North America. Random subsampling was biased towards genomes genetically close to our focal set of genomes, using the <rs id="a12895194" type="software" subtype="implicit">distance matrix calculator</rs> at <rs id="a12895195" type="url" corresp="#a12895194">github.com/nextstrain/ncov/blob/master/scripts/priorities.py</rs> . The resulting <rs id="a12895196" type="software">augur</rs> output is visualizable on <rs id="a12895197" type="software">auspice.us</rs> or can be incorporated in custom deployments using <rs id="a12895198" type="software">Google Cloud Run</rs> using our template (
github.com/dpark01/auspice-private-
template ); this template is used to showcase our data at auspice.broadinstitute.org .</p>
<p>We also conducted additional analysis of the genomes sequenced in this study. We aligned the set of 772 genomes using <rs id="a12895199" type="software">MAFFT</rs> v <rs id="a12895200" type="version" corresp="#a12895199">7.471</rs> <rs id="a12953662" type="bibr">( 41 )</rs> and trimmed 5' and 3' (first 265 and last 228 bases) UTRs from the alignment in <rs id="a12895202" type="software">R</rs> <rs id="a12953661" type="bibr">( 42 )</rs> . To estimate the root-to-tip distance, we constructed ML phylogenetic trees using <rs id="a12895204" type="software">PhyML</rs> <rs id="a12953660" type="bibr">( 43 )</rs> v <rs id="a12895206" type="version" corresp="#a12895204">3.3.20190909</rs> with default parameters using the <rs id="a12895207" type="software">MAFFT</rs> alignment of 772 genomes. We used <rs id="a12895208" type="software">TempEst</rs> <rs id="a12953659" type="bibr">( 44 )</rs> v. <rs id="a12895210" type="version" corresp="#a12895208">1.5.3</rs> and selected the best-fitting root as identified using a heuristic residual mean squared function. To estimate branch support in maximum-likelihood phylogenies, we used <rs id="a12895211" type="software">IQ-Tree</rs> <rs id="a12953658" type="bibr">( 45)</rs> with the ultrafast bootstrap and 10,000 bootstrap samples.</p>
<p>To construct Bayesian time-trees, we used <rs id="a12895213" type="software">BEAST</rs> <rs id="a12895214" type="version" corresp="#a12895213">2.6.2</rs> with a general time reversible substitution model with 4 rate categories drawn from a gamma distribution (GTR4G), a strict clock, coalescent exponential tree prior, a uniform [-inf, inf] prior for the clock rate, a 1/x [-inf, inf] prior for the coalescent effective population size; and a laplace [-inf, inf] prior for the growth rate.</p>
<p>We ran the MCMC chain in <rs id="a12895215" type="software">BEAST2</rs> for 100 million steps and thinned the chain by recording samples every 1000 steps. The first 30% of samples were discarded prior to calculating summary statistics from the posterior. We used <rs id="a12895216" type="software">TreeAnnotator</rs> v <rs id="a12895217" type="version" corresp="#a12895216">2.6.2</rs> to construct maximum clade credibility trees with a burn-in percentage of 30%. We also compared a Hasegawa-Yoshino-Gawa substitution model with kappa = 2 and with 4 rate categories drawn from the gamma distribution (HKY4G) and ran this chain for 100 million steps using the same thinning and burn-in described for the GTR4G model.</p>
<p>We used <rs id="a12895218" type="software">Kraken2</rs> <rs id="a12953657" type="bibr">( 46 )</rs> to identify other viral taxa present in NP swab samples from COVID positive patients, excluding those removed by filters i and ii described above. To do so, we ran the classify_single workflow on all reads from all samples (with kraken2_db_tgz="gs://pathogen-public-dbs/v1/kraken2-broad-20200505.tar.zst", krona_taxonomy_db_kraken2_tgz="gs://pathogen-public-dbs/v1/krona.taxonomy-20200505.tab. zst", ncbi_taxdump_tgz="gs://pathogen-public-dbs/v1/taxdump-20200505.tar.gz", trim_clip_db="gs://pathogen-public-dbs/v0/contaminants.clip_db.fasta", spikein_db="gs://pathogen-public-dbs/v0/ERCC_96_nopolyA.fasta"). Our <rs id="a12895220" type="software">kraken2</rs> database was</p>
</text>
</tei>
  <tei>
    <teiHeader>
        <fileDesc id="f491185936"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T16:04+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text lang="en">
        <p>Formalin-fixed paraffin-embedded (FFPE) skin blocks were cut into 6-μm sections and placed on slides. Sections were first deparaffinized and rehydrated, then Heat-Induced Epitope Retrieval (HIER) was performed and sections were permeabilized with PBS 0.01% Triton. Samples were stained with primary antibodies (Supplementary Table 1) for 2 hours at room temperature. For immunofluorescence analysis, sections were then stained with fluorescently labelled secondary antibodies (Supplementary Table 1) for 30 min at room temperature. For immunohistochemistry, sections were stained with HRP-conjugated secondary antibodies followed by DAB staining and Mayer counterstaining. For RNA fluorescence in situ hybridization (FISH), IFNB1 mRNA was detected in skin using RNAScope Multiplex Fluorescent V2 Assay following the manufacturer's instructions (Advanced Cell Diagnostics, Inc.). Co-staining of sections with mouse anti-human CD163 (Diagnostic Bio Systems) was performed as described above. Images were acquired with a Zeiss LSM 700 confocal microscope and analysed with 
            <rs type="software">Zen 2010</rs> software. For cell quantification, slides were digitalized using the PAN-NORAMIC 250 Flash digital scanner (3DHISTECH Ltd) and cell types were quantified using the QuantCenter plug-in 2.2 of 
            <rs type="software">Caseviewer</rs>
            <rs type="version">2.4</rs> software.
        </p>
        <p>Mouse lungs were cut into 3-μm sections. The extent of lung inflammation was quantified as the average percentage of lung surface area in which the alveolar wall is thickened with at least 50% decreased airspace area and was assessed by two independent investigators using three lung sections per mouse. TUNEL staining was performed using a commercially available kit (Promega) according to the manufacturer's instructions. Imaging was performed using the Zeiss Axioplan fluorescence microscope with the use of 
            <rs type="software">Axiovision</rs> software. Three fields were selected randomly from each lung piece. TUNEL-positive cells were quantified by automated counting performed by image analysis software (
            <rs type="software">ImageJ</rs>).
        </p>
        <p>NanoString analysis mRNA expression of 600 targets was analysed with the nCounter Human Immunology V2 panel including 20 customized probes (Nanostring Technologies, Seattle, WA, USA) on the nCounter platform (Nanostring Technologies) using 100 ng of RNA per skin sample. This commercial panel was extensively validated in-house for accuracy, repeatability and reproducibility before analysing the study samples. A quality check was run for each sample before including it into the analysis. Data were normalized and analysed using either 
            <rs type="software" id="s1">nSolver</rs>
            <rs type="version" corresp="s1">4.0</rs> (
            <rs type="creator" corresp="s1">Nanostring Technologies</rs>) or 
            <rs type="software" id="s2">ROSALIND</rs> (
            <rs type="creator" corresp="s2">ROSALIND Inc., San Diego, CA</rs>). Housekeeping probes to be used for normalization are selected based on the 
            <rs type="software">geNorm</rs> algorithm as implemented in the 
            <rs type="software">NormqPCR R</rs> library 49 . Clustering of genes for the final heatmap of differentially expressed genes was done using the PAM (Partitioning Around Medoids) method using the 
            <rs type="software" id="s3">fpc R</rs> library (
            <rs type="url" corresp="s3">https://cran.r-project.org/web/packages/fpc/index.html)</rs> that takes into consideration the direction and type of all signals on a pathway and the position, role and type of every gene. The z-scores of each gene were then calculated for the selected patients to generate heatmaps and determine specific classifiers.
        </p>
        <p>Immunoblotting SDS-loading buffer was mixed with the lung lysates in RIPA buffer and denatured at 95 °C for 10 min. Lysates were separated by 10% SDS-PAGE and transferred onto nitrocellulose membranes. Blots were incubated with anti-p-p65 (Ser468) (1:1,000 dilution), and anti-p-STAT1 (Tyr 701) (1:1,000 dilution) (Cell Signaling) and anti-β-actin-HRP (1:2,000 dilution) (Santa Cruz Biotechnology). Proteins were visualized with the enhanced chemiluminescence substrate ECL (Pierce, Thermo Fisher Scientific) and imaged using the ChemiDoc XRS Biorad Imager and 
            <rs type="software" id="s4">Image Lab</rs> Software 
            <rs type="version" corresp="s4">5.1</rs>. Uncropped images are presented in Supplementary Fig. 1.
        </p>
        <p>Infected and control LoCs were imaged using a Leica SP8 confocal microscope with a white light laser. LoCs were imaged with a 25× water immersion objective (NA = 0.95, Leica), with standard settings (voxel size 227.27 × 227.27 × 300 nm 3 ) across chips labelled the same way. Z-stacks were subsequently deconvolved using the 
            <rs type="software" id="s5">Huygens Deconvolution</rs> Software (
            <rs type="creator" corresp="s5">Scientific Volume Imaging</rs>) and 3D views were rendered using 
            <rs type="software">Imaris</rs> (Bitplane). Maximum intensity projects were rendered using 
            <rs type="software">ImageJ</rs>. The following parameters were used for generation of the surfaces in Imaris for the visualization of IFNβ, cleaved caspase-3, macrophages and p-STING. In each case, uninfected control chips and/ or infected chips and/or treated chips from the same experiment were immunostained and imaged together, to control for differences in the immunofluorescence intensities across antibody aliquots, imaging conditions, and microscopes. Chips from the same experiment were analysed using the same Imaris parameters. Three-cell component chips in Fig. 3b, Extended Data Fig. 6b, IFNβ: manual threshold: 110, smoothing: 0.455 μm. Three-cell component chips in Extended Data Fig. 6d, IFNβ: manual threshold: 110, smoothing: 0.455 μm. Two-cell component chips in Fig. 3c, Extended Data Fig. 7b, IFNβ: manual threshold: 45, smoothing: 0.455 μm. Three-cell component chips in Extended Data Fig. 6e, cleaved caspase-3: manual threshold: 110, smoothing: 0.8 μm. Two-cell component chips in Fig. 3c
        </p>
        <p>Raw data were processed using 
            <rs type="software">SEQUEST</rs>, 
            <rs type="software">Mascot</rs>, 
            <rs type="software" id="s7">MS Amanda</rs> <rs type="version" corresp="s7">51</rs> and 
            <rs type="software" id="s8">MS Fragger</rs> <rs type="version" corresp="s8">52</rs> in 
            <rs type="software" id="s9">Proteome Discoverer</rs> v.
            <rs type="version" corresp="s9">2.4</rs> against a concatenated database consisting of the Uniprot human reference proteome (release 2020_10) and Uniprot SARS-CoV-2 reference proteome (release 2020_10). Enzyme specificity was set to trypsin and a minimum of six amino acids was required for peptide identification. Up to two missed cleavages were allowed and a 1% false discovery rate (FDR) cut-off was applied both at peptide and at protein identification levels. For the database search, carbamidomethylation (C) and TMT tags (K and peptide N termini) were set as fixed modifications whereas oxidation (M) was considered as a variable. The resulting text files were processed through in-house written
            <rs type="software" subtype="environment" corresp="s10">R</rs>
            <rs type="software" subtype="implicit" id="s10">scripts</rs> (v.
            <rs type="version" corresp="s10">3.6.3</rs>). Two steps of normalization were applied: sample loading (SL) and trimmed mean of M-values (TMM) normalization. The SL normalization 53 assumes that total protein abundances are equal across the TMT channels; therefore, the reporter ion intensities of all spectra were summed and each channel was scaled according to this sum, so that the sum of reporter ion signals per channel equals the average of the signals across samples. Subsequently, the TMM normalization step was applied using the package
            <rs type="software" id="s11">EdgeR</rs> (v.
            <rs type="version" corresp="s11">3.26.8</rs>) 54 . This normalization step works on the assumption that most of the protein abundances do not change across samples therefore, it calculates normalization factors according to these presumed unchanged protein abundances. Differential protein expression analysis was performed using the
            <rs type="software" subtype="environment" corresp="s12">R</rs> bioconductor package
            <rs type="software" id="s12">limma</rs> (v.
            <rs type="version" corresp="s12">3.40.6</rs>, 2020-02-29) 55 , followed by the Benjamini-Hochberg multiple-testing method 56 . P values lower than 0.00128 (FDR &lt; 0.05) and absolute log 2 -transformed fold change (log 2 FC) &gt; 0.5 were considered as significant. For the time-course study, all quantified proteins were monitored. The significant temporal dynamics were defined with the timecourse package in
            <rs type="software" subtype="environment">R</rs>
            <rs type="software">Bioconductor</rs>, which uses a multivariate empirical Bayes model to rank proteins 57 . Replicate time-course data can be compared allowing for variability both within and between time points. The mb.long method was used to calculate the moderated Hotelling T 2 statistic, specifying a one-dimensional method (method = "1D"), in which significant proteins change over the time course. The null hypothesis is that the protein temporal profile is equal to 0.
        </p>
        <p>Statistical analyses are described in each figure legend. For experiments combining several groups, an ordinary one-way ANOVA test was used. Statistical significance was determined using 
            <rs type="software" id="s13">Prism</rs> v.
            <rs type="version" corresp="s13">8.0</rs> software (
            <rs type="creator">GraphPad</rs>). Significant differences between groups were determined by post-hoc Tukey's multiple comparisons tests, unless specified otherwise, P &gt; 0.05 was considered non-significant. The Student's t-test or the Mann-Whitney test was used to assess the P value when comparing only two groups. For LoC studies, fields of view from a given LoC are considered as biological replicates, and the number of LoCs corresponds to the number of times the experiment was repeated. Images of p-STING + endothelial cells in Fig. 3d
        </p>
        <p>We thank the Genomic Technologies Facility (GTF) at University of Lausanne (UNIL) and the Immune Landscape Laboratory (Oncology department, CHUV) for accessing the NanoString platform; A. Joncic and I. Surbeck for technical assistance with the histological analysis; B. Fehrenbacher for technical assistance with transmission electron microscopy studies of COVID-19 skin samples; J. Blanc and G. Knott for technical assistance with serial block scanning electron microscopy of LoC devices and data analysis and volumetric reconstruction; R. Hamelin, F. Armand and M. Pavlou for technical assistance for the proteomic analysis studies in the LoC devices; F. Signorino-Gelo for LoC device fabrication; and N. Samson for help with 
            <rs type="software">BioRender</rs>. This work was supported by the COVID-19 National Research Program (NRP-78) to M.G. and A.A. (4078P0_198470) with additional support by the Leenaards Foundation to M.G. and A.A. and the Fondation Acteria and European Union's Horizon 2020 Research and Innovation program grant agreement (grant no. 804933, ImAgine) to A.A. V.V.T. received support from the Novartis Foundation for Medical-Biological Research (grant no. 20C240).
        </p>
        <p>Custom 
            <rs type="software" subtype="implicit" id="s14">scripts</rs> in
            <rs type="software" subtype="environment" corresp="s14">R</rs> for the analysis of proteomics data have been deposited at Zenodo and are publicly available at https://doi.org/10.5281/ zenodo.5818157.
        </p>
        </text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f548720653"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:22+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Oxford Nanopore DNA sequencing. Library preparation was carried out using the ligation sequencing kits (Oxford Nanopore Technologies) SQK-LSK109 and SQK-LSK112 for sequencing on R.9.4.1 and the R.10.4 flowcells, respectively. Anaerobic digester and Zymo R.9.4.1 datasets were generated on a MinION Mk1B (Oxford Nanopore Technologies) device, while the Zymo R10.4 dataset was produced on a PromethION and the digester R10.4 read sequences were generated on a GridION using the <rs id="a12973013" type="software">MinKNOW</rs> v <rs id="a12973014" type="version" corresp="#a12973013">21.05.25</rs> software ( <rs id="a12973015" type="url" corresp="#a12973013">https://community. nanoporetech.com/downloads</rs>).</p>
<p>Read processing. Illumina reads were trimmed for adapters using <rs id="a12973016" type="software">Cutadapt</rs> v. <rs id="a12973017" type="version" corresp="#a12973016">1.16</rs> <rs id="a12973018" type="bibr">(ref. 29 )</rs>. The generated raw Nanopore data were basecalled in super-accurate mode using <rs id="a12973019" type="software">Guppy</rs> v. <rs id="a12973020" type="version" corresp="#a12973019">5.0.16</rs> ( <rs id="a12973021" type="url" corresp="#a12973019">https://community.nanoporetech.com/downloads</rs>) with the dna_r9.4.1_450bps_sup.cfg model for R9.4.1 and the dna_r10.4_e8.1_sup. cfg model for R10.4 chemistry. Given that the R10.4 data were observed to feature concatemeric reads that might complicate the metagenome assembly step, the concatemers in R10.4 data were split by using the <rs id="a12973022" type="software" subtype="component" corresp="#a12973023">split_on_adapter</rs> command (five iterations) of <rs id="a12973023" type="software">duplex-tools</rs> v. <rs id="a12973024" type="version" corresp="#a12973023">0.2.5</rs> ( <rs id="a12973025" type="url" corresp="#a12973023">https://github.com/nanoporetech/duplex-tools</rs>). Adapters for Nanopore reads were removed using <rs id="a12973026" type="software">Porechop</rs> v. <rs id="a12973027" type="version" corresp="#a12973026">0.2.3</rs> <rs id="a12973028" type="bibr">(ref. 30 )</rs>, and reads with a lower length than 200 bp and a Phred quality score below 7 and 10 for R9.4.1 and R10.4 reads, respectively, were removed using <rs id="a12973029" type="software">NanoFilt</rs> v. <rs id="a12973030" type="version" corresp="#a12973029">2.6.0</rs> <rs id="a12973031" type="bibr">(ref. 31 )</rs>.</p>
<p>The <rs id="a12973032" type="software">CCS</rs> tool v. <rs id="a12973033" type="version" corresp="#a12973032">6.0.0</rs> ( <rs id="a12973034" type="url" corresp="#a12973032">https://ccs.how/</rs>) was used with the PacBio sub-read data to produce HiFi reads. Read statistics were acquired via <rs id="a12973035" type="software">NanoPlot</rs> v. <rs id="a12973036" type="version" corresp="#a12973035">1.24.0</rs> <rs id="a12973037" type="bibr">(ref. 31 )</rs>. <rs id="a12973038" type="software">Counterr</rs> v. <rs id="a12973039" type="version" corresp="#a12973038">0.1</rs> ( <rs id="a12973040" type="url" corresp="#a12973038">https://github.com/dayzerodx/counterr</rs>) was used to assess homopolymer calling in reads.</p>
<p>Long-and short-read datasets for the Zymo Mock bacterial species were subsampled according to custom coverage profiles (range, 5-160) using <rs id="a12973041" type="software">Rasusa</rs> v. <rs id="a12973042" type="version" corresp="#a12973041">0.3.0</rs> ( <rs id="a12973043" type="url" corresp="#a12973041">https://github.com/mbhall88/rasusa</rs>), with the notable exception of Pseudomonas aeruginosa, which featured a maximum coverage of 92 in the short-read dataset. Saccharomyces cerevisiae data were excluded from the Zymo Mock analysis due to insufficient coverage. Anaerobic digester R9.4.1 read data were subsampled using the command '<rs id="a12973044" type="software" subtype="component" corresp="#a12973045">seqtk sample -s100 0.37</rs>' from <rs id="a12973045" type="software" subtype="environment">seqtk</rs> v. <rs id="a12973046" type="version" corresp="#a12973045">1.3</rs> ( <rs id="a12973047" type="url" corresp="#a12973045">https://github.com/lh3/seqtk</rs>).</p>
<p>Read assembly and binning. Long reads were assembled using <rs id="a12973048" type="software">Flye</rs> v. <rs id="a12973049" type="version" corresp="#a12973048">2.9-
b1768</rs> <rs id="a12973050" type="bibr">(refs. 16,32 )</rs> with the '-meta' setting enabled and the '-nano-hq' option for assembling Nanopore reads, whereas the '-pacbio-hifi' and '-min-overlap 7500read-error 0.01' options were used for assembling PacBio HiFi reads, given that it resulted in more high-quality MAGs than using the default settings. The polishing tools for the Nanopore-based assemblies consisted of <rs id="a12973051" type="software">Minimap2</rs> v. <rs id="a12973052" type="version" corresp="#a12973051">2.
17</rs> <rs id="a12973053" type="bibr">(ref. 33 )</rs>, <rs id="a12973054" type="software">Racon</rs> v. <rs id="a12973055" type="version" corresp="#a12973054">1.3.
3</rs> (used three times) <rs id="a12973056" type="bibr">34</rs> , <rs id="a12973057" type="software">Medaka</rs> v. <rs id="a12973058" type="version" corresp="#a12973057">1.4.4</rs> (used twice, <rs id="a12973059" type="url" corresp="#a12973057">https://github. com/nanoporetech/medaka</rs>), and one round of <rs id="a12973060" type="software">Racon</rs> with Illumina reads. For the short-read assembly the trimmed Illumina reads were assembled using <rs id="a12973061" type="software">Megahit</rs> v. <rs id="a12973062" type="version" corresp="#a12973061">1.
1.
4</rs> <rs id="a12973063" type="bibr">(ref. 35 )</rs>. Contigs shorter than 1 kbp were filtered out using <rs id="a12973064" type="software">Bioawk</rs> v. <rs id="a12973065" type="version" corresp="#a12973064">1.
0</rs> ( <rs id="a12973066" type="url" corresp="#a12973064">https://github.com/lh3/bioawk</rs>). The contig guanine and cytosine content was calculated using <rs id="a12973067" type="software">infoseq</rs> (v. <rs id="a12973068" type="version" corresp="#a12973067">6.6.0.0</rs>, <rs id="a12973069" type="bibr">ref. 36</rs> ).</p>
<p>Automated binning was carried out using three binners: <rs id="a12973070" type="software">MetaBAT2</rs> v. <rs id="a12973071" type="version" corresp="#a12973070">2.12.1</rs> <rs id="a12973072" type="bibr">(ref. 37 )</rs> with the '-s 500000' setting, <rs id="a12973073" type="software">MaxBin2</rs> v. <rs id="a12973074" type="version" corresp="#a12973073">2.2.7</rs> <rs id="a12973075" type="bibr">(ref. 38 )</rs>, and <rs id="a12973076" type="software">Vamb</rs> v. <rs id="a12973077" type="version" corresp="#a12973076">3.0.2</rs> <rs id="a12973078" type="bibr">(ref. 39 )</rs> with the '-o C-minfasta 500000' setting. To aid with the binning process, contig coverage profiles from different sequencer datasets (Supplementary Table 1) as well as contig coverage by nine additional time-series Illumina datasets of the same anaerobic digester (Supplementary Table 4) were provided as input to the three binners. The binning output of different tools was then integrated and refined using <rs id="a12973079" type="software">DAS Tool</rs> v. <rs id="a12973080" type="version" corresp="#a12973079">1.1.2</rs> <rs id="a12973081" type="bibr">(ref. 40 )</rs>. <rs id="a12973082" type="software">CoverM</rs> v. <rs id="a12973083" type="version" corresp="#a12973082">0.6.1</rs> ( <rs id="a12973084" type="url" corresp="#a12973082">https://github.com/wwood/ CoverM</rs>) was applied to calculate the bin coverage (using the '-m mean' setting) and the relative abundance ('-m relative_abundance'). A general overview of the processing of the sludge metagenomic data is presented in Supplementary Fig. 10.</p>
<p>Assembly processing. The completeness and contamination of the genome bins were estimated using <rs id="a12973085" type="software">CheckM</rs> v. <rs id="a12973086" type="version" corresp="#a12973085">1.1.2</rs> <rs id="a12973087" type="bibr">(ref. 41 )</rs>. The bins were classified using <rs id="a12973088" type="software">GDTB-Tk</rs> v. <rs id="a12973089" type="version" corresp="#a12973088">1.5.0</rs> <rs id="a12973090" type="bibr">(ref. 42 )</rs> and the R202 database. Protein sequences were predicted using <rs id="a12973091" type="software">Prodigal</rs> v. <rs id="a12973092" type="version" corresp="#a12973091">2.6.3</rs> <rs id="a12973093" type="bibr">(ref. 43 )</rs> with the 'p meta' setting, while the ribosomal RNA genes were predicted using <rs id="a12973094" type="software">Barrnap</rs> v. <rs id="a12973095" type="version" corresp="#a12973094">0.9</rs> ( <rs id="a12973096" type="url" corresp="#a12973094">https://github.com/tseemann/barrnap</rs>) and the transfer RNA predictions were made using <rs id="a12973097" type="software">tRNAscan-SE</rs> v. <rs id="a12973098" type="version" corresp="#a12973097">2.0.5</rs> <rs id="a12973099" type="bibr">(ref. 44 )</rs>. Bin quality was determined following the Genomic Standards Consortium guidelines, in which a MAG of high quality has genome completeness of more than 90%, contamination of less than 5%, at least 18 distinct tRNA genes, and an occurrence of at least once of the 5S, 16S and 23S rRNA genes 26 . MAGs with completeness above 50% and contamination below 10% were classified as medium quality, while low-quality MAGs featured completeness below 50% and contamination below 10%. MAGs with contamination estimates higher than 10% were classified as contaminated.</p>
<p>Illumina reads were mapped to the assemblies using <rs id="a12973100" type="software">Bowtie2</rs> v. <rs id="a12973101" type="version" corresp="#a12973100">2.4.2</rs> <rs id="a12973102" type="bibr">(ref. 45 )</rs> with the '-very-sensitive-local' setting. The mapping was converted to BAM and sorted using <rs id="a12973103" type="software">SAMtools</rs> v. <rs id="a12973104" type="version" corresp="#a12973103">1.9</rs> <rs id="a12973105" type="bibr">(ref. 46 )</rs>. The single-nucleotide polymorphism rate was then calculated using <rs id="a12973106" type="software">CMseq</rs> v. <rs id="a12973107" type="version" corresp="#a12973106">1.0.3</rs> <rs id="a12973108" type="bibr">(ref. 6 )</rs> from the mapping using <rs id="a12973109" type="software">poly.py</rs>
script with the '-mincov 10-minqual 30' setting.</p>
<p>Bins were clustered using <rs id="a12973110" type="software">dRep</rs> v. <rs id="a12973111" type="version" corresp="#a12973110">2.6.2</rs> <rs id="a12973112" type="bibr">(ref. 47 )</rs> with the '-comp 50 -con 10 -sa 0.95' setting. Only the bins that featured higher coverage than 10 in their respective sequencing platform and a higher Illumina read coverage than 5 for bins from the hybrid approach were included in downstream analysis. The IDEEL test was used to infer the level of protein truncations in the bins and was applied to provide a relative measurement of improvement in genome consensus quality via short-read polishing 20,28 . In brief, the predicted protein sequences from clustered bins and Zymo assemblies were searched against the UniProt
TrEMBL 48 database (release 2021
_01) using <rs id="a12973113" type="software">Diamond</rs> v. <rs id="a12973114" type="version" corresp="#a12973113">2.0.6</rs> <rs id="a12973115" type="bibr">(ref. 49 )</rs>. Query matches, which were not present in all datasets, were omitted to reduce noise. The IDEEL scores (estimated fraction of full-length protein sequences) were assigned as described previously 19 , where query-to-reference length ratios of more than 0.95 were counted as full-length protein sequences.</p>
<p><rs id="a12973116" type="software">QUAST</rs> v. <rs id="a12973117" type="version" corresp="#a12973116">4.6.3</rs> <rs id="a12973118" type="bibr">(ref. 50 )</rs> was applied on the Zymo assemblies and the clustered bins that had a single-nucleotide polymorphism rate less than 0.5% to determine the mismatch and indels metrics. Cases with the <rs id="a12973119" type="software">QUAST</rs> parameters genome fraction less than 75% and unaligned length more than 250 kbp were omitted to reduce noise. For homopolymer analysis, the clustered bins were mapped to each other using the asm5 mode of <rs id="a12973120" type="software">Minimap2</rs>, and <rs id="a12973121" type="software">Counterr</rs> was used on the mapping files to determine the homopolymer calling errors. For <rs id="a12973122" type="software">QUAST</rs> and <rs id="a12973123" type="software">Counterr</rs>, Illumina-polished PacBio HiFi bins were used as reference sequences. <rs id="a12973124" type="software">FastANI</rs> v. <rs id="a12973125" type="version" corresp="#a12973124">1.
33</rs> <rs id="a12973126" type="bibr">(ref. 51 )</rs> was used to calculate identity scores between Zymo assemblies and the Zymo reference sequences. The Zymo mock reference genome sequences, which were used as a substitute for PacBio HiFi, were obtained from a link in the accompanying instruction manual to the ZymoBIOMICS HMW DNA Standard Catalog No. D6332 (https://s3.amazonaws.com/zymo-files/BioPool/ D6322.refseq.zip).</p>
<p>Center for Biotechnology Information (NCBI) Reference Sequence (RefSeq) genome database were downloaded using <rs id="a12973127" type="software">ncbi-genome-download</rs> v. <rs id="a12973128" type="version" corresp="#a12973127">0.3.0</rs> (<rs id="a12973129" type="url" corresp="#a12973127">https:// github.com/kblin/ncbi-genome-download</rs>, downloaded on 24 November 2021) with the '-assembly-levels complete' option. Genomes were subsampled to include one genome per genus. Downloaded genome phylum taxonomy was determined by cross-referencing the RefSeq genome ID with the <rs id="a12973130" type="software">GTDB-tk</rs> (R202 database) metadata.</p>
<p>The raw anaerobic digester sequencing data are available at the ENA with the bio project ID PRJEB48021, while the Zymo mock community raw sequencing data are available at PRJEB48692 (Supplementary Table 4). The UniProt TrEMBL database used in the study is available at https://ftp.uniprot.org/pub/databases/uniprot/ previous_releases/release-2021_01/knowledgebase. The <rs id="a12973131" type="software">GTDB-tk</rs> database used in the study is available at <rs id="a12973132" type="url" corresp="#a12973131">https://data.ace.uq.edu.au/public/gtdb/data/releases/ release202</rs>. Links for accessing the genome assemblies, MAGs</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f151393459"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T14:14+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>one usually defers to a simpler enumeration of likely arrangements using a technique that was originally developed by Hart et al. [61][62][63] for the enumeration of atomic orderings in alloys. However, the low-vacancy and low-lithium-concentration regimes require large simulations cells, and for intermediate concentrations the number of possible lithium/vacancy orderings even in small cells may become exceedingly large due to the combinatorial explosion, preventing the first principles energy evaluation of all configurations. Intuitively, one could argue that the most stable phases should be those lithium/vacancy orderings that distribute both species homogeneously over the available sites, as such arrangements minimise the electrostatic repulsion between positively charged lithium ions. Following this line of reasoning, Hautier et al. proposed to rank the enumerated configurations by their electrostatic energy, assuming fixed atomic charges, before considering only the N lowest energy configurations for first principles calculations. 64 This approach is generally useful to determine atomic orderings for crystal structures with fractional site occupancies: For example, Kim et al. employed the enumeration technique in conjunction with filtering by electrostatic energy to identify stable LiMn 0.5 Fe 0.375 Mg 0.125 BO 3 phases in which Mn, Fe and Mg share a common sublattice. 65 We note in passing that the <rs id="a12900001" type="software">Python
Materials Genomics</rs> package provides a convenient interface for the enumeration of atomic configurations and allows filtering by electrostatic energy. 66 Finite-temperature voltage profiles Much of the discrepancies between the experimental voltage curve and the piecewise computational approximation in Figure 2a arises from temperature effects that were neglected in the previous section. Indeed, some of the sodium/vacancy orderings (shown as black crosses in Figure 2a) that are not ground states are within a small energy above the convex hull, so that they may become accessible at finite temperatures.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f187347222"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:51+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>FRAP experiments were performed on a microscope (Eclipse Ti; Nikon) equipped with a spinning disk confocal imaging system (Ultraview Vox; PerkinElmer) and an electron-multiplying charged-coupled device camera (C9100-50; Hamamatsu Photonics), using a 100×, 1.4 NA oil objective. Cells were maintained at 37°C with humidity and CO 2 control. Images were acquired using <rs id="a12972047" type="software">Velocity</rs> <rs id="a12972048" type="version" corresp="#a12972047">6.6.1</rs> software. Three prebleach images at 2-s intervals and then a laser pulse at 100% power of the 488-nm line were used to bleach a circular region of 2-µm diameter. Time-lapse images were then acquired every 10 s for 4 min. Images were corrected for photobleaching during image acquisition, and normalized FRAP curves were plotted.</p>
<p>These analyses were done essentially as previously described (Grashoff et al., 2010). High resolution live FRET imaging was performed on an Eclipse Ti microscope equipped with an Ultraview Vox spinning disk confocal imaging system and an electron-multiplying charged-coupled device C9100-50 camera, using a 100×, 1.4 NA oil objective at 37°C with humidity and CO 2 control. Images were acquired using <rs id="a12972049" type="software">Velocity</rs> software. Three sequential images with 500-ms exposure times were acquired with the following filter combinations: donor (EGFP) channel with a 488-nm line (ex) and 527/55 (em), acceptor (or tagRFP) channel with a 561-nm line (ex) and 615/70 (em), and FRET channel with a 488-nm line (ex) and 615/70 (em). Donor leakage was determined from EGFP-transfected cells, whereas acceptor cross excitation was obtained from tagRFP-transfected cells. For all the calculations, respective background subtraction, illumination gradient, and pixel shift correction were performed followed by three-point smoothening. The slope of pixel-wise donor or acceptor channel intensity versus FRET channel intensity gives leakage (x) or cross-excitation (y) fraction, respectively. FRET map and pixel-wise FRET index for the sensors were determined from</p>
<p>FAs were thresholded using intensity and size cutoff criteria. Regions within the adhesions were used to obtain mean FRET index per cell. Student's t test was performed between the two groups to calculate statistical significance and p-value. At least P &lt; 0.05 was considered significant. For central versus peripheral FA FRET histograms, paired Student's t test was performed. Other confocal images were acquired also on the same microscope using 100×, 1.4 NA oil or 60×, 1.4 NA oil or 20×, 0.45 NA air objective at 37°C with humidity and CO 2 control for live cell images or at RT for fixed cells. For live cell imaging, phenol red minus fluorobrite DMEM (Gibco) with 10% FBS (Gibco) and penicillin-streptomycin media were used, whereas normal culture media was used for the rest of the live cell imaging. <rs id="a12972050" type="software">ImageJ</rs> ( <rs id="a12972051" type="publisher" corresp="#a12972050">National Institutes of Health</rs>) was used for basic image processing. All analyses were done using custom-written <rs id="a12972052" type="software" subtype="implicit" corresp="#a12972053">software</rs> (<rs id="a12972053" type="software" subtype="environment">MAT
LAB</rs><rs id="a12972054" type="version" corresp="#a12972053">R
2014a</rs>; <rs id="a12972055" type="publisher" corresp="#a12972053">Math-Works</rs>). Graphs were plotted in <rs id="a12972056" type="software">Origin</rs> ( <rs id="a12972057" type="version" corresp="#a12972056">9.1</rs>; 64 bit).</p>
<p>TCS PC FLIM images were acquired on a multiphoton microscope (Trim-scope 2; LaVision Biotec) using a 100×, 1.4 NA objective (Nikon). The excitation source was a system (Ultra 2 ti :sapphire; Coherent Chameleon) outputting a wavelength of 890 nm. All acquisition times were of the order of 20 s. The detector used was a cooled hybrid photomultiplier tube (Hamamatsu Photonics) in TCS PC mode. During imaging, cells were sealed using parafilm and kept at 37°C using a stage heater. Laser power levels were kept low enough to avoid saturated pixels within the image. For the analysis, the files were exported from <rs id="a12972058" type="software">InSpectorPro</rs> <rs id="a12972059" type="version" corresp="#a12972058">5</rs> as OME
-Tiffs and imported into the <rs id="a12972060" type="software">FLIMfit</rs> software tool developed at <rs id="a12972061" type="publisher" corresp="#a12972060">Imperial College London</rs>.</p>
<p>Frequency domain FLIM-total internal reflectance was performed using a FLIM attachment system (Lambert Instruments), coupled to an inverted microscope (TE-2000E; Nikon) with Perfect Focus. Total internal reflectance microscopy was performed using a 100×, 1.45 NA objective (Plan Apo; Nikon) and custom-built condenser, which delivers light from a 488-nm laser (DeepStar; Omicron) through an optical fiber into a conjugate plane of the objective back focal plane. Data were acquired and analyzed using <rs id="a12972062" type="software">LiFLIM</rs> software version <rs id="a12972063" type="version" corresp="#a12972062">1.1.11</rs>.</p>
<p>A high resolution microscopy-compatible thin layer of PDMS substrate on glass-bottom dishes was fabricated by spin coating 300 µl silicone at 6,000 rpm. Approximately 3 kPa (Style et al., 2014) was made by thoroughly mixing silicone and curing agent (CY-52-276A and CY-52-276B; Dow Corning) at a 1:1 ratio. Spin-coated dishes were kept at RT overnight for curing. Approximately 30 kPa stiff substrates (Ochsner et al., 2007) was made from Sylgard 184 (Dow Corning) by mixing base and curing agent in a 40:1 ratio; spin-coated dishes were kept at 80°C for 3 h for curing. These dishes were then UV treated in a culture hood for 20 min before coating with fibronectin. 1 kPa substrate (Gutierrez and Groisman, 2011) was prepared by mixing the A and B components of the silicone gel at a ratio of 1.2:1, whereas 1.3 MPa (Ochsner et al., 2007) substrates from Sylgard 184 was prepared by mixing base and curing agent in a 10:1 ratio. To carry out traction force microscopy (Mertz et al., 2013;Style et al., 2014), spin-coated thin ∼3-kPa PDMS-layered glass substrate was prepared as described above. However, this PDMS layer was sandwiched between two layers of fluorescent beads: a bottom layer (F8797; Thermo Fisher Scientific) on glass that serves as fiduciary markers to determine the PDMS thickness, and a top layer (F8807; Thermo Fisher Scientific) on PDMS that serves as the markers to track displacement caused by cell traction. Top bead images with and without cells (removed by adding 0.1% SDS in PBS) were acquired, and traction force measurement and total work done was calculated using custom-written <rs id="a12972064" type="software" subtype="environment">MAT LAB</rs> <rs id="a12972065" type="software" subtype="implicit" corresp="#a12972064">code</rs> <rs id="a12972066" type="bibr">(Mertz et al., 2012(Mertz et al., , 2013)</rs> from E. Dufresne's laboratory. We thank E. Dufresne and R. Boltyanskiy (Yale University, New Haven, CT) for their instruction and assistance with this procedure.</p>
<p>Validation of mutants that disrupt ABS2 in the talin rod Sequence conservation of talin 913-1044 and 1461-1580. The sequences of talin residues 913-1944 (R4) and 1461-1580 (R8) were aligned across species. The alignment was performed using <rs id="a12972067" type="software">T-Coffee</rs> <rs id="a12972068" type="bibr">(Notredame et al., 2000)</rs> using the following sequences: Mus musculus talin-1, M. musculus talin-2, Homo sapiens, Gallus gallus, Danio rerio, Drosophila melanogas-ter, and Caenorhabditis elegans. The map of surface-exposed conserved residues is shown in Fig. S5 L (invariant, red; conserved, yellow).</p>
<p>Fig. S1 shows additional characterization of the talin sensor and talin1 -/-cell line. Fig. S2 contains additional controls for talin-TS. It also shows characterization of peripheral and central FAs in terms of talin tension. Fig. S3 shows the compositional difference in central and peripheral FAs. Fig. S4 shows the differential amount of integrin β3 in central and peripheral FAs and its correlation with FRET in talin-TS. Correlation between amount of integrin β1 in FAs and substrate stiffness is also shown. Fig. S5 shows effect of substrate stiffness on tension in vinculin. It also contains characterization of ABS2 mutant talin and various comparisons between talin-TS, ABS3 mutant talin-TS, and ABS2 mutant talin-TS. Online supplemental material has <rs id="a12972069" type="software" subtype="environment">MAT LAB</rs> <rs id="a12972070" type="software" subtype="implicit" corresp="#a12972069">code</rs>, included in a zip file, used to calculate FRET index from raw intensity images. The <rs id="a12972071" type="software" subtype="implicit">code</rs> also generates the FRET index map and the pixel-wise histogram of FRET indices, as well as mean FRET index per cell. Any channel intensity corresponding to pixels, where FRET is calculated, can also be obtained using this <rs id="a12972072" type="software" subtype="implicit">code</rs>. Online supplemental material is available at http ://www .jcb .org /cgi / content /full /jcb .201510012 /DC1.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f210960347"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T14:20+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Hammerhead activity with gel electrophoresis. Substrate cleavage assays were carried out in cleavage buffer (10 mM Tris-HCl, pH 8.3, and 4 mM MgCl 2 ) at 25 °C under single turnover conditions, i.e., with stoichiometric amounts of ribozyme and substrate; in this case, the ribozyme concentration is 2× the concentration of substrate. An excess of HH-min (1 μM) compared to FRET-substrate (0.5 μM) were mixed in RNAse-free water, the reaction was initiated by the addition of cleavage buffer. Aliquots were taken at varying time points and quenched on ice by the addition of four volumes of RNA gel loading buffer (formamide, EDTA (10 mM, pH 8.0), bromophenol blue (0.025% w/v). The substrate and cleavage products were separated by denaturing PAGE on 20% acrylamide gels and run in 1× TBE buffer. Bands were visualised by FAM-tag fluorescence (Typhoon FLA-5000, GE Healthcare Life Sciences, λ ex = 473 nm, λ em = 520 nm), and the intensities of cleavage product at each time point were determined by integration of band intensities using <rs id="a12967906" type="software">ImageQuant</rs>.</p>
<p>Gel electrophoresis of hammerhead activity in coacervates. HH-min and FRET-substrate at final concentrations of 1 µM and 0.5 µM, respectively, were loaded into CM-Dex:PLys (4:1 final molar ratio) bulk coacervate phase for single turnover conditions, i.e., with stoichiometric amounts of ribozyme and substrate; in this case, the ribozyme concentration is 2× the concentration of substrate. RNA was extracted from 5 µl of CM-Dex:PLys (4:1 final molar ratio) bulk coacervate phase (25 °C) or from 5 µl of CM-Dex:PLys (4:1 final molar ratio) coacervate microdroplet dispersions after 900 min (25 °C) by sequential addition of 5 µl 1 M NaCl (final concentration 4.8 mM), 5 µl of 1.25 M hexametaphosphate (final concentration -6.0 mM) and 90 µl RNA loading buffer (final volume-83%) containing EDTA (final concentration-8 mM) and Orange G and 10 s of vortexing and 1 s of centrifugation between each addition. The reaction mixture was heated at 80 °C for 10 min, centrifuged and placed on ice for at least 5 min. 10 µl of the reaction mixture was loaded into a pre-run 20% polyacrylamide gel and then run at 300 V in 1× TBE buffer until the dye had run to the bottom of the gel. The gel was imaged using Typhoon FLA-9500, GE Healthcare Life Sciences, with λ ex = 473 nm and λ em = 520 nm. Band intensities were measured using <rs id="a12967907" type="software">ImageQuant</rs> at a specific time point and uncleaved substrate was corrected by the FRET effect factor (1.69) (Supplementary Fig. 12). The fraction of cleaved substrate of the total sum of cleaved and uncleaved substrate was determined and used to normalise kinetic data obtained from spectroscopy or microscopy at specific time points. Gel electrophoresis was undertaken with FRET-substrate and FAM-substrate with a singlestrand RNA molecular marker on a pre-run 20% polyacrylamide gel and run at 15 W in 1× TBE buffer. The gel was stained with SYBR gold and imaged using a Typhoon FLA-5000, GE Healthcare Life Sciences, with λ ex = 473 nm and λ em = 520 nm. Uncropped gels are shown in Supplementary Figure 15.</p>
<p>Fluorescence recovery after photobleaching. Fluorescence recovery after photobleaching was undertaken within both CM-Dex:PLys and PLys:ATP coacervate microdroplets (4:1 final molar ratio) and the bulk coacervate phase (CM-Dex:PLys 4:1 final molar ratio) containing either TAM-HH-min (0.36 µM), FAM-substrate (0.36 µM), FAM-substrate (24.3 µM) or FAM-cleaved substrate (0.36 µM). Samples were prepared as previously described and loaded into capillary slides mounted in a Zeiss LSM 880 inverted single point scanning confocal microscope equipped with a 32 GaAsP PMT channel spectral detector and a 32-channel Airy Scan detector and imaged using a 63× oil immersion objective (Plan-Apochromat 63× 1.4 Oil DIC, Zeiss). Bleaching was achieved by additional excitation with a 405 nm laser diode and 355 nm DPSS laser, an Argon Multiline Laser produced the excitation wavelength of λ FAM = 488 nm or λ TAM = 561 nm and emission wavelengths λ FAM = 479-665 nm (Laser line blocking pin at 488 nm) or λ TAM = 562-722 nm. Imaging time varied depending on the region of interest but was typically between 12 ms/ frame and 100 ms/frame. The fluorescence intensity as a function of time for the bleached area, reference and the background was obtained using <rs id="a12967908" type="software">FIJI</rs> and the recovery of the bleached region was normalised against the background and the reference region for either bulk CM-Dex:PLys (4:1 final molar ratio) coacervate or PLys:ATP (4:1 final molar ratio) coacervate experiments. An additional normalisation for droplet-based FRAP was undertaken by dividing the fluorescence by the fluorescence of the whole droplet 47,48 . The kinetic profiles were fit to Eq. 5 using <rs id="a12967909" type="software">MATLAB</rs> to obtain the time constant, τ, of fluorescence recovery or of transport into the droplet (whole droplet FRAP).</p>
<p>Data supporting the findings of this manuscript are available from the corresponding authors upon reasonable request. Computer <rs id="a12967910" type="software" subtype="implicit">code</rs> can be downloaded at <rs id="a12967911" type="url" corresp="#a12967910">https://doi.org/ 10.17617/1.6J</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f82371767"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T14:20+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Sustainability considerations in manufacturing scheduling, which is traditionally influenced by service oriented performance metrics, have rarely been adopted in the literature. This paper aims to address this gap by incorporating energy consumption as an explicit criterion in shop floor scheduling. Leveraging the variable speed of machining operations leading to different energy consumption levels, we explore the potential for energy saving in manufacturing. We analyze the trade-off between minimizing makespan, a measure of service level and total energy consumption, an indicator for environmental sustainability of a two-machine sequence dependent permutation flowshop. We develop a mixed integer linear multi-objective optimization model to find the Pareto frontier comprised of makespan and total energy consumption. To cope with combinatorial complexity, we also develop a constructive heuristic for fast trade-off analysis between makespan and energy consumption. We define lower bounds for the two objectives under some non-restrictive conditions and compare the performance of the constructive heuristic with <rs id="a12895891" type="software">CPLEX</rs> through design of experiments. The lower bounds that we develop are valid under realistic assumptions since they are conditional on speed factors. The Pareto frontier includes solutions ranging from expedited, energy intensive schedules to prolonged, energy efficient schedules. It can serve as a visual aid for production and sales planners to consider energy consumption explicitly in making quick decisions while negotiating with customers on due dates. We provide managerial insights by analyzing the areas along the Pareto frontier where energy saving can be justified at the expense of reduced service level and vice versa.</p>
<p>• introducing the concept of green scheduling as a new approach to shop floor scheduling; • developing a novel multi-objective mathematical model, taking into account energy consumption as an explicit decision criterion by leveraging variable processing times; • defining lower bounds on total energy consumption and makespan for benchmarking; • developing a new heuristic algorithm to find a good approximation of Pareto optimal solutions in a short amount of time; • validating the performance of the heuristic algorithm through comprehensive experiments and benchmarking with <rs id="a12895892" type="software">CPLEX</rs> based on three performance metrics: accuracy, diversity and cardinality of the Pareto frontiers; • providing the managerial implications of green scheduling for production planners and sales managers of manufacturing companies.</p>
<p>To demonstrate the conflict between minimizing C max and TEC, we solved a small problem with six jobs through -constraint approach using <rs id="a12895893" type="software">CPLEX</rs> <rs id="a12895894" type="version" corresp="#a12895893">12.5</rs>. In this approach, minimizing C max was considered the objective and TEC as a constraint. In this example, processing speed factor was v = {1.2, 1, 0.8} for processing at fast, normal, and slow speeds, respectively. The conversion factor, which we used to approximate the energy consumed during the operation, was λ = {1.5, 1, 0.6} for fast, normal, and slow processing speeds, respectively.</p>
<p>The constructive heuristic was coded in <rs id="a12895895" type="language">C++</rs> and run on an Intel Xeon CPU 3.50 GHz with 32.0 GB RAM under
Windows 7 Enterprise. Moreover, we used <rs id="a12895896" type="software">CPLEX</rs> <rs id="a12895897" type="version" corresp="#a12895896">12.5</rs> in <rs id="a12895925" type="software">Concert Technology</rs> to code the MILP model in <rs id="a12895898" type="language">C++</rs>. Graphs and statistical analyses were performed on a MacBook Pro with Intel Core i7 2.2 GHz processor and OS X version 10.9.3 running <rs id="a12895899" type="software">RStudio</rs> version <rs id="a12895900" type="version" corresp="#a12895899">0.97.551</rs> and <rs id="a12895901" type="language">R</rs> version <rs id="a12895902" type="version" corresp="#a12895901">3.0.3</rs>. For fair comparison, we first solved all problems using CH and then allowed <rs id="a12895903" type="software">CPLEX</rs> to run under -constraint for at least the same time that CH had spent on that problem size. Our experiments showed that allowing <rs id="a12895904" type="software">CPLEX</rs> to run for 7n seconds for a problem with n jobs gives <rs id="a12895905" type="software">CPLEX</rs> comparable time to that of CH. To allow for exploration of the Pareto frontier and to avoid spending too much time at any level, we set a limit for 10% of the total time for each level before proceeding with the reduced value. Incidentally, in deciding on the time spent at each stage, there is a trade-off among the three performance metrics, i.e. DLB, DVR, and CRD. More time at any given level would allow <rs id="a12895906" type="software">CPLEX</rs> to improve DLB but at the expense of less iterations and hence lower DVR and lower CRD. We examined a number of values and observed that 10% provides a fair opportunity for exploration and exploitation of the search space at the same time. The best solution found at each stage was archived and ultimately filtered to obtain the set by removing dominated solutions.</p>
<p>The total number of replications, 30, provided a statistical power of 0.862 at the significance level of 0.01 that can detect even a small effect size (0.20) as suggested by Cohen (1992). This power is comparable to the power in Shin &amp; Benton's (2004) study. For power calculation, we used <rs id="a12895899A" type="software">G * Power</rs> <rs id="a12895899B" type="version" corresp="#a12895899A">3</rs> (Faul, Erdfelder, Lang, &amp; Buchner, 2007).</p>
<p>Fig. 2 shows the CPU usage of <rs id="a12895907" type="software">CPLEX</rs> for small problems. The exponential increase in CPU time is visible in problems with even 4-6 jobs. We can also see the exponential growth in the <rs id="a12895908" type="software">CPLEX</rs>'s solution time.</p>
<p>• Algorithms. Two levels: CH and <rs id="a12895909" type="software">CPLEX</rs>.</p>
<p>Table 5 summarizes the mean and standard deviation of the DLB achieved by CH and <rs id="a12895910" type="software">CPLEX</rs> for the same problem instances. Fig. 4 shows that variation of setup time has an impact on the DLB. Increasing setup time variation degrades the accuracy. However n does not affect the DLB as much, particularly for the CH algorithm as can be seen in Table 5, for the same Setup to processing time distribution, the average DLB for each n is around the same figure; e.g. for Setup to processing ratio of 25% the minimum average DLB is 12.64% and the maximum average DLB is 12.86%. It should be noted that the lower bound developed in Section 3.4 is rather conservative. Small problems solved to optimality have a DLB of approximately 9.5% (see Table 4). As such, part of the distances with lower bound (DLB) reported in Table 5 could be attributed to the looseness of the lower bound to gain a more realistic idea of the performance of the solution methods. It should be noted that the true Pareto frontiers in our problem are unknown for large-even medium-sized problem instances and cannot be found using exact optimization methods. As a result, both CH and <rs id="a12895911" type="software">CPLEX</rs> (with limited execution time) find approximations of true Pareto frontiers.</p>
<p>To compare DVR of the two solution approaches, we first calculate the 'nominal diversity' of each approach using Eq. 19. The 'nominal diversity' figures are then normalized using Eq. 22 for a given solution method (SM) where SM ∈ {<rs id="a12895912" type="software">CPLEX</rs>, CH} for each problem instance. We report the mean and standard deviation of the diversity as a percentage in Table 6 and Fig. 5 for CH and <rs id="a12895913" type="software">CPLEX</rs> for the same problem instances.</p>
<p>Table 7 and Fig. 6 report the performance of the CH and <rs id="a12895914" type="software">CPLEX</rs> in terms of the spacing metric. Spacing is influenced by both the number of jobs and the setup times for the CH. The heuristic shows better SPC for problems with larger setup to processing time ratios. In the meantime, <rs id="a12895915" type="software">CPLEX</rs> performs better in terms of spacing when the setup to processing time ratio is smaller. Overall, CH performs better in terms of SPC compared to <rs id="a12895916" type="software">CPLEX</rs>.</p>
<p>Table 8 and Fig. 7 report the number of non-dominated solutions found by the CH algorithm and the <rs id="a12895917" type="software">CPLEX</rs> for the same problem instances. Cardinality is influenced by the number of jobs. The larger the problem is, the more solutions both the CH and the <rs id="a12895918" type="software">CPLEX</rs> can find. CH was able to find more solutions in less time. However, these solutions should be interpreted under the light of the diversity as well.</p>
<p>We compare the performance of CH and <rs id="a12895919" type="software">CPLEX</rs> on the four performance metrics (DLB, DVR, SPC, and CRD) using the non-parametric Wilcoxon signed rank test instead of the paired Student's t-test because the metrics were not normally distributed for each level of n and each level of Setup. We report the mean rank difference in the respective performance followed by the probability that this difference is different from zero in parentheses. In these comparisons we assumed a significance level of 0.01. Table 9 shows the comparison of DLB, DVR, SPC and CRD performance of <rs id="a12895920" type="software">CPLEX</rs> and CH. In terms of DLB, CH performs worse than <rs id="a12895921" type="software">CPLEX</rs> only for problems with 20 jobs under the setup to processing time ratio of 25%. In terms of DVR, CH performs better than <rs id="a12895922" type="software">CPLEX</rs> in all cases. In terms of SPC, there is no difference between CH and <rs id="a12895923" type="software">CPLEX</rs> for problems with 20, 80 and 120 jobs under setup to processing time ratio of 25% and also for problems with 20 jobs under setup to processing time ratio of 50%. In terms of CRD, CH performs better than <rs id="a12895924" type="software">CPLEX</rs> in all cases.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f345239694"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:37+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Dose calculation engines designed for treatment planning at MRgRT systems should account for the impact of the magnetic field on the dose from electrons, and be sufficiently fast not to hinder online plan adaptation procedures. The magnetic field will reduce the build-up length, cause a shifted and asymmetric penumbra as well as the electron return effect and may increase skin dose away from the field by deflecting contaminating electrons [106,[133][134][135][136][137]. While Monte Carlo codes such as <rs id="a12971859" type="software">Geant4</rs>, <rs id="a12971860" type="software">PENELOPE</rs>, <rs id="a12971861" type="software">MCNP</rs> and <rs id="a12971862" type="software">EGSnrc</rs> are well benchmarked for conventional radiotherapy, and offer magnetic field capabilities, their emphasis on accuracy renders them unsuitable for treatment planning due to long calculation times. Algorithms making approximations or using variance reduction techniques have thus been developed as an alternative, and been used in conventional treatment planning systems [138][139][140][141][142][143][144]. For the 1.5 T Elekta MR-linac, <rs id="a12971863" type="software">GPUMCD</rs>, a GPU-based Monte Carlo engine, has been included in the Monaco TPS and shown to allow fast optimization <rs id="a12971864" type="bibr">[143]</rs>. <rs id="a12971865" type="software">GPUMCD</rs> uses four tissue classes (air, lung, soft tissue and bone) and a CT number to electron density conversion. For the 0.35 T ViewRay system, the <rs id="a12971866" type="software">KMC</rs> engine has been developed as an improvement to <rs id="a12971867" type="software">VMC</rs> <rs id="a12971868" type="bibr">[145]</rs><rs id="a12971869" type="bibr">[146]</rs><rs id="a12971870" type="bibr">[147]</rs> to achieve fast calculation speeds. Studies have also been published where research dose calculation engines have been developed and used to compare to the clinical implementations provided by the vendors [148,149]. <rs id="a12971871" type="publisher" subtype="person" corresp="#a12971872">Wang et al.</rs> developed a GPU-based Monte Carlo simulation <rs id="a12971872" type="software" subtype="implicit">platform</rs> for the ViewRay MRIdian using 60 Co <rs id="a12971873" type="bibr">[148]</rs>. The platform was based on a translation of <rs id="a12971875" type="software">penelope</rs> from <rs id="a12971876" type="language" corresp="#a12971875">Fortran</rs> to <rs id="a12971877" type="language" corresp="#a12971878">C++</rs>, and was called <rs id="a12971878" type="software">gpenelope</rs>. They reported calculations times improved by a factor of 152 compared to <rs id="a12971879" type="software">Penelope</rs>, and pass rates for <rs id="a12971880" type="software">KMC</rs> vs <rs id="a12971881" type="software">gpenelope</rs> of 99.1%±0.6% (2%/2 mm). Good agreement with measurements was obtained. Ahmad et al. compared <rs id="a12971882" type="software">GPUMCD</rs> to <rs id="a12971883" type="software">Geant4</rs> in the presence and absence of a 1.5 T magnetic field <rs id="a12971884" type="bibr">[149]</rs>. They however did not model the source explicitly and used a point source with a 7 MV spectrum instead. For various combinations of heterogeneities good agreement was observed between <rs id="a12971885" type="software">Geant4</rs> and <rs id="a12971886" type="software">GPUMCD</rs>.</p>
<p>Until today, target delineation, dose calculations, plan optimization and QA of the adapted plan in MRgRT contribute to considerably increase the workload with respect to conventional RT. Thus, pre-treatment plan adaptation is naturally better suited to treatments with few fractions, which is why anatomical sites where hypofractionation is indicated, such as prostate [187], pancreas [151], lung [25], liver and adrenal gland [188] as Fig. 4 Bottom rowmajor steps of the online adaptive radiotherapy process for MRgRT. Top rowthe associated QA tasks for each step. QA tasks highlighted in orange and italic are the manual checks. QA tasks highlighted in green represent automated checks. The acronyms <rs id="a12971887" type="software">VRART</rs> and <rs id="a12971888" type="software">VRADQ</rs> refer to specific in-house developed software packages. Reprinted with permission from [186] well as oligometastases [21], are preferred. More details the clinical indications MRgRT have been recently discussed by Corradini et al. [189]. Both the substantially increased workload, as well as the focus on treatment sites allowing for hypofractionated treatment, limit the routine clinical use of MRgRT. Future medical physics developments streamlining the MRgRT workflow, e.g., in terms of more accurate and robust pseudo-CT generation and improved automatic contour suggestion by means of deep learning, might play an important role to pave the way towards widespread clinical use of MRgRT.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f77014166"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:58+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The channeling minimum on the right is narrower because the critical channeling angle is about 4 times smaller than at injection energy. The loss reduction in channeling is about 24. The second minimum on the left in the VR region is clearly seen here. This minimum was also observed in our studies with bent crystals at the SPS. The minimum is at an angular distance from the channeling orientation about equal to the bend angle value, 65 μrad. In this case, the whole VR region is on the same side relative to the beam envelope direction. Therefore, angular kicks due to VR always increase the oscillation amplitudes of particles and they more quickly reach the secondary collimators. Curve 2 shows the dependence of the number of inelastic nuclear interactions on the crystal orientation obtained by simulation according to [19]. The agreement with the experiment is good enough. The walls of the loss reduction dependence coincide well with the experimental ones. This means that the crystal bend angle measured by the collimator scan is right (the angular crystal position for this scan was chosen close to the perfect one for channeling). The loss reduction for VR region is the same as in the experiment. However, the loss reduction value for channeling, R = 187, is considerably larger than the experimental one. This difference may be explained by the residual angular instabilities of the goniometer. Multi-turn simulation with a <rs id="a12970101" type="software">SixTrack</rs>
code, including other collimators in all ring inser-tions and a detailed beam aperture model [13], gives also good agreement with the angular scan data (curve 3), although the loss reduction value, R = 140, is also larger than the experimental one.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f458055361"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T10:13+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>To confirm the interaction, we performed pulldowns using K12 strains with endogenously tagged ORFs of YacL, NusG and RpoB to carry an affinity-tag. YacL affinity-enriched RNAP and NusG (Fig. 4c) and, conversely, NusG and RpoB enriched YacL, thus confirming the association of YacL with NusG and RNAP (Supplementary Fig. 10). To further constrain the binding site of YacL on the RNAP and confirm the interaction site by use of a different crosslinker we crosslinked these affinity-enriched complexes using the photoactivatable crosslinker sulfo-SDA, which can provide a higher density of crosslinks than DSSO or BS3 24 . Sulfo-SDA crosslinking either of the three affinityenriched proteins confirmed the direct binding of YacL to the RNAP and NusG with a combined 14 unique residue pairs (Supplementary Fig. 10 and Supplementary Data 3-5). The total of 20 residue pairs from DSSO, BS3 and sulfo-SDA thus constrain the binding site of YacL to RNAP and NusG. Sixteen of these residue pairs were between our <rs id="a12893494" type="software">I-TASSER</rs> model of YacL and regions of RNAP-NusG included in the solved structure (PDB 6C6U [https://doi.org/10.2210/pdb6c6u/pdb]). These were used in <rs id="a12893495" type="software">DisVis</rs> to calculate the accessible interaction space and localize YacL on RNAP next to NusG at the DNA exit site (Fig. 4d).</p>
<p>In this work, we experimentally demonstrated that Crosslinking MS can reliably identify PPIs using the target-decoy approach as a quantitative error metric. Decoys are only a model of false positives with a number of underlying assumptions 16 and they cannot model false positives that do not arise from spectral matching, such as peptides noncovalently associating during LC-MS 28 . Considering these caveats, it is reassuring that our four different controls closely agree with the outcome of the target-decoy approach. This negates the need for any additional heuristics suggested by others 27 . We showed that the target-decoy approach requires separating self and heteromeric crosslinks and that error should be estimated for the information level that is being reported. For example, when reporting residue pairs for structural analyses of individual protein complexes, residue pair-level FDR should be applied. However, when reporting PPIs, CSMs need to be merged to PPI level prior to FDR estimation. Other ways of merging CSM scores into PPI scores from the one we use here are possible. However, for accurate PPI error estimation, these methods would need to adhere to the two fundamental considerations. These concepts were implemented in our opensource FDR estimation software tool, <rs id="a12893496" type="software">xiFDR</rs> v <rs id="a12893497" type="version" corresp="#a12893496">2.0</rs>, which is crosslink search software independent. The large dataset presented here, with its internal controls, will allow testing of other aspects of the Crosslinking MS workflow in the future.</p>
<p>LC-MS protein identification with non-crosslinked samples. Protein identifications in SEC fractions and from pull-down experiments via LC-MS were conducted using a Q Exactive HF mass spectrometer (Thermo Fisher Scientific, Bremen, Germany) coupled to an Ultimate 3000 RSLC nano system (Dionex, Thermo Fisher Scientific, Sunnyvale, USA), operated under <rs id="a12893498" type="software">Tune</rs> <rs id="a12893499" type="version" corresp="#a12893498">2.9</rs>, <rs id="a12893586" type="software" subtype="component" corresp="#a12893587">SII</rs> for <rs id="a12893587" type="software" subtype="environment">Xcalibur</rs> <rs id="a12893501" type="version" corresp="#a12893587">1.4</rs> and <rs id="a12893502" type="software">Xcalibur</rs> <rs id="a12893503" type="version" corresp="#a12893502">4.1</rs>. 0.1% (v/v) formic acid and 80% (v/v) acetonitrile, 0.1% (v/v) formic acid served as mobile phases A and B, respectively. Samples were loaded in 1.6% acetonitrile, 0.1% formic acid on an Easy-Spray column (C18, 50 cm, 75 µm ID, 2 µm particle size, 100 Å pore size) operated at 45 °C and running with 300 nl/min flow. Peptides were eluted with the following gradient: 2 to 6% buffer B in 1 min, 6 to 10% B in 2 min, 10 to 30%B in 37 min, 30 to 35% in 5 min followed by 35 to 45%B in 2 min. Then, the column was set to washing conditions within 1.5 min to 90% buffer B and flushed for another 5 min. For the mass spectrometer the following settings were used: MS1 scans resolution 120,000, AGC (automatic gain control) target 3 × 10 6 , maximum injection time 50 ms, scan range from 350 to 1600 m/z. The ten most abundant precursor ions with z = 2-6, passing the peptide match filter ("preferred") were selected for HCD (higher-energy collisional dissociation) fragmentation employing stepped normalized collision energies (29 ± 2). The quadrupole isolation window was set to 1.6 m/z. Minimum AGC target was 2.5 × 10 4 , maximum injection time was 80 ms. Fragment ion scans were recorded with a resolution of 15,000, AGC target set to 1 × 10 5 , scanning with a fixed first mass of 100 m/z. Dynamic exclusion was enabled for 30 s after a single count and included isotopes. Each LC-MS acquisition took 75 min.</p>
<p>Quantitative proteomics database search. Raw data from bottoms-up proteomics experiments were processed using <rs id="a12893504" type="software">MaxQuant</rs> <rs id="a12893505" type="bibr">29</rs> version <rs id="a12893506" type="version" corresp="#a12893504">1.6.0.16</rs> operated under default settings (fully tryptic digestion with two missed cleavages maximum; up to five variable modifications per peptide (oxidised methionine and acetylated protein N-termini), MS1 match tolerance 20 ppm (first search)/4.5 ppm (main search), MS/MS match tolerance 20 ppm); carbamidomethylation of cysteine set as fixed modification; 1% PSM and protein group FDR). Each SEC fraction or pulldown replica injection was treated as an individual experiment. Quantitation by iBAQ 30 requiring a minimum of two peptides (unique + razor) and matching between runs were enabled. For data from pull-down experiments, label-free quantitation was enabled with default settings (LFQ minimum ratio count of 2, Fast LFQ enabled, minimum number/average of neighbour 2/6, stabilize large LFQ ratios and requirement for MS2 for LFQ comparisons enabled). Supernatant samples from cell lysis were included to increase absolute protein identifications via the matching between runs feature. The database used was the Uniprot curated reference proteome UP000000625 with two unreviewed entries removed summing to a total 4350 proteins (retrieved on 04/08/2019).</p>
<p>Protein enrichment from pull-down experiments was assessed using <rs id="a12893507" type="software">Perseus</rs> <rs id="a12893508" type="bibr">31</rs> version <rs id="a12893509" type="version" corresp="#a12893507">1.5.6.0</rs>. Proteins identified by site only, reverse hits and contaminants were filtered out. LFQ protein quantitation data was log2-transformed and filtered to contain three valid values in at least one experiment (e.g. in any TEV eluate). Missing values were imputed on the total matrix with default settings (width: 0.3, downshift 1.8). Volcano plots comparing TEV eluates of targeted affinity enrichment with K12 wildtype mock enrichment were created using a two-sided, two-sample t-test with 1% FDR and an artificial variance S0 of 2. For highresolution figures, the matrix and the cut-off curve were exported to reproduce the plots in <rs id="a12893510" type="software" subtype="environment">python</rs> <rs id="a12893511" type="version" corresp="#a12893510">3.7</rs> with <rs id="a12893512" type="software" subtype="component" corresp="#a12893510">pandas</rs> <rs id="a12893513" type="version" corresp="#a12893512">0
.24.2</rs> using the <rs id="a12893514" type="software" subtype="component" corresp="#a12893512">seaborn</rs> <rs id="a12893515" type="version" corresp="#a12893514">0.9.0</rs> package.</p>
<p>LC-MS for crosslink identification. LC-MS analysis of crosslinked peptides derived from the SEC-separated E. coli proteome and multidimensional fractionation was performed using a Q Exactive HF mass spectrometer (Thermo Fisher Scientific, Bremen, Germany) coupled to an Ultimate 3000 RSLC nano system (Dionex, Thermo Fisher Scientific, Sunnyvale, USA), operated under <rs id="a12893516" type="software">Tune</rs> <rs id="a12893517" type="version" corresp="#a12893516">2.11</rs>, <rs id="a12893588" type="software" subtype="component" corresp="#a12893589">SII</rs> for <rs id="a12893589" type="software" subtype="environment">Xcalibur</rs> <rs id="a12893519" type="version" corresp="#a12893589, 12893520">1.5</rs> and <rs id="a12893520" type="software">Xcalibur</rs> <rs id="a12893521" type="version" corresp="#a12893520">4.2</rs>. Mobile phases A and B consisted of 0.1% (v/v) formic acid and 80% (v/v) acetonitrile, 0.1% (v/v) formic acid, respectively. Samples were loaded in 1.6% acetonitrile, 0.1% formic acid on an Easy-Spray column (C18, 50 cm, 75 µm ID, 2 µm particle size, 100 Å pore size) running at 300 nl/min flow and kept at 45 °C. Analytes were eluted with the following gradient: 2 to 7.5% buffer B in 5 min, followed by a linear 80-min gradient of 7.5 to 42.5% and an increase to 50% B over 2.5 min. Then, the column was set to washing conditions within 2.5 min to 95% buffer B and flushed for another 5 min. The massspectrometric settings for MS1 scans used were: resolution set to 120,000, AGC of 3 × 10 6 , maximum injection time of 50 ms, scanning from 400-1450 m/z in profile mode. The ten most intense precursor ions that passed the peptide match filter ("preferred") and with z = 3-6 were isolated using a 1.4 m/z window and fragmented by HCD using in-house optimized stepped normalized collision energies (BS3: 30 ± 6; DSSO: 24 ± 6). Fragment ion scans were acquired at a resolution of 60,000, AGC of 5 × 10 4 , maximum injection time of 120 ms scanning from 200-2000 m/z, underfill ratio set to 1%. Dynamic exclusion was enabled for 30 s (including isotopes). In-source-CID was enabled at 15 eV to minimize gas-phase associated peptides 28 . Each LC-MS run took 120 min.</p>
    <p>For LC-MS/MS analysis of sulfo-SDA crosslinked samples, we used an Orbitrap Fusion Lumos Tribrid mass spectrometer (Thermo Fisher Scientific, Germany) connected to an Ultimate 3000 RSLCnano system (Dionex, Thermo Fisher Scientific, Germany), which were operated under <rs id="a12893522" type="software">Tune</rs> <rs id="a12893523" type="version" corresp="#a12893522">3.4</rs>, <rs id="a12893590" type="software" subtype="component" corresp="#a12893591,12893526">SII</rs> for <rs id="a12893591" type="software" subtype="environment">Xcalibur</rs> <rs id="a12893525" type="version" corresp="#a12893591">1.6</rs> and <rs id="a12893526" type="software">Xcalibur</rs> <rs id="a12893527" type="version" corresp="#a12893526">4.4</rs>. Fractions from SEC were resuspended in 1.6% acetonitrile 0.1% formic acid and loaded onto an EASY-Spray column of 50 cm length (Thermo Scientific) running at 300 nl/min. Gradient elution using water with 0.1% formic acid and 80% acetonitrile with 0.1% formic acid was accomplished using optimised gradients for each SEC fraction (from 2-18% mobile phase B to 37.5-46.5% over 90 min, followed by a linear increase to 45-55 and 95% over 2.5 min each). Each fraction was analysed in duplicate. The settings of the mass spectrometer were as follows: Data-dependent mode with 2.5s-Top-speed setting; MS1 scan in the Orbitrap at 120,000 resolution over 400 to 1500 m/z with 250% normalized AGC target; MS2 scan trigger only on precursors with z = 3-7+, AGC target set to "standard", maximum injection time set to "dynamic"; fragmentation by HCD employing a decision tree logic with optimised collision energies 34,35 ; MS2 scan in the Orbitrap at resolution of 60,000; dynamic exclusion was enabled upon a single observation for 60 s.</p>
<p>Crosslink database search for BS3 and DSSO. Raw data from mass spectrometry were processed using <rs id="a12893528" type="software">msConvert</rs> (version <rs id="a12893529" type="version" corresp="#a12893528">3.0.11729</rs>) <rs id="a12893530" type="bibr">36</rs> including denoising (top 20 peaks in 100 m/z bins) and conversion to mgf-file format. Precursor masses were re-calibrated to account for mass shifts during measurement. Obtained peak files were analysed using <rs id="a12893531" type="software">xiSEARCH</rs> <rs id="a12893532" type="version" corresp="#a12893531">1.6.746</rs> <rs id="a12893533" type="bibr">5</rs> with the following settings: MS1/MS2 error tolerances 3 and 5 ppm, allowing up to two missing isotope peaks 37 , tryptic digestion specificity with up to two missed cleavages, carbamidomethylation on cysteine as fixed and oxidation on methionine as variable modification, losses: -CH 3 SOH/-H 2 O/-NH 3 , crosslinker BS3 (138.06807 Da linkage mass) or DSSO (158.0037648 Da linkage mass) with variable crosslinker modifications on linear peptides ("BS3-NH2" 155.09463 Da, "BS3-OH" 156.07864 Da, "DSSO-NH2" 175.03031 Da, "DSSO-OH" 176.01433 Da).<rs id="a12893534" type="software" subtype="environment">xiSEARCH</rs> <rs id="a12893535" type="software" subtype="implicit" corresp="#a12893534">algorithms</rs> are identical for both crosslinkers (BS3 and DSSO). For samples crosslinked with DSSO, additional loss masses for crosslinker-containing ions were defined accounting for its cleavability ("A" 54.01056 Da, "S" 103.99320 Da, "T" 85.98264). Matches were not filtered for having DSSO-specific signature peaks. Crosslink sites for both reagents were allowed for side chains of Lys, Tyr, Ser, Thr and the protein N-terminus. Note that we included a non-covalent crosslinker with a mass of zero to flag spectra potentially arising from gas-phase associated peptides. These spectra were removed prior to false-discovery-rate (FDR) estimation 28 .</p>
<p>FDR calculation for BS3 and DSSO datasets. Results were filtered prior to FDR to crosslinked peptide matches having a minimum of three matched fragments per peptide, a delta score of 15% of the match score and a peptide length of at least six amino acids. Additionally, identifications ambiguously matching to two proteins or more were removed. FDR was calculated based on decoy matches by <rs id="a12893536" type="software">xiFDR</rs> (version <rs id="a12893537" type="version" corresp="#a12893536">2.0dev</rs>) using Eq. ( 1): 16</p>
<p>Correlation of protein elution profiles. Proteins were quantified in each SEC fraction as described above. iBAQ values for each protein were normalized by the maximum of the respective protein over the course of fractionation, leading to normalized abundance values between 1 and 0. For each combination of proteins, elution peaks were detected via the <rs id="a12893538" type="software" subtype="component" corresp="#a12893539">scipy</rs> <rs id="a12893539" type="software" subtype="environment">python</rs> package (<rs id="a12893540" type="version" corresp="#a12893538">1.4.1</rs>). In an elution window of 7 or more fractions, the abundances of the proteins were correlated (Pearson). PPIs with elution profiles with a correlation coefficient &gt;0.5 were counted as having similar elution profiles. <rs id="a12893541" type="software" subtype="implicit" corresp="#a12893542">Code</rs> was written in <rs id="a12893542" type="software" subtype="environment">python</rs> <rs id="a12893543" type="version" corresp="#a12893542">3.7</rs>.</p>
<p>Plotting protein elution profiles. For the creation of protein elution profiles, fraction-wise iBAQ intensities for each protein from the <rs id="a12893544" type="software">MaxQuant</rs> search were used (see above), hereinafter referred to as abundance. Individual proteins are represented by their gene names while protein complexes, when shown in the figures, are labelled with their respective complex name. Abundance values for protein complexes were averaged for all components as listed in EcoCyc 39 , (retrieved from https://ecocyc.org/ on 9/25/18). Plots were created in <rs id="a12893545" type="software" subtype="environment">python</rs> <rs id="a12893546" type="version" corresp="#a12893545">3.
7</rs> with <rs id="a12893547" type="software" subtype="component" corresp="#a12893545">pandas</rs> <rs id="a12893548" type="version" corresp="#a12893547">0.24.2</rs> using the <rs id="a12893549" type="software" subtype="component" corresp="#a12893547">seaborn</rs> <rs id="a12893550" type="version" corresp="#a12893549">0.
9.0</rs> package.</p>
<p>Protein structural models. Models of protein complexes with mapped residue pairs (Supplementary Data 8) were prepared with <rs id="a12893551" type="software">xiVIEW</rs> <rs id="a12893552" type="bibr">40</rs> , <rs id="a12893553" type="software" subtype="environment">python</rs> <rs id="a12893554" type="version" corresp="#a12893553">3.7</rs> with <rs id="a12893555" type="software" subtype="component" corresp="#a12893553">pandas</rs> <rs id="a12893556" type="version" corresp="#a12893555">0.24.2</rs> and <rs id="a12893557" type="software" subtype="component" corresp="#a12893553">ChimeraX</rs> <rs id="a12893558" type="version" corresp="#a12893557">0.92</rs> <rs id="a12893559" type="bibr">41</rs> .</p>
<p>All structural PPI models were downloaded from the protein data bank (https:// www.rcsb.org/): PDB 5t4O [https://doi.org/10.2210/pdb5T4O/pdb] (ATP synthase 42 ), PDB 6RKW [https://doi.org/10.2210/pdb6RKW/pdb] (DNA gyrase 43 Database search and FDR calculation for sulfo-SDA crosslinked pulldowns. A recalibration of the precursor m/z was conducted based on high-confidence linear peptide identifications 37 . The re-calibrated peak lists were searched against the sequences of proteins identified in a given pull-down and with an iBAQ ≥ 5e6 along with their reversed sequences (as decoys) using <rs id="a12893560" type="software">xiSEARCH</rs> (v. <rs id="a12893561" type="version" corresp="#a12893560">1.7.6.2</rs>) for identification. MS-cleavability of the sulfo-SDA crosslinker was considered 50 . Final crosslink lists were compiled using the identified candidates filtered to 2% FDR on residue pair-level and 5% on PPI level with <rs id="a12893562" type="software">xiFDR</rs> v.<rs id="a12893563" type="version" corresp="#a12893562">2.1.5</rs> <rs id="a12893564" type="bibr">17</rs> .</p>
<p>RNA polymerase binding site of YacL. An <rs id="a12893565" type="software">I-TASSER</rs>41 (v. <rs id="a12893566" type="version" corresp="#a12893565">5.1</rs>) <rs id="a12893567" type="bibr">51</rs> model for YacL was generated with default settings based on the Uniprot sequence (see above). <rs id="a12893568" type="software">DisVis</rs> (v. <rs id="a12893569" type="version" corresp="#a12893568">2.0</rs>) <rs id="a12893570" type="bibr">52</rs> ran under default settings, with YacL as scanning model and fixed model PDB 6C6U [https://doi.org/10.2210/pdb6c6u/pdb] 53 with residue 118-127 of NusG modelled using the <rs id="a12893571" type="software" subtype="component" corresp="#a12893573">Modeller</rs> <rs id="a12893572" type="bibr">54</rs> plug-in in <rs id="a12893573" type="software" subtype="environment">Chimera</rs> <rs id="a12893574" type="bibr">55</rs> . Residue pairs of YacL to RNAP and NusG were used as restraints with a minimal distance of 2 Å, and a maximal distance of 30 or 20 Å for DSSO/BS3 and sulfo-SDA, respectively. The density displayed in Fig. 4d corresponds to the accessible interaction space with 14 satisfied restraints. The <rs id="a12893575" type="software">I-TASSER</rs> model was placed for visualisation purposes only.</p>
<p>Raw data and <rs id="a12893576" type="software">MaxQuant</rs> outputs from quantitative proteomics SEC-MS experiments were deposited with the ProteomeXchange Consortium partner repository jPOSTrepo under the accession codes JPST000843 56 and PXD019004. Raw data and <rs id="a12893577" type="software">MaxQuant</rs> outputs from quantitative proteomics AP-MS experiments were deposited with the ProteomeXchange Consortium partner repository jPOSTrepo under the accession codes JPST001090 56 and PXD024146. All raw data, peak lists and search result files from BS3/ DSSO crosslinking experiments in the SEC fractions and after multidimensional fractionation were deposited with the ProteomeXchange Consortium partner repository jPOSTrepo under the accession codes JPST000845 56 and PXD019120. All raw data, peak lists and search result files from affinity-enrichment and crosslinking experiments were deposited with the ProteomeXchange Consortium partner repository jPOSTrepo under the accession JPST001091 56 and PXD024148. We accessed the STRING database (v10.5) via https://string-db.org/. The new link for this version is https://version-10-5.string-db. org/. The used resource can be downloaded using the following link: https://version-10-5. string-db.org/download/protein.links.detailed.v10.5/511145.protein.links.detailed.v10.5. txt.gz. Models from the protein data bank (PDB) can be found under the following links: PDB 5t4O [https://doi.org/10.2210/pdb5T4O/pdb] (ATP synthase 42 ), PDB 6RKW [https://doi.org/10.2210/pdb6RKW/pdb] (DNA gyrase 43 ), PDB 4PKO [https://doi.org/ 10.2210/pdb4PKO/pdb] (GroEL 44 ), PDB 4S20 [https://doi.org/10.2210/pdb4S20/pdb] (RapA 45 ), PDB 6RIN [https://doi.org/10.2210/pdb6RIN/pdb] (GreB 46 ), PDB 5MS0 [https://doi.org/10.2210/pdb5MS0/pdb] (NusG 47 ), PDB 6FLQ [https://doi.org/10.2210/ pdb6FLQ/pdb] (NusA 48 ), PDB 4ZH3 [https://doi.org/10.2210/pdb4ZH3/pdb] (RpoD 49 ), PDB 6C6U [https://doi.org/10.2210/pdb6c6u/pdb] (RNAP-NusG 53 ). Source data are provided with this paper.</p>
<p>The <rs id="a12893578" type="software">xiFDR</rs> version <rs id="a12893579" type="bibr">57</rs> used in this manuscript (v<rs id="a12893580" type="version" corresp="#a12893578">2.0.dev</rs>) is available via Zenodo at <rs id="a12893582" type="url" corresp="#a12893578">https:// doi.org/10.5281/zenodo.4682917</rs>. More recent <rs id="a12893583" type="software">xiFDR</rs> versions can be downloaded from <rs id="a12893584" type="url" corresp="#a12893583">https://github.com/Rappsilber-Laboratory/xiFDR</rs> or <rs id="a12893585" type="url" corresp="#a12893583">https://www.rappsilberlab.org/ software/xifdr/</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f561917842"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:50+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The key advantage of this general approach is simplicity as it builds on relatively wellunderstood mechanisms that have been extensively tested experimentally. Aggregate models have been used, among other things, to explain patterns of language coordination in the <rs id="a12951717" type="software">Maze Task</rs> (see discussion below and also, for example, Garrod &amp; Doherty, 1994;Pickering &amp; Garrod, 2004).</p>
<p>We explore these claims through experiments on referring expressions in the <rs id="a12951718" type="software">Maze Task</rs>. These experiments provide a direct test of the causal effects of feedback on coordination by selectively interfering with these mechanisms during live text-chat conversations. They also provide a direct contrast of the contribution of positive evidence of understanding such as "yeah" and "aha" with negative evidence of misunderstanding such as "wot?" and "sorry?" to language coordination in conversation.</p>
<p>The Running Repairs Hypothesis proposes that the bilateral processes of signaling and resolving misunderstandings are key to people's ability to build new conventions or "sublanguages" on-the-fly in task-oriented dialogues (Healey, 1997(Healey, , 2008;;Healey, Swoboda, Umata, &amp; King, 2007;Mills, 2013). Negative evidence provides people with specific new information about differences in language use which provide a more fine-grained, crossspeaker basis for revising their referring expressions than positive evidence can. We apply this argument in the context of coordination of language use in the <rs id="a12951719" type="software">Maze task</rs>.</p>
<p>The <rs id="a12951720" type="software">Maze Task</rs> was devised by <rs id="a12951721" type="publisher" subtype="person" corresp="#a12951720">Garrod and Anderson</rs> <rs id="a12951752" type="bibr">(Anderson &amp; Garrod, 1987;Garrod &amp; Anderson, 1987)</rs> as an interactive game in which pairs of people collaborate to navigate through a maze to reach a goal point (Fig. 1). They cannot see each other's positions in the maze and in order to reach their respective goals they must describe to each other where switch points are that open and close gates in each other's mazes. As a result, they repeatedly exchange location descriptions. Importantly the configuration of the Maze changes over trials and this pushes participants toward developing systematic approaches to describing sequences of different locations in sequences of different mazes.</p>
<p>The standard pattern of coordination observed in the <rs id="a12951723" type="software">Maze Task</rs> is that people migrate from using Figural and Path descriptions in early trials toward more abstract Line and Matrix descriptions in later trials (Garrod &amp; Anderson, 1987). This pattern is illustrated by the sequence of descriptions in Table 1.</p>
<p>In lieu of explicit negotiation, explanations of coordination in the <rs id="a12951724" type="software">Maze Task</rs>, such as Input-Output Coordination (Garrod &amp; Anderson, 1987) and Interactive Alignment (Pickering &amp; Garrod, 2004&amp; Garrod, , 2007)), have proposed Aggregate mechanisms of coordination. At any particular point in the conversation, the activation of representations involved in comprehending a description of a given type makes the production of a new description of the same type much more likely. So if one person produces, say, a Path description of a location, this will also activate the lexical, syntactic and semantic representations associated with those descriptions for the addressee who hears it. When they subsequently produce a description of another location, these representations are already active and this facilitates the production of a new Path style description. In this way, language processing becomes entrained across participants.</p>
<p>A problem for this kind of explanation is that it doesn't predict the pattern of migration captured in Table 1. People initially have most success in this task with the Figural and Path descriptions and statistically these will be most strongly activated; nevertheless, as noted, people consistently migrate away from these toward Line and Matrix description types (Garrod &amp; Anderson, 1987;Garrod &amp; Doherty, 1994;Healey, 1997). This trend is counter to what is predicted by priming, precedent, or recency effects alone. A second difficulty is that this picture of coordination is too passive. At the start of the <rs id="a12951725" type="software">Maze Task</rs> people often differ on the meaning of even relatively simple words like "box" or "row" in the context of the maze. Healey (1997) found that around 65% of people's initial referring expressions in the <rs id="a12951726" type="software">Maze Task</rs> are subject to some form of explicit clarification request. These overt, active clarification and repair processes seem, by definition, to require an Interactional explanation. A third issue is that Aggregate accounts presuppose that the lexical, syntactic, and semantic resources needed to produce or comprehend each scheme are in some sense already available to participants. The coordination problem is modeled primarily as a matter of ensuring the same scheme is activated. However, there is evidence that the solutions to the coordination problems which people develop in this task are, to a significant degree, novel conventions that emerge from the interaction. For example, there is evidence that novel and semantically distinct task sub-languages emerge by default in different groups who perform the task separately (Healey, 1997(Healey, , 2008)).</p>
<p>To summarize, the Running Repairs Hypothesis combines the claim that interactive feedback is central to the coordination of language use in conversation with the claim that negative feedback plays an especially critical role in coordinated language use by underpinning coordinated semantic change. This leads to two basic experimental hypotheses about the patterns of language use and language change observed in the <rs id="a12951727" type="software">Maze Task</rs>: H1: Positive evidence of understanding promotes re-use of existing maze referring schemes.</p>
<p>The manipulations of live <rs id="a12951728" type="software">Maze Task</rs> dialogues were carried out using a customized text chat tool (see <rs id="a12951730" type="bibr">Eshghi &amp; Healey, 2016;Healey &amp; Mills, 2006;Healey, Purver, King, Ginzburg, &amp; Mills, 2003;Howes, Purver, Healey, Mills, &amp; Gregoromichelaki, 2011</rs>) which enables substitutions of words in real-time interactions without participant's knowledge. The two experimental manipulations are run between-subjects with a third control condition with no manipulations for comparison.</p>
<p>The experiments were conducted using a version of the <rs id="a12951731" type="software" subtype="component" corresp="#a12951732">Maze Task</rs> developed for the <rs id="a12951732" type="software" subtype="environment">DiET</rs> chat tool experimental platform. As each turn is typed, the <rs id="a12951733" type="software">DiET</rs> server monitors it for target expressions, substitutes them with experimental probe expressions, and then sends the modified turn to the other participant. Only the recipient of the turn sees the substitution; the person who originally typed it does not. The list of target expressions and substitutions is generated from previous text-chat experiments to ensure natural text-chat usage is accommodated. This enables two experimental manipulations:</p>
<p>The <rs id="a12951734" type="software">DiET</rs> chat tool makes a variety of potential dependent variables available for analysis. We use three simple measures of task performance (c.f. Healey, 2008;Howes et al., 2011). Number of Mazes completed within a fixed time (in this case 50 min) gives a All the way to the right i mean basic measure of global task performance. This is combined with two measures of task process. First, total number of turns used per maze completed, with turn completion operationalized as the text sent on pressing the enter key. This gives a simple measure of how much interaction is required to complete each maze. Second, typing speed in characters (including spaces) per second between onset of the first character and pressing enter. This provides a measure of how easy participants find it to formulate their turns, the assumption being that people who are confident of how to describe their position or what to do next will, all things being equal, type faster than those who are not (this approach is also used in Howes et al., 2011).</p>
<p>For measures of semantic coordination we use three dependent variables to assess the forms of referring expression used in turns that contain a location description. Following the previous <rs id="a12951735" type="software">Maze Task</rs> literature, we use the content based hand coding of description types: Figural, Path, Line, Matrix, following the classification described by Garrod and Anderson (1987) (see description above). In addition to this we provide two automatically coded measures. First, number of digits used in each location description that counts all instances of &lt; 1st, 2nd, 3rd, 4th,. . . &gt; and &lt; 1, 2, 3, 4,. . ., &gt; in each turn (defined as above) but ignores number words such as "one," "two," "three," "four," and ordinals such as "first" and "last." The rationale is that this provides a simple but fine-grained index of participants' assumptions about how well coordinated their ontology of locations is. Roughly, if participants know what things to count and are doing it frequently enough to use digits, this suggests a relatively stable concept of what a "possible location" is in the maze and a relatively well-organized scheme (like the Line and Matrix schemes) for individuating them. The last measure is description length measured as the number of characters (including spaces) in a turn containing a location description. For example, "in the penultimate square on the left wing" is classified as a Figural description that uses 0 digits and 42 characters; "1st col 2nd row" is classified as a Line description containing 2 digits and 15 characters and "5,5" is a Matrix description using 2 digits and 3 characters.</p>
<p>The number of turns or mazes completed is not independent for the members of a pair. To allow for this the relevant data is analysed as scores per pair. For the rest of the analyses pair is included as a random factor in order to allow for possible statistical dependencies between responses by the different members of a pair. Following Healey and Mills (2006); Eshghi and Healey (2016) we pool the scores for the first six Early and last six Late mazes to provide an index of task experience. This simplifies the analysis and also reduces the variability contributed by individual mazes. To accommodate the mix of repeated and between subjects measures and the different distributions of the dependent variables, Generalized Linear Mixed Models procedure in <rs id="a12951736" type="software">SPSS</rs> (v. <rs id="a12951737" type="version" corresp="#a12951736">21</rs>) was used for all statistical analyses other than simple non-parametric comparisons.</p>
<p>All spoof turns produced by the <rs id="a12951738" type="software">DiET</rs> server are excluded from all analyses to avoid biasing any of the typing measures; however, the original, unmodified turns are included in turn counts. One pair from the amplification condition is also excluded as the transcript indicated they had failed to understand the task. After these exclusions the three conditions together yielded a total corpus of 13,744 turns. Using the criteria described above, all turns were coded by the authors for whether they contained a location description (Y/N) and the type of location description (Figural, Path, Line or Matrix). This coding yielded a total of 2,844 location descriptions.</p>
<p>A second possible explanation for the conflict with previous findings is the difference between the <rs id="a12951739" type="software">Maze Task</rs> and the typical definite reference tasks. In tasks such as the Tangram Task and its variants, people are required to coordinate repeated references to the same specific items. In contrast to this, the <rs id="a12951740" type="software">Maze Task</rs> requires people to coordinate references to a succession of different specific instances of items of a similar type, sequences of locations. It is possible that positive evidence of understanding may be more important to the collaborative processes of establishing a name for a recurring item, that is, for the processes of contraction or abbreviation typically seen in the Tangram Task (Clark &amp; Schaefer, 1989;Clark &amp; Wilkes-Gibbs, 1986), than it is for developing general referring schemes that work across non-repeating instances of a class of things that is, the process of abstraction seen in the <rs id="a12951741" type="software">Maze Task</rs> (Garrod &amp; Anderson, 1987;Healey, 2008).</p>
<p>The results also suggest the potential for a productive new interface between Conversation Analysis, Formal Semantics, and Psycholinguistics. The detailed structure of repair sequences is well understood (Schegloff, 1987(Schegloff, , 1992;;Schegloff, Jefferson, &amp; Sacks, 1977). However, their effects on semantic change have not previously been explored experimentally. The fine-grained, real-time interventions made possible by the <rs id="a12951742" type="software">DiET</rs> chat tool enable selective interference with different repair processes. This enables a form of experimental conversation analysis that, we hope, can complement existing work by causally testing the effects of a variety of different interactional mechanisms on the subsequent trajectory of an interaction. Some caveats are needed. One methodological issue with this approach is the use of text-based interaction. Text chat in its various forms is now a common form of communication, but we know relatively little about how processes such as turn-taking and repair in text-chat formats differ from spoken conversation (Scho€ onfeldt &amp; Golato, 2003). 2 The plausibility of the generalization made here is supported by the conversational style of the <rs id="a12951743" type="software">Maze Task</rs> chat exchanges, including the natural production of simple grounding signals and repair phenomena on which the experimental manipulations depend. It is also supported by the similarity in patterns of maze description type produced in text-chat and spoken versions of the task (see, e.g., Garrod &amp; Anderson, 1987;Garrod &amp; Doherty, 1994;Healey, 1997). Nonetheless, only further work can determine whether this is a serious problem.</p>
<p>The ideal response would be to extend the techniques used by <rs id="a12951744" type="software">DiET</rs> to spoken interaction to test whether the same results are obtained. However, there is a practical difficulty. The experiments require that what has been said is recognized before an intervention is triggered. For example, the spoken target "Okay" is recognized before substituting it for a spoof "Ummm." This introduces a delay that significantly disrupts the dynamics of live spoken or non-verbal interaction but is compatible with the feedback cycles typical of text-based interaction.</p>
<p>Another general methodological concern is the use of task-oriented dialogues instead of the naturally occurring conversation data favored by conversation analysis. This is not an intrinsic limitation of the <rs id="a12951745" type="software">DiET</rs> method which can equally well be used in unstructured text interactions. The particular value of using task-oriented dialogues here is that it provides a relatively well-understood, independent measure of semantic coordination. Comparable measures are not currently available for natural dialogue.</p>
<p>This leads to the critical question of whether the present findings generalize to other tasks and situations. The Running Repairs Hypothesis makes a general claim about the primary mechanisms that coordinate language processing in dialogue and it is a large step to generalize from the present experimental results to processes of language coordination in general. Some convergent support comes from prior work that shows that patterns of (spoken) repair are closely correlated with semantic coordination in the <rs id="a12951746" type="software">Maze Task</rs> <rs id="a12951753" type="bibr">(Healey, 1997)</rs> and that interfering with the resources available for identifying and resolving communication problems systematically alters semantic coordination in both a spoken version of the <rs id="a12951748" type="software">Maze task</rs> and in graphically mediated interactions <rs id="a12951754" type="bibr">(Healey, 2008;Healey, Swoboda, Umata, &amp; King (2007)</rs>.</p>
<p>Positive and negative evidence of understanding appear to play systematically different roles in people's ability to converge on particular description schemes in the <rs id="a12951750" type="software">Maze Task</rs>. In particular, while Attenuation of positive evidence has no significant effect, Amplification of negative evidence promotes convergence on systematic, abstract ways of describing maze locations. The processes involved in detecting and repairing misunderstandings do more than keep the conversation afloat (Schegloff, 1992), they have systematic effects on subsequent language use by driving changes in the speed and form of semantic coordination that emerges. This provides support for the Running Repairs Hypothesis: Coordination of language use depends first and foremost on processes used to deal with misunderstanding on the fly. The implication is that language processing in dialogue involves mechanisms that are qualitatively different from those needed for individual language processing. This also suggests an opportunity for developing new conceptual and empirical connections between Conversation Analysis, Psycholinguistics, and Formal Semantics.</p>
<p>This research was partly supported by ESRC through the Dynamics of Conversational Dialogue project (RES-062-23-0962) and EPSRC through the <rs id="a12951751" type="software">Dialogue Experimentation Toolkit (DiET)</rs> project (EP/D057426/1). Eshghi has also been gratefully supported by the EPSRC, under grant number EP/M01553X/1 (BABBLE project).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f201780344"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T05:56+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The readability statistics were computed with the <rs id="a12900026" type="software">Readability Calculator</rs><rs id="a12900027" type="bibr">1</rs> , a free online program used by previous studies to assess the readability of Web materials (Mcinnes and Haglund, 2011;Jayaratne et al., 2014). The edited text for each article was processed by this website to count the number of characters, words and sentences. The website also computed five readability indices that indicate the school grade appropriate for that reading difficulty. The readability grade level indices were the Automated Readability Index, Coleman-Liau Index, Flesch-Kincaid Grade Level, Gunning Fog index and Simple Measure of Gobbledygook index. All these indices produce an output that approximates the grade level (in United States) estimated necessary to understand the text. Each of them use a formula that differs from each other slightly, particularly the Automated Readability Index and Coleman-Liau Index rely on character counts whereas the others rely on syllable counts. The Gunning Fog index was the most frequently used index to evaluate readability of journal articles (Roberts et al., 1994;Rochon et al., 2002;Weeks and Wallace, 2002), whereas the others were also often used to check readability of journal articles as well as materials on websites targeting patients (Sawyer et al., 2008;Jayaratne et al., 2014). An average reading grade level (AGL) was calculated by taking the mean of these five indices (Sawyer et al., 2008;Jayaratne et al., 2014). Besides the school grade indices, a Flesch Reading Ease (FRE) score was also computed for each article. This score was also frequently used to evaluate readability of journal articles (Roberts et al., 1994;Rochon et al., 2002;Weeks and Wallace, 2002;Plavén-Sigray et al., 2017). In contrast to the school grade indices, a higher FRE score means easier to read. The formulas of the abovementioned indices are listed in Table 1. It should be noted that these formulae assume the readability is related to the number of long words or sentences without considering the contextual difficulty of the words, such as the use of jargon. However, it is difficult to define what is jargon. There is a formula called New Dale-Chall readability formula that outputs a numerical value representing the comprehension difficulty of the surveyed text (Chall and Dale, 1995): Raw Score = 0.1579 * (% of difficult words) + 0.0496 * (words/sentences). It uses a list of 3,000 common words, and words outside this list are considered as difficult words. However, no published report has defined the common word list within the context of scientific journal articles, so it would be difficult to consider the effect of jargon and hence that issue was not followed in the 100 articles investigated.</p>
<p>Finally, if the above univariate tests revealed significant associations, multi-way ANCOVAs were performed to evaluate if any of these factors, when considered together, would still be associated with the readability scores. All statistical analyses were performed in <rs id="a12900028" type="software">SPSS</rs> <rs id="a12900029" type="version" corresp="#a12900028">24.0</rs> ( <rs id="a12900030" type="publisher" corresp="#a12900028">IBM</rs>, Armonk, New York, United States). Test results were significant if p &lt; 0.05.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f427528916"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T09:36+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>These prioritized genes include seven ion-channel genes (SCN1A, SCN2A, SCN3A, GABRA2, KCNN2, KCNAB1, and GRIK1), three transcription factors (ZEB2, STAT4 and BCL11A), the histone modification gene BRD7, the synaptic transmission gene STX1B and the pyridoxine metabolism gene PNPO. Notably, a conditional transcriptome-wide association study (TWAS) analysis suggests that the signal for genetic generalized epilepsy at 17q21.32, which was also observed in an earlier study 12 , seems driven by regulation of expression of PNPO (Supplementary Fig. 8). This suggests that the biology behind pyridoxine (vitamin-B6)-responsive epilepsy 19,20 could be relevant to common genetic generalized epilepsies. Biological prioritization implicates SCN1A, SCN2A, SCN3A, and TTC21B as the most likely genes underlying the signal at 2q24.3 for ). Previously known loci are indicated in black; novel loci in red. The names above each locus represent the prioritized gene in the locus (see Fig. 2) or the name of the locus itself in case of multiple prioritized genes in the locus all epilepsy, focal epilepsy and genetic generalized epilepsy. Pathogenic variants in the sodium channels SCN1A, SCN2A and SCN3A are associated with various epilepsy syndromes 16 and mutations in TTC21B impair forebrain development 21,22 . Our analyses implicate STX1B as a potential gene underlying the association of JME at the 16p11.2 locus and the top variant in the locus is an eQTL that strongly correlates with expression of STX1B in the dorsolateral prefrontal cortex (Spearman's correlation: Rho = 0.33, p = 3 × 10 -14 ) 23 . Interestingly, for one of the prioritized genes in genetic generalized epilepsy, PCDH7, an eQTL was recently detected in epileptic hippocampal tissue 24 . Prioritized genes associated with focal epilepsy with hippocampal sclerosis include the gap-junction gene GJA1. In addition we identified eight genes from Fig. 2 (BCL11A, GJA1, ATXN1, GABRA2, KCNAB1, SCN3A, PCDH7, STAT4) with evidence of co-expression in at least two independent brain expression resources, using a brain gene co-expression analysis with <rs id="a12871232" type="software">brain-coX</rs> <rs id="a12871233" type="bibr">25</rs> . These eight candidates are embedded in several established epilepsy gene co-expression modules (Supplementary Fig. 9; Supplementary Table 9).</p>
<p>To gain further biological insight into our results, we next used a partitioned heritability method 29 to assess whether our genomewide significant signals contained SNPs associated with enhanced transcription in any of 88 tissues. We found significant enrichment of H3K4me1 markers in all epilepsy (stratified LDscore regression; p = 4 × 10 -5 ) and H3K27ac markers in genetic generalized epilepsy (stratified LD-score regression; p = 1.3 × 10 -6 ), specifically in the dorsolateral prefrontal cortex. Moreover, the distribution of heritability enrichment P-values was strongly skewed towards brain tissues for all epilepsy phenotypes (Fig. 3d, Supplementary Figs. 101112). Biological gene criteria Fig. 2 Genome-wide significant loci of all analyses and prioritized biological epilepsy genes. Genes were prioritized based on 6 criteria and scored based on the number of criteria met per gene (filled red boxes). The highest scoring gene, or multiple if they have the same score, in each locus is reported as 'prioritized biological epilepsy gene(s)'. Similar to previous studies 17,18 , we used a minimum score of 2 to define these genes and we noted 'none' if no gene in the locus reached this score. Filled blue boxes indicate overlap with known targets of anti-epileptic drugs and established monogenic epilepsy genes. The lead SNP is defined as the SNP with the lowest P-value in the locus and the minor allele is displayed in brackets. P-values and Z-scores for All epilepsy, Focal epilepsy and Generalized epilepsy were calculated with fixed-effects trans-ethnic meta-analyses. P-values and Z-scores for JME, CAE, and Focal HS were calculated with <rs id="a12871235" type="software">BOLT-LMM</rs>. MAF minor allele frequency in the Human Reference Consortium reference panel. The direction of the Z-score is signed with respect to the minor allele. TWAS: significant TWAS association (based on data from the CommonMind Consortium), eQTL: significant eQTL within locus (based on data from the ROS/MAP projects), Brain exp: the gene is preferentially expressed in the brain, Missense: epilepsy GWAS missense variant in locus, PPI: gene prioritized by protein-protein interaction, KO mouse: relevant knockout mouse phenotype H3K27ac and H3K4me1 are epigenetic markers associated with regulating gene transcription, suggesting that altered transcription in the dorsolateral prefrontal cortex could be one of the underlying mechanisms of epilepsy. This is further supported by a tissue-specific heritability enrichment analysis (using data from the GTEx Consortium), showing strongest enrichment for genetic generalized epilepsy with genes expressed in Brodmann Area 9 (stratified LD-score regression; p = 1.56 × 10 -6 ), which encompasses the dorsolateral prefrontal cortex (Fig. 3e). These findings further corroborate our TWAS results (using data from the unrelated CommonMind Consortium database), which shows significant associations of epilepsy with gene expression of several b Minimum (most active) chromatin state across 127 tissues for all genome-wide significant SNP in all phenotypes; TSS -transcription start site. c The RegulomeDB score for all genome-wide significant SNPs in all phenotypes, where 7 represents no evidence for affecting regulation and lower scores represent increasing evidence; NA -the variant does not exist in RegulomeDB. d Heritability enrichment for genetic generalized epilepsy with 6 different chromatin markers in 88 tissues, calculated with stratified LD-score regression using data from the Roadmap Epigenomics Project. The main bar chart represent the 10 tissues with the strongest heritability enrichment and the inset shows the full distribution of all chromatin markers in all tissues. e Heritability enrichment of genes expressed in 53 tissues, calculated with stratified LD-score regression using data from the gene-tissue expression (GTEx) Consortium genes in the dorsolateral prefrontal cortex (Fig. 2; Supplementary Table 3). Although genetic generalized epilepsy has been regarded as a generalized process, anatomical, electrophysiological, cognitive, and functional imaging studies implicate dysfunction in the frontal lobes [30][31][32][33][34] . Altogether, we have converging evidence from several unrelated methods and databases suggesting epigenetic regulation of gene expression in the dorsolateral prefrontal cortex as a potential pathophysiological mechanism underlying our epilepsy GWAS findings.</p>
<p>Finally, we leveraged the Brainspan database, as implemented in <rs id="a12871239" type="software">FUMA</rs> <rs id="a12871240" type="bibr">35</rs> , to assess whether the genes implicated by our GWAS are differentially expressed in the brain at various prenatal and post-natal ages. These analyses were performed for the genes prioritized in any epilepsy phenotype (21 genes), any focal epilepsy subtype (8 genes) or any genetic generalized epilepsy syndrome (15 genes). The results suggest that the expression of genes associated with focal epilepsy is up-regulated in late-infancy and young adulthood, whereas expression of those genes associated with genetic generalized epilepsy is down-regulated in early childhood and differentially expressed prenatally and at adolescence (Supplementary Fig. 13).</p>
<p>Heritability analyses. Twin-based and genetic heritability studies have suggested that while epilepsy is strongly heritable 8,39 , there is a substantial missing heritability component 40,41 . We used <rs id="a12871241" type="software">LDAK</rs> to estimate h 2 SNP : the proportion of heritability that can be attributed to SNPs [42][43][44] . We estimate h 2 SNP = 32.1% (95%CI: 29.6-34.5%) for genetic generalized epilepsy and h 2 SNP = 9.2% (8.4-10.1%) for focal epilepsy (estimates are on the liability scale, assuming a prevalence of 0.002 and 0.003, respectively) which are consistent with previous estimates 8 . These results indicate that SNPs explain a sizeable proportion of the liability of genetic generalized epilepsy syndromes, but less so for focal epilepsy phenotypes (Fig. 4). To delineate the heritability of the different epilepsy phenotypes, we used <rs id="a12871242" type="software">LDAK</rs> to perform genetic correlation analyses between the different forms. We found evidence for strong genetic correlations between the genetic generalized epilepsies, whereas we found no significant correlations between the focal epilepsies (Fig. 4). Interestingly, we found a significant genetic correlation between JME and lesion-negative focal epilepsy (<rs id="a12871243" type="software">LDAK</rs> genetic correlation: R 2 =0.46, p=8.77 × 10 -4 ), suggesting either pleiotropy and/or misclassification. It is known that focal EEG features can be seen in JME 45 .</p>
<p>Study design. We conducted a case-control study in subjects of Caucasian, Asian (Han Chinese) and African-American ethnicities. Our primary analyses were structured to test common genetic variants for association with epilepsy according to broad epilepsy phenotypes. We pooled cases from cohorts of the same ethnic group to perform linear mixed model analysis, followed by subsequent metaanalyses of regression coefficients across the three ethnic groups. Our secondary analyses tested for associations with specific syndromes of genetic generalized epilepsy (childhood absence epilepsy, juvenile absence epilepsy, juvenile myoclonic epilepsy, and generalized tonic-clonic seizures alone) and phenotypes of focal epilepsy (lesion negative, focal epilepsy with hippocampal sclerosis, and focal epilepsy with other lesions). The secondary analyses were limited to Caucasian subjects due to sample size. We prioritized the results of the GWAS by incorporating eQTL information, transcriptome-wide analysis, and biological annotation. Finally, we estimated the genetic correlation of epilepsy phenotypes using <rs id="a12871244" type="software">Linkage-Disequilibrium Adjusted Kinships (LDAK)</rs>.</p>
<p>Genotyping quality control and imputation. Quality control of genotyping was performed separately for each cohort using <rs id="a12871245" type="software">PLINK</rs> <rs id="a12871246" type="version" corresp="#a12871245">1.9 55</rs> . Each genotype cohort was temporarily merged with a genetically similar reference population from the 1000 Genomes Project (CEU, CHB, or YRI). A test for Hardy-Weinberg equilibrium (HWE) was performed and SNPs significant at p &lt; 1 × 10 -10 were removed. All samples and all SNPs with missing genotype rate &gt;0.05 and all SNPs with minor allele frequency (MAF) &lt;0.01 were removed. Next, we pruned SNPs using the <rs id="a12871247" type="software" subtype="environment">PLINK</rs> <rs id="a12875528" type="software" subtype="component" corresp="#a12871247">--indep-pairwise</rs> command (settings: window size 100 kb, step size 25, R 2 &gt; 0.1). Using this subset of SNPs, we removed samples with outlying heterozygosity values (&gt;5 SD from the median of the whole cohort). Identity by descent (IBD) was calculated in <rs id="a12871248" type="software">PLINK</rs> to remove sample duplicates (&gt;0.9 IBD) and to identify cryptic relatedness. We removed one from each sample pair with IBD&gt;0.1875, with the exception of the EPGP familial epilepsy cohort. Subjects were removed if sex determined from X-chromosome genotype did not match reported gender. Arrayspecific maps were used to update all SNPs positions and chromosome numbers to the Genome Reference Consortium Human Build 37 (GRCh37), and remove all A/ T and C/G SNPs to avoid strand issues. We applied pre-imputation checks according to <rs id="a12871249" type="software" subtype="implicit">scripts</rs> available on the website of <rs id="a128712500" type="publisher" subtype="person" corresp="#a12871249">Will Rayner</rs> of the Wellcome Trust Centre for Human Genetics (<rs id="a12871250" type="url" corresp="#a12871249">www.well.ox.ac.uk/~wrayner/tools/</rs>) to remove SNPs with allele frequencies deviating &gt;20% from the frequency in the Haplotype Reference Consortium. Samples were submitted to the <rs id="a12871251" type="software">Sanger Imputation Service</rs> (<rs id="a12871252" type="url" corresp="#a12871251">https://imputation.sanger.ac.uk/</rs>) <rs id="a12871253" type="bibr">56</rs> . We selected the Human Reference Consortium (release 1.1; n = 32470) dataset as reference panel for Caucasian and Asian datasets and the African Genome Resources (n = 4956) for the African-American datasets. Post-imputation quality control filters were applied to remove SNPs within each imputed cohort with an imputation info score &lt;0.9 or HWE p&lt;1e-6. Imputed genotype dosages with a minimum certainty of 0.9 per subject were converted to hard-coded <rs id="a12871254" type="software">PLINK</rs> format after which all samples were pooled into a single cohort. We performed a principal components analysis using <rs id="a12871255" type="software">GCTA</rs>. From the PCA results we stratified our subjects into three broad ethnic groups (Caucasian, Asian and African) while removing extreme outliers. After stratifying by ethnicity, we removed SNPs with HWE p &lt; 1e-6, call rate &lt;0.95 or MAF&lt;0.01. In total 816 subjects out of 45705 subjects were filtered out by quality control procedures, leaving 44889 subjects for analyses.</p>
<p>Study power. We estimated using <rs id="a12871256" type="software">PGA</rs> <rs id="a12871257" type="bibr">57</rs> that the study had 80% power to detect a genetic predictor of relative risk for all epilepsy (approximated to odds ratio) ≥1.45 with MAF = 1% and an alpha level of 5 × 10 -8 . We estimated that our metaanalyses had 80% power to detect genome-wide significant SNPs of MAF = 1% with relative risks ≥1.5 and ≥1.8, for focal and generalized epilepsy respectively (see Supplementary Figure 15). Our analysis of generalized epilepsy sub-phenotypes had 80% power to detect genome-wide significant SNPs of MAF = 1% with relative risks ≥2.6, ≥3.3, and ≥2.4 for CAE, JAE, and JME respectively. Our analysis of focal epilepsy sub-phenotypes had 80% power to detect genome-wide significant SNPs of MAF = 1% with relative risks ≥1.9, ≥2.8, and ≥1.9 for focal epilepsy lesion negative, focal epilepsy with hippocampal sclerosis and focal epilepsy with lesion other than hippocampal sclerosis, respectively.</p>
<p>Statistical analyses. Association analyses were conducted within the three ethnic subgroups using a linear mixed model in <rs id="a12871258" type="software">BOLT-LMM</rs> <rs id="a12871259" type="bibr">58</rs> . A subset of SNPs, used to correct for (cryptic) relatedness and population stratification by <rs id="a12871260" type="software">BOLT-LMM</rs>, were derived by applying SNP imputation info score &gt;0.99, MAF &gt;0.01, call rate &gt;0.99 before pruning the remaining variants using <rs id="a12871261" type="software">LDAK</rs> with a window size of 1 Mb and R 2 &gt; 0.2 43 . All analyses included gender as a covariate and the threshold for statistical significance was set at 5 × 10 -8 . We compared χ 2 values of the <rs id="a12871262" type="software">BOLT-LMM</rs> output between all pairs of SNPs in high LD (R 2 &gt; 0.4) and removed pairs of SNPs with extreme χ 2 differences using a formula that scales exponentially with magnitude of χ 2 and LD:</p>
<p>where SNP1-χ 2 and SNP2-χ 2 are the χ 2 -statistic of the two SNPs in each pair and R 2 is their squared correlation (LD). We tested the homogeneity of all SNPs by splitting the pooled cohort into 13 distinct clusters of ethnically matched cases and controls and removed SNPs exhibiting significant heterogeneity of effect (P het &lt; 1 × 10 -8 ). Fixed effects, trans-ethnic meta-analyses were conducted using the software package <rs id="a12871263" type="software">METAL</rs> <rs id="a12871264" type="bibr">59</rs> . Manhattan plots for all analyses were created using <rs id="a12871265" type="software">qqman</rs>. Considering that our study had unequal case-control ratios, we calculated the effective sample size per ethnicity using the formula recommended by <rs id="a12871266" type="software">METAL</rs>: N eff = 4/(1/N cases + 1/N ctrls ). Since &gt;95% of all cases were Caucasian, we included all SNPs that were present in at least the Caucasian dataset (~5 million).</p>
<p>Conditional association analysis was performed with <rs id="a12871267" type="software">PLINK</rs> on loci containing significant SNPs to establish whether other genetic variants in the region (500 Kb upstream and downstream) were independently associated with the same phenotype. The conditional threshold for significance was set at 2 × 10 -5 , based on approximately 2500 imputed variants per 1MB region.</p>
<p>Gene mapping and biological prioritization. Genome-wide significant loci of all phenotypes were mapped to genes in and around these loci using <rs id="a12871268" type="software">FUMA</rs> <rs id="a12871269" type="bibr">35</rs> . Genome-wide significant loci were defined as the region encompassing all SNPs with P &lt; 10 -4 that were in LD (R 2 &gt; 0.2) with the lead SNP (i.e. the SNP with the lowest P-value in the locus with P &lt; 5 × 10 -8 ). Positional mapping was performed to map genes that were located within 250 kb of these loci. Additionally, we mapped genes that were farther than 250 kb away from the locus using chromatin interaction data to identify genes that show a significant 3D interaction (P FDR &lt; 10 -6 ) between their promoter and the locus, based on Hi-C data from dorsolateral prefrontal cortex, hippocampus, and neural progenitor cells 62 . This resulted in a total of 146 mapped genes across all phenotypes, of which some genes (e.g. SCN1A) were associated with multiple epilepsy phenotypes.</p>
<p>1. A significant correlation between the epilepsy phenotype and expression of the gene, as assessed with a transcriptome-wide association study (TWAS). Default settings of the <rs id="a12871270" type="software">FUSION</rs> software package <rs id="a12871271" type="bibr">64</rs> were used to impute geneexpression based on our GWAS summary statistics and RNA-sequencing data from dorsolateral prefrontal cortex tissue (n = 452, CommonMind Consortium 65 ), after which the association between the epilepsy phenotype with gene-expression was calculated. It was possible to test the TWAS expression association for 53 out of our 146 mapped genes, since only the expression of these 53 genes was significantly heritable (heritability p-value &lt;0.01, as suggested by Gusev et al. 64 ). We set a Bonferroni corrected p-value threshold of 0.05/53 = 0.00094 to define significant TWAS associations. 2. Genes for which a SNP in the genome-wide significant locus (as defined above) is a significant cis-eQTL (Bonferroni corrected P &lt; 8 × 10 -10 ) 23 based on data from the ROS and MAP studies, which includes RNA-sequencing data from 494 dorsolateral prefrontal cortex tissues 23 . 3. The gene is preferentially expressed in the brain. This was assessed by using gene-expression data from all 53 tissues of the Gene-Tissue expression (GTEx) Consortium 66 . Genes were considered to be preferentially expressed in the brain when the average expression in all brain tissues was higher than the average expression in non-brain tissues. 4. Genes for which a SNP in the genome-wide significant locus (as defined above) is a missense variant, as annotated by <rs id="a12871272" type="software">ENSEMBL</rs> <rs id="a12871273" type="bibr">67</rs>
. 5. Genes prioritized by protein-protein interaction network, as calculated using the default settings of <rs id="a12871274" type="software">DAPPLE</rs> <rs id="a12871275" type="bibr">68</rs> , which utilizes protein-protein interaction data from the InWeb database 69 . The 146 genes implicated by our GWAS were input after which <rs id="a12871278" type="software">DAPPLE</rs> assessed direct and indirect physical interactions to create a protein-interaction network. Next, <rs id="a12871279" type="software">DAPPLE</rs> assigned a significance score to each gene according to several connectivity parameters; genes with a corrected P &lt; 0.05 were considered to be prioritized by <rs id="a12871280" type="software">DAPPLE</rs>. 6. Genes for which a nervous system or behavior/neurological phenotype was observed in knockout mice. Phenotype data of knockout mice was downloaded from the Mouse Genome Informatics database (http://www. informatics.jax.org/) on 17 January 2018 and nervous system (phenotype ID: MP:0003631) and behavior/neurological phenotype (MP:0005386) data were extracted. All 146 genes were scored based on the number of criteria met (range 0-6 with an equal weight of 1 per criterion), see Supplementary Data 1 for a full list. We considered the gene(s) with the highest score in each locus as the most likely biological epilepsy candidate gene. Multiple genes in a locus were selected if they had an equally high score whilst no genes were selected in a locus if all genes within it had a score &lt;2, similar to previous studies 17,18 .</p>
<p>Gene co-expression analysis for epilepsy with <rs id="a12871281" type="software">brain-coX</rs>. In silico gene prioritization was performed using <rs id="a12871282" type="software">brain-coX</rs> <rs id="a12871283" type="bibr">25</rs> . <rs id="a12871284" type="software">brain-coX</rs> uses a compendium of seven large-scale normal brain gene expression data resources to identify coexpressed genes with a set of given genes (known, or putative, disease causing genes) likely to encapsulate gene expression networks involved in disease. This approach can identify, and thus leverage networks that are not currently known and not present in available resources such as PPI networks and is a complementary approach to these. We used a set 102 monogenic epilepsy genes (Supplementary Table 8) as the set of known disease genes. An FDR of 0.2 was used to identify genes that significantly co-express with the known set of genes. Prioritization in at least three datasets at an FDR of 0.2 led to a specificity of 0.9 25 .</p>
<p>Functional annotations. We annotated all genome-wide significant SNPs (p &lt; 5 × 10 -8 ) from all phenotypes using the <rs id="a12871285" type="software" subtype="component" corresp="#a12871286">Variant Effect Predictor</rs> of <rs id="a12871286" type="software" subtype="environment">ENSEMBL</rs> <rs id="a12871287" type="bibr">67</rs> and the RegulomeDB database 28 . We annotated chromatin states using epigenetic data from the <rs id="a12875535" type="publisher" corresp="#a12875533">NIH</rs> <rs id="a12875533" type="software">Roadmap Epigenomics Mapping Consortium</rs> <rs id="a12875534" type="bibr">70</rs> and <rs id="a12875531" type="software">ENCODE</rs> <rs id="a12875532" type="bibr">71</rs> . We used <rs id="a12871290" type="software">FUMA</rs> <rs id="a12871291" type="bibr">35</rs> to annotate the minimum chromatin state (i.e. the most active state) across 127 tissues and cell types for each SNP, similar to a previous study 27 .</p>
<p>Heritability enrichment of epigenetic markers and gene-expression. We used stratified LD-score regression 72 to assess tissue-specific heritability enrichment of epigenetic markers in 88 tissues, using standard procedures 29 . We used the same Enrichment analyses. Hypergeometric tests were performed with <rs id="a12871292" type="software" subtype="environment">R</rs> (version <rs id="a12871293" type="version" corresp="#a12871292">3.4.0</rs>) to assess whether the genes mapped to genome-wide significant loci and the subset of prioritized biological epilepsy genes (see above) were enriched for: (i) known monogenic epilepsy genes (n = 102) and (ii) known anti-epileptic drug target genes (n = 64), relative to the rest of the protein-coding genes in the genome (n = 19180). We supplemented the list of 43 known dominant epilepsy genes 36 with an additional 59 monogenic epilepsy genes from the GeneDX comprehensive epilepsy panel (www.genedx.com). We compiled the list of drug target genes from 73 , supplemented with additional FDA &amp; EMA licensed AEDs. The full list of gene targets considered in each analysis are listed in Supplementary Tables 8 and10.</p>
<p>We also observed that the 102 established monogenic epilepsy genes are on average 2.44 times longer than non-epilepsy genes (152.0 kb vs 62.2 kb). As a conservative approach to correct for this size difference, we have used the Wallenius' noncentral hypergeometric distribution, as implemented in the <rs id="a12871294" type="software" subtype="environment">R</rs>package '<rs id="a12871295" type="software" subtype="component" corresp="#a12871294">BiasedUrn</rs>'. Using this distribution, we repeated our hypergeometric analyses under the conservative assumption of a 2.42 times increased likelihood of mapping epilepsy genes as opposed to non-epilepsy genes. Using this distribution, the 146 genes that were mapped to genome-wide significant loci were significantly enriched for monogenic epilepsy genes (Wallenius' noncentral hypergeometric test p = 8.3×10 -3 ). When limiting our results to the 21 biological prioritized genes, the enrichment of monogenic epilepsy genes became more significant (Wallenius' noncentral hypergeometric distribution p = 5.3×10 -4 ).</p>
<p>Gene-expression changes associated with epilepsy were imputed from the all epilepsy GWAS summary statistics using the <rs id="a12871296" type="software">FUSION</rs> software package <rs id="a12871297" type="bibr">64</rs> and dorsolateral prefrontal cortex tissue RNA-sequencing data (n = 452, CommonMind Consortium 65 ). We calculated z-scores for the association between epilepsy and changes in expression of all 5261 significantly heritable genes, using default settings of the <rs id="a12871298" type="software">FUSION</rs> software package as described above <rs id="a12871299" type="bibr">64</rs> . The top 10% of the gene-expression changes most strongly associated with epilepsy were used to construct the disease signature. Then, we identified drugs that change geneexpression in the opposite direction in cell lines, using the <rs id="a12871300" type="software" subtype="component" corresp="#a128713000">Combination Connectivity Mapping</rs> <rs id="a128713000" type="software" subtype="environment">bioconductor</rs> package and the Library of Integrated Network-Based Cellular Signatures (LINCS) data 74 . This package utilizes cosine distance as the (dis)similarity metric 75,76 . A higher (more negative) cosine distance value indicates that the drug induces gene-expression changes more strongly opposed to those associated with the disease. A lower (more positive) cosine distance value indicates that the drug induces gene-expression changes more similar to those associated with the disease. In the LINCS dataset, some drugs have been profiled in more than one cell line, concentration, and time-point. For such drugs, the highest absolute cosine distance, whether positive or negative, was selected, as this value is less likely to occur by chance. The output of this analysis comprised 24,051 drugs or 'perturbagens', each with a unique cosine distance value.</p>
<p>We determined whether, in our results, clinically effective drugs are ranked higher than expected by chance. The median rank of all drugs was 12,026. The median rank of clinically effective drugs was 3725. Hence, the median rank of clinically-effective drugs was 8301 positions higher than that of all drugs. A permutation-based p-value was determined by calculating the median ranks of 1,000,000 random drug-sets, each equal in size to the number of clinically effective drugs in the LINCS database. This permutation-based p-value was &lt;1.0 × 10 -6 . Similarly, we determined whether, in our results, experimentally-validated drugs are ranked higher than expected by chance. The median rank of experimentallyvalidated drugs was 6372. Hence, the median rank of experimentally-validated drugs was 5654 positions higher than that of all drugs. A permutation-based p-value was determined by calculating the median ranks of 1,000,000 random drug-sets, each equal in size to the number of experimentally-validated drug repurposing candidates in the LINCS database. This permutation-based p-value was &lt;1.0 × 10 -6 . Heritability analysis. <rs id="a12871301" type="software">Linkage-Disequilibrium Adjusted Kinships (LDAK</rs> <rs id="a12871302" type="bibr">42,43</rs> ) was used to calculate SNP-based heritability of all epilepsy phenotypes. Since these analyses require homogeneous cohorts, only Caucasian subjects (which represent &gt;95% of epilepsy cases) were used for these analyses. SNP based heritabilities (h 2 o ) were converted to liability scale heritability estimates (h 2 L ) using the formula: 8 h 2 L ¼ h 2 o Ã K 2 ð1 À KÞ 2 =pð1 À pÞ Ã Z 2 , where K is the disease prevalence, p is the proportion of cases in the sample, and Z is the standard normal density at the liability threshold. We estimated disease prevalence based on a combination of previous studies 8,78,79 (Supplementary Table 11). Although prevalence estimates vary between studies, the h 2 L estimate has been shown to be fairly robust to such differences 8 . Similarly, we have modeled h 2 L using half and double of our prevalence estimates which lead to h 2 L estimates that varied between 0.4 and 11% (Supplementary Table 11). In addition, we compared the heritability estimates from <rs id="a12871303" type="software">LDAK</rs> with the alternative methods <rs id="a12875516" type="software">BOLT-REML</rs> <rs id="a12875517" type="bibr">80</rs> and <rs id="a12875518" type="software">LDSC</rs> <rs id="a12871304" type="bibr">58</rs> (Supplementary Table 12). Next, <rs id="a12871305" type="software">LDAK</rs> was used to calculate the genetic correlation between the 7 epilepsy subtypes. Subjects with a diagnosis of both CAE and JAE were excluded from heritability and genetic correlation analyses.</p>
<p>We computed the genetic correlation between all, focal and genetic generalized epilepsy with other brain diseases and traits using LDSC, as implemented in <rs id="a12875520" type="software">LD hub</rs> <rs id="a12875521" type="bibr">81</rs> . <rs id="a12875522" type="software">LD hub</rs> is a centralized database that contains publicly available GWAS summary statistics from various diseases and traits. We selected published GWAS of psychiatric, neurological, auto-immune diseases with known brain involvement and cognitive/behavioral traits from <rs id="a12875523" type="software">LD hub</rs>. We contacted the authors of published GWAS to provide us with summary statistics when no summary statistics were available on <rs id="a12875524" type="software">LDhub</rs> or when a more recent GWAS of a disease/trait was published that was not included on <rs id="a12875525" type="software">LDhub</rs>. The Caucasian subset of our data was used for all analyses and only other GWAS with primarily Caucasian subjects were included in our analyses. We used a conservative Bonferroni correction to assess significance of genetic correlations (p = 0.05/48 = 0.001).</p>
</text>
</tei>
  <tei>
    <teiHeader>
        <fileDesc id="f480166633"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:57+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text lang="en">
        <p>The deletion H69/V70 is present in over 600,000 SARS-CoV-2 genome sequences worldwide and has seen global expansion, particularly across much of Europe, Africa, and Asia (Figure 1). DH69/V70 is observed in multiple global lineages (Figure 1A). Although variants with deletions in this region of spike are observed in <rs type="software" id="s1">GISAID</rs> (<ref type="bibr">Shu and McCauley, 2017</ref>), the earliest unambiguous sequences that included the DH69/V70 were detected on a D614 background in January 2020 (United States and Thailand). The earliest DH69/V70 detected on a D614G background occurred in Sweden in April 2020. Prevalence of DH69/V70 has increased since August 2020 (Figure 1B). Further analysis of sequences revealed, first, that single deletion of H69 or V70 was uncommon and, second, that some lineages of DH69/V70 alone were present as well as DH69/V70 in the context of other mutations in spike, specifically those in the receptor binding motif (RBM; Figure 1). DH69/V70 is not a neutralizing antibody escape mechanism We hypothesized that DH69/V70 might confer reduced susceptibility to neutralizing antibodies. We first examined the protein structural context of DH69/V70 for clues regarding alterations in epitopes (Figures 2A and2B). In the absence of experimentally derived structural data for DH69/V70, the protein structure of the NTD possessing the double deletion was modeled in silico. The DH69/V70 was predicted to alter the conformation of a protruding loop comprising residues 69-76, pulling it in toward the NTD (Figure 2B). In the post-deletion structural model, the positions of the alpha carbons of residues on either side of the deleted residues, Ile68 and Ser71, were each predicted to occupy positions 2.9 A ˚from the positions of His69 and Val70 in the pre-deletion structure. Concurrently, the positions of Ser71, Gly72, Thr73, Asn74, and Gly75 are predicted to have changed by 6.5 A ˚, 6.7 A ˚, 6.0 A ˚, 6.2 A ˚, and 8 A ˚, respectively, with the overall effect of these residues moving inward, resulting in a less dramatically protruding loop. This predicted change in the surface of spike could be consistent with antibody evasion. To test this, we explored whether DH69/V70 conferred reduced susceptibility to neutralizing antibodies in sera from 15 recovered individuals (Figures 2C and2D). We performed serial dilutions of sera before mixing with lentiviral particles pseudotyped with spike proteins with and without DH69/V70 (with virus input normalized for infectivity). We plotted infection of target cells as a function of serum dilution (Figure 2D). All but two sera demonstrated clear titratable neutralization of WT and DH69/V70 virus. There was no overall change in susceptibility to serum neutralization for DH69/V70 relative to the WT (Figure 2C), but there was a proportion of individuals with slightly increased neutralization sensitivity of DH69/V70 (Figures 2C and2D). To further explore the role of DH69/V70 in inducing immune escape, we tested the binding of 12 NTD mAbs to the WT and DH69/V70 NTD by biolayer interferometry (Figures 2E-2G). All NTD mAbs showed less than a 2-fold decrease in binding to DH69/V70 compared with the WT. These data suggest that DH69/V70 does not represent an important antibody escape mechanism.</p>
        <p>Detailed methods are provided in the online version of this paper and include the following: using the GTR+F+I model with 1000 ultrafast bootstrap replicates (Minh et al., 2013). All trees were visualized with 
            <rs type="software">Figtree</rs> v.
            <rs type="version">1.4.4</rs> (
            <rs type="url">http://tree.bio.ed.ac.uk/software/figtree/)</rs> and 
            <rs type="software">ggtree</rs> v
            <rs type="version">2.2.4</rs> rooted on the SARS-CoV-2 reference sequence and nodes arranged in descending order. Nodes with bootstraps values of &lt; 50 were collapsed using an in-house 
            <rs type="software">script</rs>.
        </p>
        <p>To reconstruct a phylogeny for the 69/70 spike region of the 20 Sarbecoviruses examined in Figure S4, Rdp5 (Martin et al., 2015) was used on the codon spike alignment to determine the region between amino acids 1 and 256 as putatively non-recombinant. A tree was reconstructed using the nucleotide alignment of this region under a GTR+G substitution model with 
            <rs type="software">RAxML-NG</rs> (Kozlov et al., 2019). Node support was calculated with 1000 bootstraps. Alignment visualization was done using 
            <rs type="software">BioEdit</rs> (Hall et al., 2011).
        </p>
        <p>The structure of the post-deletion NTD (residues 14-306) was modeled using 
            <rs type="software">I-TASSER</rs> (Roy et al., 2010), a method involving detection of templates from the protein data bank, fragment structure assembly using replica-exchange Monte Carlo simulation and atomic-level refinement of structure using a fragment-guided molecular dynamics simulation. The structural model generated was aligned with the spike structure possessing the pre-deletion conformation of the 69-77 loop (PDB 7C2L (Chi et al., 2020)) using 
            <rs type="software">Py-MOL</rs> (
            <rs type="creator">Schro ¨dinger</rs>). Figures prepared with 
            <rs type="software">PyMOL</rs> using PDBs 7C2L, 6M0J (Lan et al., 2020), 6ZGE28 and 6ZGG (Wrobel et al., 2020).
        </p>
        <p>RNA secondary structure modeling 2990 nucleotides centered around the spike protein amino acids 69-70 from SARS-CoV2 sequence from an individual 12 were aligned in CLUSATL-Omega (nucleotides 20277-23265 of the Wuhan isolate MN908947.3) and a consensus structure was generated using 
            <rs type="software">RNAalifold</rs> (Bernhart et al., 2008)).
        </p>
        <p>Cell fusion assay was carried out as previously described (Papa et al., 2021). Briefly, Vero cells and 293T cells were seeded at 80% confluency in a 24 multiwell plate. 293T cells were co-transfected with 1.5 mg of spike expression plasmids in pCDNA3 and 0.5 mg mCherry-N1 using Fugene 6 and following the manufacturer's instructions (Promega). Vero cells were treated with CellTracker Green CMFDA (5-chloromethylfluorescein diacetate) (Thermo Scientific) for 20 minutes. 293T cells were then detached 5 hours post transfection, mixed together with the green-labeled Vero cells, and plated in a 12 multiwell plate. Cell-cell fusion was measured using an Incucyte and determined as the proportion of merged area to green area over time. Data were then analyzed using 
            <rs type="software">Incucyte</rs> software analysis. Data were normalized to cells transfected only with mCherry protein and mixed with green labeled Vero cells. Graphs were generated using 
            <rs type="software">Prism</rs>
            <rs type="version">8</rs> software.
        </p>
        <p>All available full-genome SARS-CoV-2 sequences were downloaded from the GISAID database (https://gisaid.org/; Shu and McCauley, 2017) on 16 th February 2021. Low-quality sequences (&gt; 5% N regions) were removed, leaving a dataset of 491,395 sequences with a length of &gt; 29,000bp. Sequences were deduplicated and then filtered to find the mutations of interest. All sequences were realigned to the SARS-CoV-2 reference strain MN908947.3, using <rs type="software" id="s2">MAFFT</rs> <rs type="version" corresp="s2">v7.475</rs> with automatic strategy selection and the-keeplength-addfragments options (Katoh and Standley, 2013) Serum pseudotype neutralization assay Spike pseudotype assays have been shown to have similar characteristics as neutralization testing using fully infectious wild-type SARS-CoV-2 (Schmidt et al., 2020).Virus neutralization assays were performed on 293T cell transiently transfected with ACE2 and TMPRSS2 using SARS-CoV-2 spike pseudotyped virus expressing luciferase (Mlcochova et al., 2020). Pseudotyped virus was incubated with serial dilution of heat inactivated human serum samples or convalescent plasma in duplicate for 1h at 37 C. Virus and cell only controls were also included. Then, freshly trypsinized 293T ACE2/TMPRSS2 expressing cells were added to each well. Following 48h incubation in a 5% CO 2 environment at 37 C, the luminescence was measured using Steady-Glo Luciferase assay system (Promega). Case et al., 2020). Next, 3 contiguous overlapping fragments were fused by a first overlap PCR (step 2) using the utmost external primers of each set, resulting in 3 larger fragments with overlapping sequences. A final overlap PCR (step 3) was performed on the 3 large fragments using the utmost external primers to amplify the full-length S gene and the flanking sequences including the restriction sites KpnI and NotI. This fragment was digested and cloned into the expression plasmid phCMV1. For all PCR reactions the Q5 Hot Start High fidelity DNA polymerase was used (New England Biolabs Inc.), according to the manufacturer's instructions and adapting the elongation time to the size of the amplicon. After each PCR step the amplified regions were separated on agarose gel and purified using Illustra GFX PCR DNA and Gel Band Purification Kit (Merck KGaA).</p>
        <p>Pseudovirus neutralization assay for testing NTD monoclonal antibodies MLV-based SARS-CoV-2 S-glycoprotein-pseudotyped viruses were prepared as previously described (Pinto, 2020). HEK293T/ 17cells were cotransfected with a WT, B.1.1.7 or B.1.1.7 H69/V70 SARS-CoV-2 spike glycoprotein-encoding-plasmid, an MLV Gag-Pol packaging construct and the MLV transfer vector encoding a luciferase reporter using X-tremeGENE HP transfection reagent (Roche) according to the manufacturer's instructions. Cells were cultured for 72 h at 37 C with 5% CO 2 before harvesting the supernatant. VeroE6 stably expressing human TMPRSS2 were cultured in Dulbecco's Modified Eagle's Medium (DMEM) containing 10% fetal bovine serum (FBS), 1% penicillin-streptomycin (100 I.U. penicillin/mL, 100 mg/mL), 8 mg/mL puromycin and plated into 96-well plates for 16-24 h. Pseudovirus with serial dilution of mAbs was incubated for 1 h at 37 C and then added to the wells after washing 2 times with DMEM. After 2-3 h DMEM containing 20% FBS and 2% penicillin-streptomycin was added to the cells. Following 48-72 h of infection, Bio-Glo (Promega) was added to the cells and incubated in the dark for 15 min before reading luminescence with Synergy H1 microplate reader (BioTek). Measurements were done in duplicate and relative luciferase units were converted to percent neutralization and plotted with a non-linear regression model to determine IC 50 values using 
            <rs type="software" id="s3">GraphPad PRISM</rs> software (version
            <rs type="version" corresp="s3">9.0.0</rs>).
        </p>
        <p>Measurements were done in duplicate and relative luciferase units were converted to percent neutralization against no-drug control which was set as 100%. Data were plotted with a non-linear regression model to determine IC 50 values using 
            <rs type="software" id="s4">GraphPad PRISM</rs> software (version
            <rs type="version" corresp="s4">9.0.0</rs>). The 50% inhibitory dilution (EC 50 ) was defined as the serum dilution at which the relative light units (RLUs) were reduced by 50% compared with the virus control wells (virus + cells) after subtraction of the background RLUs in the control groups with cells only. The EC 50 values were calculated with non-linear regression, log (inhibitor) versus normalized response using
            <rs type="software" id="s5">GraphPad Prism</rs>
            <rs type="version" corresp="s5">8</rs> (
            <rs type="creator" corresp="s5">GraphPad Software, Inc., San Diego, CA, USA</rs>). The neutralization assay was positive if the serum achieved at least 50% inhibition at 1 in 3 dilution of the SARS-CoV-2 spike protein pseudotyped virus in the neutralization assay. The neutralization result was negative if it failed to achieve 50% inhibition at 1 in 3 dilution. Statistical tests are described in the figure legends along with the value of n, mean, and standard deviation/error. Data were normally distributed consistent with statistical methods used.
        </p>
    </text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f554493134"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T10:14+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p> <rs id="a12893592" type="publisher" corresp="#a12893593">Microsoft</rs>® <rs id="a12893593" type="software">Office Excel</rs> (WA, USA), <rs id="a12893594" type="publisher" corresp="#a12893595">IBM</rs>® <rs id="a12893595" type="software">SPSS Statistics</rs> v <rs id="a12893596" type="version" corresp="#a12893595">27</rs> (New York, USA), <rs id="a12893597" type="software">EPI Suite</rs>™ version <rs id="a12893598" type="version" corresp="#a12893597">4.1</rs> (<rs id="a12893599" type="publisher" corresp="#a12893597">US Environmental Protection Agency's Office of Pollution Prevention and Toxics and Syracuse Research Corporation (SRC)</rs>) and <rs id="a12893600" type="software">Python</rs> version <rs id="a12893601" type="version" corresp="#a12893600">3.7.9</rs> were utilised for statistical and data analysis. Linear statistical analysis was used for stablishing the correlations of CEC concentrations and weather data using <rs id="a12893602" type="publisher" corresp="#a12893603">Microsoft</rs>® <rs id="a12893603" type="software">Office Excel</rs> and correlations are expressed either as Pearson or Spearman coefficients, as specified. For the generation of molecular descriptors and physicochemical properties, ACD Labs <rs id="a12893604" type="software">Percepta</rs> (<rs id="a12893605" type="publisher" corresp="#a12893604">Advanced Chemistry Development Laboratories</rs>, ON, Canada) and <rs id="a12893606" type="software">Dragon</rs> version <rs id="a12893607" type="version" corresp="#a12893606">7.0</rs> ( <rs id="a12893608" type="publisher" corresp="#a12893606">Kode Chemoinformatics</rs>, Pisa Italy) were used.</p>
<p>Aside from CEC dominance in either matrix, the remaining compounds showed a wide range of occurrence in either phase. Taking all data for all CECs, no appreciable correlations existed between relative CEC concentration in either matrix, except for a few compounds such as mefenamic acid in the urban area (R 2 = 0.78) with either logP or logD (pH of urban and rural influent was ∼7.5 and 6.3, respectively). However, for CECs quantified in more than five samples in both wastewater influent and effluent at each site (n = 21 compounds) and on which to base a more confident comparison, a moderate correlation existed between sites (Pearson r = 0.65) indicating that there were some similarities in performance levels across the two different regional treatment works (Fig. 4 (c)). A brief evaluation of &gt;1900 molecular descriptors for these molecules (using <rs id="a12894781" type="software">ACD Labs Percepta</rs> and <rs id="a12893611" type="software">Dragon</rs> version <rs id="a12893612" type="version" corresp="#a12893611">7.0</rs> software) revealed no significant correlations existed in general between CEC proportions in effluent/influent for molecular physicochemical properties, basic constitutional indices or functional group counts. However, some moderate correlations existed for a few descriptors including 2D autocorrelations (e.g., SpMax6_Bh(e), the largest eigenvalue n. 6 of Burden matrix weighted by Sanderson electronegativity, average r = 0.52 across sites); and Burden eigenvalues such as GATS5v (Geary autocorrelation of lag 5 weighted by van der Waals volume, r = 0.49). Whilst prediction of sorption behaviour of CECs to solid materials has been demonstrated in the past (Barron et al., 2009), such correlations did not provide a useful basis to develop predictive models on which to understand wastewater treatment efficiency here. Clearly, understanding the removal efficiency of specific mid-polarity CECs during wastewater treatment remains very complex and is not easily generalisable. Another hypothesis is the desorption of CECs from biological materials. Possible desorption of parent compounds could occur during biological treatment processes thereby changing the effluent CEC concentration (and variance thereof) (Angeles et al., 2020).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f327120643"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:31+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Most of the generated electricity in Kosovo is produced from fossil fuel, a part of the energy comes from the import, while participation of renewable resources is symbolic, and a bias between the grid extension and the load of power generated sometimes results in shortage of electricity and thus frequent power cuts. The use of renewable energy and particularly the solar thermal energy represents one of the most promising alternative strategies. In Kosovo, the global horizontal radiation ranges from 1241 kWh/m 2 per year in Shterpce to 1461 kWh/m 2 per year in Gjakova, while the average for Kosovo can be estimated at 1351 kWh/m 2 per year. The average sun duration for the city of Pristine is 5.44 h, while the average horizontal irradiation is 3.79 kWh/m² per day. Participation of energy consumption in household is still dominant -about 41.4% of the total consumption in Kosovo, 15% of this energy is used for domestic hot water. This energy demand can be lowered significantly by using improved building construction techniques and utilization of RESs, especially solar thermal. The first step is to map the city in different areas to locate suitable locations for the installation of solar collectors serving sanitary hot water. The demand for sanitary hot water varies from object to object, this variation depends on whether the building is individual or collective, school institutions or religious buildings, for this reason the classification of buildings was done according to the request for sanitary hot water. After that the demand for sanitary hot water is calculated for several different institutions: Residential houses, Dormitories and Hospitals. For all of the above-mentioned cases the data for: solar fraction, solar contribution, CO 2 avoided, collector temperature, financial analysis etc. are gained using the <rs id="a12965837" type="software">TSOL</rs> <rs id="a12965838" type="version" corresp="#a12965837">2018</rs> software. To evaluate the activation energy for a time period, the daily, monthly and annual performance for three systems which are located in University Clinical Center of Pristine, Kosovo have been analyzed. In addition the results of the mathematical model, simulation and measured solar energy contribution for solar station in Infective disease clinic have been compared. In this paper, a proposal for replacing the conventional water heaters with the domestic solar water heaters (DSWH) is made. A case study for 38289 Residential households in Pristine has been selected. The initial cost of the solar water heater for the city is 60113730 €. The system saves 7274910 € annually and reduced C𝑂 2 emission by 22973400 kg. The results from the paper show that the DSWH is economically feasible in Pristine and can result in fuel saving and CO 2 emission reduction.</p>
<p>To locate suitable locations for the installation of solar collectors serving sanitary hot water, the first step is to divided the city in some areas Figure 2b). The second step is to calculate the roofs area for city of Pristine, which area estimates the space available for solar thermal installation [5]. To estimate how much energy could be generated from the sun on a surface, first the area should be calculated to see how many solar panels could be placed on it. There are various roof tops measuring tools online or software, the software <rs id="a12965839" type="software">AutoCAD</rs> is used Figure 3.</p>
<p>For simulation is used <rs id="a12965840" type="software">T*SOL</rs> software, the <rs id="a12965841" type="software">T*SOL</rs> software is the program that allows to accurately calculate the yield of a solar thermal system dynamically over the annual cycle. With <rs id="a12965842" type="software">T*SOL</rs> one can optimally design solar thermal systems, dimension collector arrays and storage tanks, and calculate the economic efficiency [9]. Since SWH cannot meet 100% of the residential apartment's demands or social institutions, there is a need for auxiliary boiler in all stations. In order for the system to function only with renewable energy throughout the year it is connected with district heating in winter season. As a general conclusion it can be suggested that for sanitary hot water temperature from 45 to 60˚C: the average total solar fraction for Pristine is 58%, the average solar contribution is 605 kWh/m 2 , the average CO 2 emissions per panel's meter square is 146 kg/m 2 and total annual savings in CO 2 emissions in total is 69127.5 kg, and the average Savings District heating is 4355.96 kWh.</p>
<p>The fraction of the annual water heating load supplied by solar energy is determined by repeating the calculation of X, Y, and f for each month and summing the results as indicated by Equation 4. The Table 7 shows the results of these calculations. In this part the f-chart numerical model will be used to calculate the monthly absorbed solar energy for the year 2018, these results will be compared to data from the <rs id="a12965843" type="software">T*SOL</rs> software. Weather data, monthly average ambient temperature, monthly solar radiation, monthly clarity index, ground reflectance/absorptance, the overall loss coefficient and optical performance previously calculated were also used in the calculations as well as the daily heat load previous are mentioned before. In the following, an annual overview of the Infectious diseases Clinic was made. Annual solar contribution, money saves and C𝑂 2 , reduction were calculated as follows:</p>
<p>The Table 10 shows the total annual solar contribution that is simulated with <rs id="a12965844" type="software">T*SOL</rs> is 40960 kWh per year, while the solar contribution calculated with mathematical model is 38397.06 kWh per year, but data from measured with less than mathematical model and simulation, 29602 kWh per year. Mathematical model and <rs id="a12965845" type="software">T*SOL</rs> yielded somewhat higher total solar contribution compared to the measurements, which is mainly explained by the lower solar irradiation the measured year compared to the climatic data used in the simulation.</p>
<p>Participation of energy consumption in household is still dominant -about 41.4% of the total consumption in Kosovo, 15% of this energy used for domestic hot water. This energy demand can be lowered significantly by using improved building construction techniques and utilization of RES-s, especially solar thermal. Replacing the electrical heaters with solar water heaters could also lead to reducing the peak load demands. The use of clean energy in electricity generation will also lead to the reduction of carbon dioxide emission to some extent. The energy model was developed for a typical house in the Pristine City using the <rs id="a12965846" type="software">T*SOL</rs> Software and compared with F-chart method. This basic scenario considers a typical house with 5 occupants (the average size of the family is 4.8 occupants) situated at the Pristine city baring alone the full initial cost of the SWH purchase and installation. Other scenarios can also be developed for number of Households in the city. Based in data from the Kosovo Agency of Statistics the number of residential households in Pristine is 38289.</p>
<p>But we managed to partly widen the system boundaries and method basis in the study of solar thermal especially for domestic hot water, which seems to be a prerequisite to understand the complexity of the dissemination process and identify barriers that tend to appear in the different implementation phases. Based on the result from mathematical model, experimental part, and simulation with <rs id="a12965847" type="software">T*SOL</rs> software, we can conclude that: altogether with a good technological base, creates favorable conditions for the exploitation of solar energy in Kosovo.</p>
<p>The aim of this project was to study a solar domestic water heating system, and replacing the electric water heating system with solar water heating system for city of Prishtina is proposed, to develop a numerical model based on the fchart method and to compare results obtained from the model with data obtained from <rs id="a12965848" type="software">T*SOL</rs> software and experimental data.</p>
<p>To locate suitable locations for the installation of solar collectors serving sanitary hot water, the first step is to divide the city in some areas, and after that, to calculate appropriate roof space the software <rs id="a12965849" type="software">AutoCAD</rs> is used.</p>
<p>It is clear that the demand for sanitary hot water varies from object to object. In some sectors, sanitary hot water is necessary for example in residential buildings, while in others it increases the quality of life, for example in medical education institutions etc. Another important factor is to calculate monthly and yearly average total radiation on a tilted surface for the city. For this calculation based on the Duffie and Beckman (2013) [7], mathematical software <rs id="a12965850" type="software">MathCAD</rs> has been used:  The monthly average radiation incident on the collector is 4.198 kWh/m 2 /day, After that, the demand for sanitary hot water for several different institutions has been calculated, in different points of the city: residential house, building, dormitory and two clinics. For these institutions for simulation is
used <rs id="a12965851" type="software">T*SOL</rs> software that allows to accurately calculate the yield of a solar thermal system dynamically over the annual cycle.</p>
<p>Based on the gained values for Solar Hot Water System at the Infectious diseases Clinic, Solar I is seen that the maximum value is around 12-13 o'clock, the highest recorded value is 30000 Wh in July, in this system usually at night from 19 to 9 o'clock there is no energy exchange. After the comparison the data recorded by the controller from Solar station II in Infectious diseases Clinic and the data obtained with the <rs id="a12965852" type="software">T*SOL</rs> Software for daily solar contribution. The analysis shows a slight difference of solar contribution between the values recorded in this station and the values obtained by simulating with <rs id="a12965853" type="software">TSOL</rs> software. The main reason is the weather data for 2018 year, where it is known that this year especially July had lower temperatures compared to what <rs id="a12965854" type="software">T*SOL</rs> receives from <rs id="a12965855" type="software">Meteonorm</rs> software.</p>
<p>The mathematical model also developed for Infectious diseases Clinic -Solar II based on the f-chart method by Duffie and Beckman (2013) [7]. For sanitary hot water temperature 45˚C average total solar fraction for this station is 64.9%, the annual solar contribution is 38397.06 kWh. Then the <rs id="a12965856" type="software">T*SOL</rs> software was used to compare data for solar water heating system: between mathematical model and data from software, this difference for solar fraction is 0.05%. Based in the data from mathematical model, for annual solar contribution 38397.06 kWh is calculated:  The money saved 2788.839 €;  The C𝑂 2 , reduction 8851.533 kg.</p>
<p>Based on the results for annual solar contribution and annual financial savings was reached the result for cumulative cash flow analysis. The simple and equity payback periods are 8.6 years. The investigation for replacing the electrical heaters with solar water heaters for city of Prishtina was based in terms of the electricity cost, capital cost, maintenance cost and the CO 2 emission. The energy model was developed for a typical house with 5 occupants in the Prishtina City using the <rs id="a12965857" type="software">T*SOL</rs> Software and compared with F-chart method. Based on results of the numerical analysis:</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f81074199"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T08:18+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>As we have stated in ''Liberal view of democracy'' section, in the liberal view of democracy, filter bubbles can be seen as a form of market failure that diminishes user control and hence autonomy, hide available options and coerce people in such a way that they cannot get what they want. Users will not get the search results they were looking for, or do not receive the updates from friends they want to in a social networking platform. Designers that takes this view will develop tools that aim to promote awareness of filter bubbles and attempt to give users some sense of control. User satisfaction and awareness of options and choice seem to be the most common goals. As we will show in this subsection, this view of the filter bubble can be realized by giving users the control over the filters, increasing awareness of their own biases or increasing the awareness of the presence of filters that are implemented in common web services. <rs id="a12951556" type="publisher" subtype="person" corresp="#a12951557">Munson et al.</rs> (2013) developed a browser tool called <rs id="a12951557" type="software">Balancer</rs>, that tracks users' reading activities and shows their reading behavior and biases, in order to increase awareness (See Also Fig. 1b). Munson et al. argue that, while many people agree that reading a diverse set of news is good, many do not realize how skewed their own reading behavior is. <rs id="a12951558" type="software">Balancer</rs> therefore shows an approximate histogram of the user's liberal and conservative pages, with the hope that the given feedback will nudge users to make their reading behavior more balanced. Munson et al. (2013) found that very low number of users changed their reading habits (conservatives consuming more liberal items and liberals more conservative). The majority of the users did not change their reading habits at all. While <rs id="a12951559" type="software">Balancer</rs> aims for users to reflect their preferences and on the long-term increase the epistemic quality of the incoming information, Measures must be taken so that antagonism is transformed into agonism</p>
<p>The minorities are excluded from the democratic process, their voices are lost the primary goal is to increase user-awareness. Hence this tool belongs to the user autonomy enhancing technologies that are motivated by a liberal conception of democracy. <rs id="a12951560" type="software">Scoopinion</rs><rs id="a12951561" type="bibr">1</rs> is a browser add-on that tracks news sites and the type of stories one reads while using the browser. <rs id="a12951562" type="software">Scoopinion</rs> (See Fig. 1a) provides a visual summary of one's reading habits by displaying user's media fingerprint. The tool also personalizes recommended stories based upon user's reading habits, but by displaying the media fingerprint, it assumes that the user will choose to read more diversely. It works with a white list of news sites and does not make diverse recommendations. It provides a visualization of users' information consumption habit to increase their autonomy, but it has no clear goals such as tolerance or better information quality. Again this fits a liberal conception of democracy and prioritizes the value of choice autonomy. <rs id="a12951563" type="publisher" subtype="person" corresp="#a12951564">Xing et al.</rs> (2014) developed a browser add-on called <rs id="a12951564" type="software">Bobble</rs> that allows users to compare their Google search results with other profiles worldwide. The tool (See Fig. 2) uses hundreds of nodes to distribute a user's <rs id="a12951566" type="software">Google</rs> search queries worldwide each time the user performs a Google search. For example, when a user performs a Google search with keyword ''Obamacare'', this search keyword is distributed to 40? worldwide <rs id="a12951569" type="software">Bobble</rs> clients that perform the same Google search and return corresponding search returns. Users then can see which results are displayed on their browser, but not on others, and vice versa. It is a tool for users to get an idea of the extent of personalization taking place. The tool aims to increase user's awareness of Google's filters. However, it does not aim to increase deliberation or provide challenging information by its design.</p>
<p>As we have mentioned in ''Deliberative democracy'' section, filter bubbles can be seen as a problem, not because they prevent users getting what they want, but because they diminish the quality of the public discussion. Deliberative democracy assumes that users are, or should be, exposed to diverse viewpoints, so that they can discover disagreements, truths, perspectives and finally make better decisions. Polarized users or users exposed to low quality (but agreeable and relevant) information will have bad 2014), a browser add-on that displays user's news consumption habits. Larger circles are news outlets that the user consumed the most items. b <rs id="a12951572" type="software">Balancer</rs> (<rs id="a12951573" type="bibr">Munson et al. 2013</rs>) is a browser add-on that shows users their biases. In this picture the user is biased towards reading from liberal news outlets consequences. In order to increase the epistemic quality of information, a wide range of opinions and perspectives on a particular topic may be made more visible and users can compare their opinions with others, even if they are opposing their own views. In the end, respect, legitimacy and consensus can be reached. In this subsection, we will list some of the tools that allow users to discover different viewpoints by visualization, showing pro/con arguments for a controversial topic, nudging them to listen to others, or by diversifying search results by modifying them for political search queries.</p>
<p><rs id="a12951575" type="software">ConsiderIt</rs> (<rs id="a12951576" type="bibr">Kriplean et al. 2012</rs>;<rs id="a12951577" type="bibr">Freelon et al. 2012</rs>) is a deliberation (pro/con) tool that is developed with the aims of (1) helping people learn about political topics and possible tradeoffs between different opinions, (2) nudging them toward reflective consideration of other voters' thoughts, (3) enabling users to see how others consider tradeoffs. <rs id="a12951578" type="software">ConsiderIt</rs> (Fig. 4) provides an interface where users can create pro/con lists by including existing arguments others have contributed, to contribute new points themselves, and to use the results of these personal deliberations to expose salient points by summarizing their stance rather than a yes/no vote. Users can see ranked lists of items that were popular full opposers, firm opposers, slight opposers, neutrals, slight supporters, firm supporters and full supporters. In a pilot study called ''The Living Voters Guide'' (LVG), the system was put into testing during the 2010 Washington state elections that had certain proposals on areas of tax, sale of alcohol, candy or bottled water, state debt, bail and other political topics. In LVG, 8823 unique visitors browsed the site and 468 people submitted a position on at least one item. In a small survey of 7 users, 46.3 % of them have reported that they have actually changed their stances on at least one measure and 56 % of them saying they switched from support to oppose or vice versa. 32 % of them have reported that they moderated their stances and 12 % saying they strengthened them (Kriplean et al. 2012).</p>
<p> <rs id="a12951547" type="software">OpinionSpace</rs> (<rs id="a12951579" type="bibr">Faridani et al. 2010</rs>) plots on a two-dimensional map the individual comments in a web forum, based on the commenters' responses to a short value-based questionnaire. By navigating this space, readers are better able to seek out a diversity of comments as well as prime themselves for engaging the perspective of someone with different values (Fig. 5). When users interrogate an individual comment, they are prompted to rate comments for how much they agree with and respect it. The size of the comment's dot on the map then grows when people with different values than the speaker respect and/or agree with it, facilitating users in seeking out comments that resonate widely.</p>
<p><rs id="a12951580" type="software">Reflect</rs> (<rs id="a12951581" type="bibr">Kriplean et al. 2011</rs>) modifies the comments of webpages in order to encourage listening and perspective taking. It adds a listening box next to every comment, where other users are encouraged to succinctly restate the points that the commenter is making, even if there is disagreement (Fig. 6). This is a nudge to listen to other users. Other readers can afterwards read the original comment Fig. 4 <rs id="a12951582" type="software">ConsiderIt</rs> (<rs id="a12951583" type="bibr">Kriplean et al. 2012</rs>;<rs id="a12951584" type="bibr">Freelon et al. 2012</rs>) helps people learn about political topics and possible tradeoffs between different opinions and other listeners' interpretations of what was said, supporting broader understanding of the discussion. In this way, users do not have to ''like'' or ''recommend'' the comment to recognize or appreciate the speaker. By nudging towards listening and reflecting, an empathetic and constructive normative environment is formed, where not only those who speak and reflect are positively affected, but those who read as well. In mid-September 2011, the popular online discussion platform <rs id="a12951585" type="software">Slashdot</rs> enabled <rs id="a12951586" type="software">Reflect</rs> on four stories. During the trial, 734 reflections were written by 247 discussants, an average of 1.0 reflection per comment. While flaming and pure replies were present (31 %), the majority of the reflections were neutral, different neutral interpretations or meta observations. The tool also allowed the community to rate reflections, making certain reflections under a threshold invisible. After users downvoted flaming or cheeky replies on those reflections, almost 80 % of all the visible reflections were neutral reflections.</p>
<p><rs id="a12951587" type="software" subtype="component" corresp="#a12951588">Rbutr2</rs> is a community driven <rs id="a12951588" type="software" subtype="environment">Chrome</rs> add-on, that informs a user when the webpage they are viewing has been disputed, rebutted or contradicted elsewhere on the Internet (Fig. 7). Users can add opposing viewpoints for an item, so that future users will see that an opposing viewpoint exists for the item they are reading. <rs id="a12951589" type="software">Rbutr</rs> aims to increase information quality and informed opinions by promoting fact and logic-checking.</p>
<p>There are other tools and studies that aim to increase epistemic quality of information. Liao andFu (2013, 2014) studied the effect of the perceived threat, the level of topic involvement, and the effect of expertise and position indicators. Munson and Resnick (2010) studied the effect of nudging by sorting or highlighting agreeable news items and experimenting with the ratio of challenging and agreeable news items. <rs id="a12951590" type="software">Newscube</rs> (<rs id="a12951591" type="bibr">Park et al. 2009</rs>(<rs id="a12951592" type="bibr">Park et al. , 2011</rs>) is a tool that detects different aspects of a news using keyword analysis, and displays users news items with different perspectives in order to decrease media bias. <rs id="a12951593" type="software">Hypothes.is3</rs> is a community peer-review tool that allows the users to highlight text and add comments and sentence-level critic. <rs id="a12951594" type="software">Political Blend</rs> (<rs id="a12951595" type="bibr">Doris-Down et al. 2013</rs>) is a mobile application that matches people with different political views and nudges them to have a cup of coffee face to face and discuss politics.</p>
<p>One of the key finding of our analysis is that the norms specified by agonistic and contestatory models of democracy are completely missing in all of the tools that aim to fight the filter bubble. While it is possible to come across critical voices, disadvantaged views or contestation using tools such as <rs id="a12951548" type="software">OpinionSpace</rs> or <rs id="a12951596" type="software">ConsiderIt</rs>, it is also highly (1983) have argued, media should not only proportionally reflect differences in politics, religion, culture and social conditions, but provide equal access to their channels for all people and all ideas in society. If the population preferences were uniformly distributed over society, then Increase the epistemic quality of information satisfying the first condition (reflection) would also satisfy the second condition (equal access). However, this is seldom the case (Van Cuilenburg 1999). Often population preferences tend toward the middle and to the mainstream. In such cases, the media will not satisfy the openness norm, and the view of minorities will not reach a larger public. This is undesirable, because social change usually begins with minority views and movements (van Cuilenburg 1999).</p>
<p>Several agonistic design attempts have been developed in the industry throughout the years to reveal hegemony (one of the requirements of agonistic design). Most of these tools perform social network analysis to identify actors and their connections (networks of force) and represent the multifaceted nature of hegemony. For instance the project <rs id="a12951598" type="software">Mulksuzlestirme</rs> (dispossession in Turkish) compiles data collectively and then uses mapping and visualization techniques to show the relations between the capital and power within urban transformations in Turkey. The interactive map (See Fig. 8) displays the established partnerships between the government and private developers and shows to which investors collected taxes have been transmitted through the redevelopment/privatization of public spaces. 4 For instance, it shows that one corporation that is involved in many government projects also owns major news organizations in the country, including the Turkish version of the CNN. By means of visualization, the designer allows users to browse and discover interesting relationships between the media and corporations to reveal hegemony.</p>
<p>While tools such as <rs id="a12951599" type="software">Mulksuzlestirme</rs> might reveal key information for political debates and elections, many of these tools are not widely known. Tools like these can spread in unfiltered platforms such as Twitter, if powerful actors and opinion leaders can spread them through their followers (Ju ¨rgens et al. 2011). However, Twitter has stated that it plans to deploy a personalized algorithmic timeline in the future (Panzarino 2014). If one wants their message to spread in a filtered/personalized platform, it has to bypass the filters or perhaps trick them. In order to accomplish this, one either has to pay for advertisements (and must hence possess the necessary financial means) or one must have the technical skills (such as search engine optimization). Many people do not have either of these means, but yet, they might have key information that is vital for contestation. Further, we could not find designs/tools that implement other benchmarks of agonism, such as special attention to minority voices.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f491168109"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:48+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Bayesian regression models were fitted using <rs id="a12965465" type="software">RStan</rs> <rs id="a12965486" type="bibr">[63]</rs>, with some of the analysis carried out using the <rs id="a12965467" type="software">rethinking</rs> package <rs id="a12965487" type="bibr">[64]</rs>. Linear models were fitted to logtransformed viral loads, with random effects for each patient and study applied to the parameters determining the peak viral load, which we assumed coincides with the onset of symptoms, and rate of its decline over time. Samples for which no virus was detected were treated as being below limit of detection (LOD), rather than truly virus negative. We show the general form of the regression model here, where L is the likelihood of each data point, to illustrate the random-effect structure and the censoring of the data:</p>
<p>Here variable 'Age' is equal to 1 for patients aged 60 or over (equal to 0 otherwise). We also looked at including subject age as a continuous variable, but the goodness of fit did not change appreciably. We added the 3 terms (age, severity, sex) to the regression models both separately and in combination. We assessed the goodness of fit using the Watanabe Akaike Information Criterion (WAIC) [65], with the best-fitting model having the lowest WAIC value. As the WAIC for each candidate model is estimated from a finite sample, its standard error was calculated using the <rs id="a12965469" type="software">rethinking</rs> package <rs id="a12965488" type="bibr">[64]</rs> to appreciate the uncertainty in its value. These standard errors are useful when appraising the differences in WAIC between candidate models. The relative goodness of fit of a given model can also be appraised by calculating its Akaike weight amongst the set of all considered models. This can be interpreted as the probability that this model, out of the set of models considered, would provide the best fit to new (i.e. out of sample) data [65].</p>
<p>We use simulation-based estimation of statistical power to assess our capacity to detect a difference in viral load dynamics due to one of the three factors (sex, age, severity of disease) assessed here. We generated synthetic datasets of the same size as the one considered here, with study-and patient-specific random effects of the same magnitude (Supplementary Table 3). For simplicity, we generated datasets with an equal number of samples per subject, and the sampling times were randomly generated from a uniform distribution. When generating the data, we assumed that the peak viral load was influenced by one of the three aforementioned factors. We then ran the regression analysis, to see if the modelled effect could be detected. To reduce computation time, we here used frequentist regression via the <rs id="a12965471" type="software" subtype="component" corresp="#a12965472">lme4</rs> package in <rs id="a12965472" type="software" subtype="environment">R</rs> <rs id="a12965489" type="bibr">[66]</rs>, with a p value &lt; 0.05 for the included fixed effect indicating a significant finding. Generating 1000 synthetic datasets, the statistical power can be estimated as the percentage of datasets for which the regression analysis found a significant effect. Supplementary Fig. 4 shows how the statistical power varies with the magnitude of the modelled effect, which is here expressed as a fold difference in the peak viral load. The results of the power analysis suggest that we were not well powered to measure relatively small differences in peak viral load (under 10-fold difference). power to measure sex-specific differences in peak viral load was slightly higher than the power to measure severity-or age-specific differences, as we have a better balance between samples from male and female samples than we do between samples in under 60 s and over 60 s, or samples from subjects from mild disease versus samples from subjects with moderate or severe disease.</p>
<p>This means that k 0 a and I 0 max represents the population-level average value of each parameter. The system of equations was solved numerically in <rs id="a12965474" type="software" subtype="environment">R</rs>, using the <rs id="a12965475" type="software" subtype="component" corresp="#a12965476">dopri</rs> function from the <rs id="a12965476" type="software" subtype="component" corresp="#a12965474">dde</rs> package <rs id="a12965490" type="bibr">[67]</rs>. We show how this numerical solution can be obtained in the code repository that accompanies this article (see the section 'Availability of Code'). We write the modelled viral load trajectory at time t for patient i in study j as V ij (t). Samples from the posterior distribution of the fitted parameter were obtained using Markov Chain Monte Carlo methods, via the <rs id="a12965478" type="software" subtype="environment">R</rs> package <rs id="a12965479" type="software" subtype="component" corresp="#a12965478">drjacoby</rs> <rs id="a12965491" type="bibr">[68]</rs>. As with the regression modelling, the form of the likelihood for each data point depends on whether the sample is above the studyspecific LOD or is recorded as being virus-negative. We write D ijk to indicate the kth viral load sample, taken t k after the onset of symptoms, from patient i in study j. The likelihood for each data point has the form:</p>
<p>The <rs id="a12965481" type="software" subtype="environment">R</rs> <rs id="a12965482" type="software" subtype="implicit" corresp="#a12965481">scripts</rs> used to carry out the analyses presented and generate the figures are available in a Github repository: <rs id="a12965484" type="url" corresp="#a12965482">https://github.com/JDChallenger/ Viral_Load_Covid19</rs>.</p>
<p>Anonymised viral load data is available alongside this article, in the <rs id="a12965485" type="software">Excel</rs> worksheet 'CombinedDataset.xlsx'.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f334591864"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:46+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>This was an IMPRINT members-only survey, circulated via the members' website and no formal ethical approval was sought. The UK National Health Service (NHS) Health Research Authority decision tool was used and determined that the study did not require review by an NHS Research Ethics Committee. Members could decide freely whether to contribute or not and, by participating, they implied their consent. Experienced researchers facilitated the surveys and ethical considerations for survey research were followed; as for other social science research methods, this involved informing potential participants of the purpose of the research and how their contribution would be used, as well as assuring confidentiality and appropriate use of sensitive material. The study was compliant with general data protection regulation and followed published recommendations on conducting good quality survey research [18,19]. Email addresses were given by survey respondents, although any identifiable data was anonymised for the data analysis (undertaken in Microsoft <rs id="a12893777" type="software">Excel</rs>, <rs id="a12893778" type="publisher" corresp="#a12893777">Microsoft Corporation</rs>, Redmond, WA, USA). Members of the IMPRINT network automatically consent to sharing contact details, such as email addresses, with the other members when joining the network; this is a core element of the network enabling fruitful communication and collaboration. Details can be found on the IMPRINT website, https://www.imprint-network.co.uk [16].</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f81845784"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T14:15+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Read extraction, assembly and phylogenetic analysis of L-lactate dehydrogenase (ldh) genes For phylogenetic assignment, ldh genes were reassembled based on raw reads and contigs from existing assemblies, with hits to KEGG gene K00016 from both metagenome and metatranscriptome data (see Additional file 5: Text S1) and ldh genes with a protein length of ≥310 aa were considered near full-length and included into the phylogenetic analysis. Reference ldh sequences from rumen bacterial isolates with hits to K00016 were extracted from the <rs id="a12895546" type="software">IMG/M</rs> database in June 2015. Amino acid sequences were aligned using <rs id="a12895547" type="software">MUSCLE</rs> <rs id="a12953721" type="bibr">[47]</rs>, and alignments were imported into <rs id="a12895549" type="software">ARB</rs> (v. <rs id="a12895550" type="version">6</rs>) <rs id="a12953722" type="bibr">[48]</rs> for manual refinement. Phylogenetic maximum likelihood bootstrap trees with 100 re-samplings were constructed using <rs id="a12895552" type="software">RAxML</rs> (v. <rs id="a12895553" type="version">7.7.2</rs>) <rs id="a12953723" type="bibr">[49]</rs>, and the best scoring tree including bootstrap values was re-imported into <rs id="a12895555" type="software">ARB</rs> for cluster annotation.</p>
<p>Reference genome sequences and gene annotations from the rumen isolates S. azabuensis DSM20406 and M. elsdenii J1 were obtained from the Department of Energy Joint Genome Institute Genome portal [50]. Metatranscriptome reads of each HMY and LMY sample were mapped to the two reference genomes as well as metagenome and metatranscriptome reads to all reassembled ldh genes and all genes in the custom lcdA genes database (for information on database construction, see Additional file 5: text S1) using <rs id="a12895556" type="software">BBmap</rs> ( <rs id="a12895557" type="url" corresp="#a12895556">http://sourceforge.net/projects/bbmap/</rs>) with an ID cut-off of 98 % sequence similarity for ldh genes and genome sequences, and 60 % sequence similarity for lcdA genes and counting ambiguous reads for all matching genes. Read counts were normalised to RPM, and statistical analysis of normalised read counts was conducted in R via the WRS test and Benjamini-Hochberg correction (for all genes in isolate genomes and ldh genes) to select genes or transcripts with significantly different abundances between the HMY and LMY animals.</p>
<p>Functional identifiers of KEGG orthology genes from the metagenome dataset that showed significant correlation to methane yield in both the WRS test and sPLS analyses were uploaded into <rs id="a12895558" type="software">IMG/MER</rs> and used as screening IDs for all the bacterial genomes available from the Hungate 1000 project (http://genome.jgi.doe.gov/The-Hunmicrobiome/) and all additionally available bacterial genomes derived from rumen habitats in June 2015 using the "<rs id="a12895559" type="software" subtype="component">functions versus genomes</rs>" tool in <rs id="a12895560" type="software" subtype="environment">IMG/MER</rs>.</p>
<p>Additional file 1: Table S1. Overview of samples analysed in this study and methods of analysis conducted. (PDF 223 kb) Additional file 2: Figure S1. Sparse partial least squares regression analysis (sPLS) of gene (A) or transcript (B) abundances correlated with animal methane yield plotting low (green), intermediate (blue) and high (red) methane yield animals based on gene abundance or expression values of selected predictor genes. (TIF 168 kb) Additional file 3: Table S2. KEGG genes from metagenome (DNA) and metatranscriptome (RNA) data commonly identified in sparse partial least squares analysis (sPLS) and Wilcoxon rank sum test (WRS) correlated to methane yield/significantly differentially abundant between high and low methane yield sheep. (PDF 507 kb) Additional file 4: Table S3. Gene set enrichment analysis of metagenome and metatranscriptome datasets. Pathways with genes differentially present or expressed based on nominal P value (NOM P val ≤0.05) are shown, ranked by normalised enrichment score (NES). Corrected P values for false discovery rate (FDR q val) and familywise-error rate (FEWER P val S8. KEGG functions from metagenome data commonly identified in Wilcoxon rank sum test and sparse partial least squares analysis as related to methane yield against their occurrence in Hungate 1000 and other rumen-derived genomes available in the <rs id="a12895561" type="software">IMG</rs> database in June 2015. (XLSX 204 kb)</p>
</text>
</tei>
  <tei>
    <teiHeader>
        <fileDesc id="f148333522"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T16:15+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text lang="en">
        <p>Figure 1A illustrates the processing steps involved in the discrimination of bursts of beta activity. The digitally filtered and the rectified bipolar LFP signal that was generated online as a substrate for thresholding was also recorded for off-line analysis. Such data were first visually inspected and 200 s of artefact-free rectified signal in each condition was selected using 
            <rs type="software" id="s1">Spike</rs>
            <rs type="version" corresp="s1">2</rs> software (
            <rs type="creator" corresp="s1">Cambridge Electronic Design</rs>).
        </p>
        <p>All further signal processing and data analysis steps were performed using 
            <rs type="software" id="s2">Matlab</rs> (version
            <rs type="version" corresp="s2">R 2015b</rs>;
            <rs type="creator" corresp="s2">MathWorks</rs>, Natick, MA). Data were downsampled to 200 Hz. To adjust for potential baseline shifts, we performed a direct current (DC) correction (20 s time constant) of the selected rectified signal.
        </p>
        <p>Statistical analyses were performed using 
            <rs type="creator" corresp="s3">IBM</rs>
            <rs type="software" id="s3">SPSS Statistics</rs> Version
            <rs type="version" corresp="s3">23</rs>. All data are presented as means AE SEM, unless otherwise stated. The assumption of a normal distribution was tested by visual inspection of the QQ-plots. For all repeated measures ANOVAs, if Mauchly's test indicated that the assumption of sphericity was violated, Greenhouse-Geisser corrections were applied. In this case, corrected F and P-values with corrected degrees of freedom were reported. Post hoc paired t-tests were Bonferroni corrected. All reported P-values from the correlation analyses have been corrected for multiple comparisons using the false discovery rate procedure.
        </p>
        </text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f592039787"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:33+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The
COCONUT database is free and open to all users and there is no login required to access it. Its web interface allows diverse simple searches (e.g. by molecule name, InChI, InChI key, SMILES, drawn structure, molecular formula), advanced search by molecular features, together with substructure and similarity searches. Users can also download the whole dataset or search results in different formats. The database can be queried programmatically via a REST API, which facilitates
COCONUT integration in workflows. The web interface, the back-end and the database are deployed as <rs type="software" subtype="environment" id="a1">Docker</rs> <rs type="software" subtype="component" corresp="#a1">containers</rs>, making it easily portable for hosting other sets of NPs and to be deployed on local installations.</p>
<p>Each unique NP is then assigned a unique identifier, composed of the "CNP" prefix and 7 digits. An automatic curation for NP metadata is performed, comprising the retrieval of its official name, synonyms, cross-references AfroDB 874 [34] AfroMalariaDB 252 [35] AnalytiCon Discovery NPs 4908 [36] BIOFACQUIM 400 [37] BitterDB 625 [38] Carotenoids Database 986 [39] ChEBI NPs 14603 [20] ChEMBL NPs 1585 [21] ChemSpider NPs 9027 [40] CMAUP (cCollective molecular activities of useful plants) 20868 [7] ConMedNP 2504 [41] ETM (Ethiopian Traditional Medicine) DB 1633 [42] Exposome-explorer 478 [43] FooDB 22123 [9] GNPS (Global Natural Products Social Molecular Networking) 6740 [44] HIM (Herbal Ingredients in-vivo Metabolism database) 962 [45] HIT (Herbal Ingredients Targets) 470 [46] Indofine Chemical Company 46 [47] InflamNat 536 [48] InPACdb 122 [49] InterBioScreen NPEdia 16166 [59] NuBBEDB 2022 [5] p-ANAPL 467 [60] Phenol-explorer 681 [61] PubChem NPs 2828 [27] ReSpect 699 [62] SANCDB 592 [63] Seaweed Metabolite Database (SWMD) 348 [64] Specs Natural Products 745 [65] Spektraris NMR 242 [66] StreptomeDB 6058 [67] Super Natural II 214420 [3] TCMDB@Taiwan (Traditional Chinese Medicine database) 50862 [8] TCMID (Traditional Chinese Medicine Integrated Database) 10552 [68] TIPdb (database of Taiwan indigenous plants) 7742 [69] to other major chemical databases. Then, a range of molecular properties, descriptors and fingerprints (full list in Table 2) are computed using the in-build CDK [13] libraries. As the number of the computed properties is quite big (73 fields in each document corresponding to one unique NP), only a selected fraction of them is displayed on the
COCONUT web interface. Finally, the first round of automatic curation of NP metadata, in particular the molecular name synonyms, cross-references with other major chemical databases, correction of the literature references (PubMed identifiers and DOIs) and taxonomy is performed. All original data, unified NPs and the derived and calculated information are stored in <rs id="a12967013" type="software">MongoDB</rs>. The chemical classification of all NPs in COCONUT is performed with <rs id="a12967015" type="software">ClassyFire</rs> <rs id="a12967156" type="bibr">[14]</rs>, and, when available, is displayed in the corresponding section of the compound page. <rs id="a12967017" type="software">ClassyFire</rs> provides a hierarchical chemical classification of chemical compounds and enables grouping NPs by their chemical class. Additionally, frameworks facilitating NPs analyses for their chemical and therapeutic properties are computed for NPs, such as Murcko frameworks [15], Ertl Functional Groups [16] and deepSMILES [17]. <rs type="software" id="a2">DeepSMILES</rs> is an adaptation of SMILES for use in deep machine learning of chemical structures. Due to the increased usage of deep learning in chemistry, it is indeed interesting to provide this new chemical representation type pre-computed for NPs.</p>
<p>NPs common names in COCONUT have been retrieved, when available, from their databases of origin. The remaining NPs have been searched by InChI in major chemical databases (PubChem, ChEMBL and ChEBI) and common names and synonyms were retrieved when occurences the compound were found. Additionally to this, IUPAC names were computed for all COCONUT NPs using <rs id="a12967036" type="publisher" corresp="#a12967037">ChemAxon</rs>'s <rs id="a12967037" type="software">MolCovert</rs> <rs id="a12967157" type="bibr">[20]</rs>, to add nomenclature homogeneity to the dataset. Furthemore, the IUPAC names are used as the main NP name when no official chemical name has been found nor in the original sources, nor by searching big chemical databases. Therefore, all NPs in COCONUT have at least one molecular name.</p>
<p>Figure 2 demonstrates the distributions and relationships of a small selection of computed molecular features within COCONUT. Sugar moieties occur frequently in NPs and have an important impact on their bioactivities and physicochemical properties. However, they are often redundant and therefore obstruct the study of the aglycon. For this reason, NPs in COCONUT have been analysed for sugar moieties presence, and a deglycosylated structure representation was made available in the database. Sugar moieties manipulations were performed using the <rs id="a12967042" type="software">Sugar Removal Utility</rs> <rs id="a12967158" type="bibr">[21]</rs>. To track their influence on other features, their absence and presence are colour-mapped (no sugar moiety in the molecular structure in blue, and the presence of at least one sugar moiety in orange). The wide molecular weight range is typical for NPs; it is, however, interesting to notice its correlation with the number of oxygen atoms in the molecule, regardless of the presence and absence of sugar.</p>
<p>Another interesting correlation to be noted is between the molecular weight and the nitrogen atom number in sugar-free molecules. The NP-likeness score [22], trained on high-quality NPs dataset and computed with <rs id="a12967044" type="software">NaPLeS</rs> <rs id="a12967159" type="bibr">[23]</rs>, which was trained on high-quality NPs dataset, has a typical distribution for an NPs set, where most molecules have a positive score. At this point, an additional NPs curation step has been performed, due to the possible inconsistency in genuine NPs of one of the used sources, SuperNatural II. NPs that are not occuring in other datasets used to assemble COCONUT, but only in SuperNatural II, have been thoroughly tested. To be kept in COCONUT and be considered as a genuine or predicted NP, such a molecule has to have a strictly positive NP-likeness score, be classified as a NP by <rs id="a12967050" type="software">NPclassifier</rs> <rs id="a12967160" type="bibr">[24]</rs>, a deep neural network-based structural classification tool specialised in NPs or have a sugar moiety in its structure. The 24,880 molecules from SuperNatural II that didn't pass this additional quality control have been removed from COCONUT, until further proof of their natural provenance.</p>
<p>The geographic location of the collection or the natural presence of the NP-producing organism is a piece of information that is even more difficult to obtain. Nowadays, a range of organisms, and in particular plants, can be found in different parts of the planet due to globalisation and their success in human consumption (e.g. garlic, tomatoes, curcuma or ginger). It is, therefore, difficult, if not impossible, to determine their original provenance. Also, the geographical information is often omitted in literature and most NPs databases. When available, the geographical provenance is stored in the <rs id="a12967067" type="software">MongoDB</rs> dump of COCONUT, but not displayed on the website.</p>
<p>All COCONUT data is stored with <rs id="a12967072" type="software">MongoDB</rs>, a crossplatform document-oriented NoSQL database program.</p>
<p>The smallest unit in <rs id="a12967073" type="software">MongoDB</rs> is a document, composed of key and value pairs that are similar to JSON objects. Documents of the same nature are organized in collections, which are the equivalent of the SQL-based databases tables. <rs id="a12967074" type="software">MongoDB</rs> is particularly adapted to big and complex data, supports multiple indexing, including text indexing allowing enhanced text search in text-indexed fields and contains a wide range of in-build search and analysis functions.</p>
<p>Two major collections are present in the
COCONUT database: SourceNaturalProduct, which contains the original NPs data collected from the open sources, and UniqueNaturalProduct, the unified and curated collection of NPs. The full version of
COCONUT with all the calculated features can be accessed as a <rs id="a12967077" type="software">MongoDB</rs> dump in the Downloads section of the website. Requests for displaying additional crucial features in the web interface and making them searchable through the advanced search interface are welcome via the
COCONUT
GitHub tracker (see below). The
COCONUT online front-end is developed entirely with <rs id="a12967162" type="software" subtype="component" corresp="#a12967163">React.js</rs> <rs id="a12967161" type="bibr">[26]</rs>, a <rs id="a12967163" type="software" subtype="environment">JavaScript</rs> library to build responsive and efficient user interfaces. The <rs id="a12967084" type="software">OpenChemLib</rs> library <rs id="a12967164" type="bibr">[27]</rs> is used to handle the chemical editor for the search functions. The
COCONUT back-end, allowing to process the front-end requests and to communicate with the database is written in <rs id="a12967087" type="language" corresp="#a12967086">Kotlin</rs> and <rs id="a12967088" type="language" corresp="#a12967086">Java</rs> <rs id="a12967089" type="version" corresp="#a12967088">11</rs> using the <rs id="a12967090" type="software">Spring</rs> framework. The <rs id="a12967091" type="software">CDK</rs> <rs id="a12967165" type="bibr">[13]</rs> library is used to process chemical information and formats.</p>
<p>
COCONUT web interface, back-end and database are entirely Dockerised, allowing a quick and easy deployment on local servers and cloud. All the <rs id="a12967094" type="software" subtype="implicit">code</rs>, for both front-end and back-end, is available on
GitHub ( <rs id="a12967096" type="url" corresp="#a12967094">https :// githu b.com/mSoro k/Natur alPro ducts Onlin e</rs>).</p>
<p>
COCONUT online has been developed to be a fullfledged chemical database and in particular to fit the NPs structural and annotational particularities, with all the subsequent functions. At the moment, the chemical search is uncommon with <rs id="a12967098" type="software">MongoDB</rs>, therefore several approaches have been implemented to run molecular substructure and similarity searches.</p>
<p>The so-called "simple" search can be performed using the header search bar. The users can enter there molecule names (e.g. "curcumin"), SMILES, InChI, InChi key, COCONUT ids and molecular formulas. Name search uses native <rs id="a12967100" type="software">MongoDB</rs> text indexing, allowing fuzzy, flexible search in the "name" and "synonyms" fields. The input string type is first identified using regular expressions, then the DB is queried against the appropriate fields, and the result, when exists, is returned to the front-end.</p>
<p>Searching for an exact substructure in a <rs id="a12967101" type="software">MongoDB</rs> database of molecules appears to be surprisingly easy. Each molecule in the database needs to have their fingerprints of choice (in
COCONUT are used the <rs id="a12967103" type="software">PubChem</rs> fingerprints) to be precomputed and stored as a list of bytes (
BinData type in <rs id="a12967104" type="software">MongoDB</rs>). The query molecule (substructure) then needs to have its fingerprint to be also computed and to be matched against the database using the $allBitsSet function [28]. This native to <rs id="a12967107" type="software">Mon-goDB</rs> function allows to select documents in a collection where a BinData field has all the query bits set to "on" (but can have bits set to "on" that are not present in the query). To confirm the substructure match, the user can select between the default Ullmann [29], the Vento-Foggia [30] and the depth-first (DF) [31] pattern matching approaches, all performed using the CDK in-build algorithms. These three pattern matching techniques tend to, generally, return very similar results, the difference between them lying rather in their approach to matching substructures, therefore the usage of the default, Ullmann, method is to be privileged by users unfamiliar with the intricacies of such matching.</p>
<p>Similarity search with <rs id="a12967108" type="software">MongoDB</rs> was implemented following the excellent ChEBML blog post on LSH-based similarity search in <rs id="a12967167" type="software">MongoDB</rs> <rs corresp="#a12967167" type="bibr">[32]</rs> and adapted to <rs type="software" subtype="language" id="a3">Java</rs>, <rs type="software" subtype="language" id="a4">Kotlin</rs> and <rs type="software" subtype="environment" id="a42">Spring</rs> data. In this approach, the <rs id="a12967114" type="software">MongoDB</rs> aggregation framework is used to perform inverted indexing search against PubChem fingerprints stored in a separate table and referencing COCONUT identifiers that contain the molecular features encoded by each bit.</p>
<p>The online
COCONUT database is an open tool for researchers in the natural products community. COCO-NUT is the biggest collection of NPs in 2020 and the data it contains already benefits researchers in NPs with various aims, such as biodiversity research and drug discovery. The web interface allows querying and parsing the data collection in various, chemically relevant ways with adequate performance. It is also the first big chemical database using <rs id="a12967125" type="software">MongoDB</rs> as a storage management system.</p>
<p>All COCONUT data, code to process raw NPs data, data quality control and annotation, and the code for the font-and the back-end of the COCONUT online website are freely available without any restriction. The latest COCONUT data, as <rs id="a12967132" type="software">MongoDB</rs> full dump can be downloaded at https ://cocon ut.natur alpro ducts .net/downl oad. <rs id="a12967134" type="software" subtype="implicit">Code</rs> for data assembly, processing and quality control process codes is available on GitHub at <rs id="a12967136" type="url" corresp="#a12967134">https ://githu b.com/mSoro k/COCON
UT</rs>. The <rs id="a12967137" type="software" subtype="implicit">code</rs> for the front-end and back-end is also available on GitHub at <rs id="a12967139" type="url" corresp="#a12967137">https
://githu b.com/mSoro k/Natur alPro ducts Onlin e</rs>.</p>
<p>the <rs id="a12967140" type="software" subtype="implicit">source code</rs> of the web interface and the back-end is available on GitHub at <rs id="a12967142" type="url" corresp="#a12967140">https ://githu b.com/mSoro k/Natur alPro ducts Onlin e</rs>. The data was curated and processed using the
COCONUT code suite available on
GitHub at
https :// githu b.com/mSoro k/COCON UT. All
COCONUT data can be accessed on the website at https ://cocon ut.natur alpro ducts .net/ and downloaded entirely or partially in several formats (<rs type="software">MongoDB</rs> dump, SDF and SMI (SMILES)).</p>
</text>
</tei>
  <tei>
    <teiHeader>
        <fileDesc id="f322694223"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:58+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text lang="en">
        <p>The authors, members of "The NETwork to Target Neutrophils in COVID-19," thank Andrew Whiteley for organizing the discussions that are summarized in this paper; Laura Maiorino for generating the illustrations; Laura Maiorino and Stephen Hearn for the photomicrograph of NETs; and Bruce Stillman and David Tuveson for discussions and critical review of the manuscript. We acknowledge the Cold Spring Harbor Laboratory CCSG P30CA45508 and Microscopy shared resource. 
            <rs type="software">BioRender</rs> was used to generate the illustrations in the figures. We apologize to those with relevant work that was not included in the discussion due to space limitations.
        </p>
    </text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f204144905"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:18+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>In order to establish the presence of each type of variation (between populations, size classes, contexts and individuals) potentially present in orangutan voiceless calls, we conducted generalized linear mixed model analyses (GLMM) using <rs id="a12951842" type="software">R</rs> <rs id="a12951837" type="bibr">26</rs> and the function <rs id="a12951838" type="software" subtype="component" corresp="#a12951840">lmer</rs> of the <rs id="a12951839" type="software" subtype="environment">R</rs>-package <rs id="a12951840" type="software" subtype="component" corresp="#a12951839">lme4</rs> <rs id="a12951841" type="bibr">27</rs> . Our two acoustic parameters max frequency and durationrepresented the response variable of two separate models. "Size class" factor comprised 3 levels (i.e. adolescent, adult, large flangedmale morph) and "context" 5 levels (i.e. towards other orangutans, other animals, observers, other humans, and predator models), and were inserted in our models as fixed effects. Because individuals and populations were sampled repeatedly, these factors were considered random effects, with "population" factor exhibiting 4 levels (i.e. 4 different populations) and "individual" factor 48 levels (i.e. 48 different individuals).</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f81085808"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T07:05+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Clustering scientific publications in an important problem in bibliometric research. We demonstrate how two software tools, <rs id="a12971715" type="software">CitNetExplorer</rs> and <rs id="a12971716" type="software">VOSviewer</rs>, can be used to cluster publications and to analyze the resulting clustering solutions. <rs id="a12971717" type="software">CitNetExplorer</rs> is used to cluster a large set of publications in the field of astronomy and astrophysics. The publications are clustered based on direct citation relations. <rs id="a12971718" type="software">CitNetExplorer</rs> and <rs id="a12971719" type="software">VOSviewer</rs> are used together to analyze the resulting clustering solutions. Both tools use visualizations to support the analysis of the clustering solutions, with <rs id="a12971720" type="software">CitNetExplorer</rs> focusing on the analysis at the level of individual publications and <rs id="a12971721" type="software">VOSviewer</rs> focusing on the analysis at an aggregate level. The demonstration provided in this paper shows how a clustering of publications can be created and analyzed using freely available software tools. Using the approach presented in this paper, bibliometricians are able to carry out sophisticated cluster analyses without the need to have a deep knowledge of clustering techniques and without requiring advanced computer skills.</p>
<p>In this paper, our aim is to demonstrate how two software tools that we have developed, <rs id="a12971722" type="software">CitNetExplorer</rs> (<rs id="a12971723" type="bibr">Van Eck and Waltman 2014a, b</rs>; <rs id="a12971724" type="url" corresp="#a12971722">www.citnetexplorer.nl</rs>) and <rs id="a12971725" type="software">VOSviewer</rs> (<rs id="a12971726" type="bibr">Van Eck andWaltman 2010, 2014b</rs>; <rs id="a12971727" type="url" corresp="#a12971725">www.vosviewer.com</rs>), can be used to cluster publications and to analyze the resulting clustering solutions. We use <rs id="a12971728" type="software">CitNetExplorer</rs> to cluster publications based on their citation relations and to analyze the resulting clustering solutions at the level of individual publications. We use <rs id="a12971729" type="software">VOSviewer</rs> to analyze the clustering solutions obtained using <rs id="a12971730" type="software">CitNetExplorer</rs> at an aggregate level. <rs id="a12971731" type="software">CitNetExplorer</rs> and <rs id="a12971732" type="software">VOSviewer</rs> both rely strongly on visualizations to facilitate the analysis of clustering solutions.</p>
<p> <rs id="a12971733" type="software">CitNetExplorer</rs>, which is an abbreviation of 'citation network explorer', is a software tools that we have developed for analyzing and visualizing citation networks. In the approach that we take in this paper, we first use <rs id="a12971734" type="software">CitNetExplorer</rs> to cluster publications based on their citation relations. For this purpose, <rs id="a12971735" type="software">CitNetExplorer</rs> employs a clustering technique that we have introduced in earlier papers (Waltman andVan Eck 2012, 2013).</p>
<p>We then use <rs id="a12971736" type="software">CitNetExplorer</rs> to analyze the resulting clustering solution at the level of individual publications. To facilitate the analysis of a clustering solution, the following features of <rs id="a12971737" type="software">CitNetExplorer</rs> are essential:</p>
<p>• Visualizing a citation network. <rs id="a12971738" type="software">CitNetExplorer</rs> can be used to visualize a citation network of publications, with publications shown along a time axis and with colors indicating the clusters to which publications belong. Using the visualization functionality of <rs id="a12971739" type="software">CitNetExplorer</rs>, we obtain an overview of the most frequently cited publications in a citation network, the citation relations between these publications, and the clusters to which the publications belong. • Drilling down into a citation network. The drill down functionality of <rs id="a12971740" type="software">CitNetExplorer</rs> can be used to analyze a clustering solution at different levels of detail. We may for instance start with a visualization at the level of the entire citation network. We may then perform a drill down into one or more selected clusters, after which we are provided with a visualization at the level of the subnetwork consisting of the publications belonging to the selected clusters. • Searching for publications. We can search for publications based on title, publication year, author name, and journal name. The search functionality of <rs id="a12971741" type="software">CitNetExplorer</rs> can be used to find publications that are of special interest, for instance all publications in a specific journal, and to find out to which clusters these publications belong.</p>
<p> <rs id="a12971742" type="software">VOSviewer</rs> is a software tool for constructing and visualizing bibliometric networks. In this paper, <rs id="a12971743" type="software">VOSviewer</rs> is used to complement <rs id="a12971744" type="software">CitNetExplorer</rs>. While we use <rs id="a12971745" type="software">CitNetExplorer</rs> to analyze a clustering solution at the level of individual publications, we use <rs id="a12971746" type="software">VOSviewer</rs> to analyze a clustering solution at an aggregate level. Two visualizations provided by <rs id="a12971747" type="software">VOSviewer</rs> play an important role. The first visualization shows the clusters in a clustering solution and the citation relations between these clusters. The second visualization uses a so-called term map to indicate the topics that are covered by a cluster. This visualization shows the most important terms occurring in the publications belonging to a cluster and the co-occurrence relations between these terms.</p>
<p>This paper is organized as follows. ''Clustering technique'' section discusses the clustering technique that is used by <rs id="a12971748" type="software">CitNetExplorer</rs> to cluster publications based on their citation relations. ''Results'' section demonstrates the use of <rs id="a12971749" type="software">CitNetExplorer</rs> and <rs id="a12971750" type="software">VOSviewer</rs> to cluster publications and to analyze the resulting clustering solutions. <rs id="a12971751" type="software">CitNetExplorer</rs> is used to cluster more than 100,000 publications in the field of astronomy and astrophysics, and <rs id="a12971752" type="software">CitNetExplorer</rs> and <rs id="a12971753" type="software">VOSviewer</rs> are used together to analyze the resulting clustering solutions. ''Conclusion'' section concludes the paper.</p>
<p>In this paper, we use the clustering technique that is available in the <rs id="a12971754" type="software">CitNetExplorer</rs> software tool. This section provides a discussion of this clustering technique. ''Determining the relatedness of publications'' section explains how the relatedness of publications is determined, and ''Clustering publications'' section describes how publications are assigned to clusters. We refer to Waltman andVan Eck (2012, 2013) for a more extensive discussion of our clustering technique.</p>
<p>The value of the resolution parameter c in (1) should be chosen based on the purpose of the cluster analysis. Higher values of this parameter will yield a larger number of clusters. In other words, the higher the value of c, the higher the level of detail of the clustering solution that will be obtained. In <rs id="a12971755" type="software">CitNetExplorer</rs>, the default value of c is 1. However, we emphasize that there is no generally optimal value of c. Our recommendation to users of our clustering technique is to try out different values of c and to choose the value that seems to give the most useful results for the specific needs of a user.</p>
<p>We now demonstrate how <rs id="a12971756" type="software">CitNetExplorer</rs> and <rs id="a12971757" type="software">VOSviewer</rs> can be used to cluster publications and to analyze the resulting clustering solutions. In our demonstration, we work with a large data set of publications in the field of astronomy and astrophysics. We emphasize that in this paper it is not our aim to assess the quality of our clustering solutions or to compare our clustering solutions with other alternative solutions. We do not have the domain knowledge required to provide an in-depth interpretation of our clusters and to assess their quality. For a comparison of our clustering solutions with other alternative solutions, we refer to the comparison paper by Velden et al. (2017) in this special issue. data set is 111,616. The publications appeared in 59 different journals. Of the 4,311,953 cited references provided in the publications in the data set, 929,364 point to publications in the data set. The statistics for the data set are summarized in Table 1.</p>
<p> <rs id="a12971758" type="software">CitNetExplorer</rs> requires a citation network to be acyclic. When analyzing a citation network, <rs id="a12971759" type="software">CitNetExplorer</rs> will make sure that the network is acyclic by removing citation relations that cause the network to have cycles. <rs id="a12971760" type="software">CitNetExplorer</rs> will also remove citation relations for which the citing publication appeared in an earlier year than the cited publication (e.g., a publication from 2009 citing a publication from 2010). In the case of our data set, of the 929,364 citation relations between publication in the data set, 3824 were removed by <rs id="a12971761" type="software">CitNetExplorer</rs>. Hence, the citation network analyzed using <rs id="a12971762" type="software">CitNetExplorer</rs> included 925,540 citation relations.</p>
<p>We clustered the publications in our data set using the clustering technique that is available in <rs id="a12971763" type="software">CitNetExplorer</rs>. We refer to ''Clustering technique'' section for a discussion of this clustering technique. Our citation network of 111,616 publications has a largest component that includes 101,828 publications. Only these 101,828 publications were included in the cluster analysis. The other 9788 publications were not assigned to a cluster.</p>
<p>As already explained in ''Clustering publications'' section, clustering solutions can be created at different levels of detail. The choice of the most suitable level of detail is not a technical one but instead depends on the purpose of the cluster analysis. Our recommendation is to create multiple clustering solutions at different levels of detail and to use the solution (or the solutions) that fits best with the needs one has. In line with this idea, we used <rs id="a12971764" type="software">CitNetExplorer</rs> to create four clustering solutions, each providing a different level of detail. The clustering solutions are based on different values of the resolution parameter and the minimum cluster size parameter. Clusters that did not meet the minimum cluster size criterion were merged with larger clusters. We note that the four clustering solutions do not have a hierarchical relationship with each other. For instance, a cluster in the most detailed clustering solution may overlap with more than one cluster in the second most detailed clustering solution.</p>
<p>We first use <rs id="a12971765" type="software">CitNetExplorer</rs> to analyze the level 1 clustering. The analysis takes place at the level of individual publications. In the next subsection, we use <rs id="a12971766" type="software">VOSviewer</rs> to perform an analysis at an aggregate level.</p>
<p>For a given set of publications, <rs id="a12971767" type="software">CitNetExplorer</rs> can be used to get an overview of the most frequently cited publications, the citation relations between these publications, the temporal order of the publications, and the assignment of the publications to clusters. Suppose we are interested to get a better understanding of the publications belonging to level 1 clusters 1, 2, 3, and 4 (i.e., the four largest level 1 clusters). Figure 1 provides a <rs id="a12971768" type="software">CitNetExplorer</rs> visualization of the 100 most frequently cited publications in these four clusters. Each publication is indicated by a circle, and publications are labeled by the last name of the first author. The vertical dimension represents time, with publications in the top part of the visualization being older and publications in the bottom part being more recent. In the horizontal dimension, publications are positioned based on their relatedness in terms of citations. Publications that are strongly related in terms of citations, taking into account not only direct citation relations between publications but also indirect citation relations, tend to be located close to each other in the horizontal dimension. Publications that are only weakly related in terms of citations are located further away from each other. The curved lines between publications indicate citation relations, with the citing publication always being located below the cited publication. The darker lines represent direct citation relations, while the lighter lines represent indirect citation relations. There is an indirect citation relation from publication A to publication B if publication A does not directly cite publication B but if publication A for instance cites publication C and publication C in turn cites publication B. The color of a publication indicates the cluster to which the publication belongs, with blue, green, purple, and orange corresponding with, respectively, clusters 1, 2, 3, and 4.</p>
<p>The visualization provided in Fig. 1 is static. In the <rs id="a12971769" type="software">CitNetExplorer</rs> software tool, the same visualization is presented in an interactive way. This for instance makes it possible to zoom in on a specific area in the visualization and to explore in more detail the publications located in that area. Also, by hovering the mouse over a publication, bibliographic information on the publication is presented, for instance the authors, the title, and the journal in which the publication appeared.</p>
<p>Suppose next that we would like to explore level 1 cluster 2 in more detail. This can be done using the drill down functionality of <rs id="a12971770" type="software">CitNetExplorer</rs>. This functionality makes it possible to drill down into a specific subnetwork of a citation network. In this case, a drill down is performed into the subnetwork consisting of the publications belonging to cluster 2 and the citation relations between these publications. After drilling down, the visualization presented in Fig. 2 is obtained. Of the 8954 publications belonging to cluster 2, the visualization shows the 100 most frequently cited ones. As discussed in ''Using <rs id="a12971771" type="software">CitNetExplorer</rs> to cluster publications'' section, publications were clustered at four levels of detail. In the visualization, the color of a publication is determined by the cluster to which the publication belongs in the level 3 clustering. As can be seen in the visualization, the most frequently cited publications in level 1 cluster 2 belong mostly to three different level 3 clusters. These clusters are indicated using the colors red, brown, and light blue in the visualization.</p>
<p>An analysis using <rs id="a12971772" type="software">CitNetExplorer</rs> takes place at the level of individual publications. In many cases, one may also want to analyze a clustering solution at an aggregate level. This is not possible using <rs id="a12971773" type="software">CitNetExplorer</rs>, but it can be accomplished using other software tools. In particular, <rs id="a12971774" type="software">VOSviewer</rs> can be used for this purpose, as discussed in the next subsection.</p>
<p>We now use <rs id="a12971775" type="software">VOSviewer</rs> to carry out a further analysis of the level 1 clustering. The analysis is performed at an aggregate level and uses two visualizations. One visualization shows the level 1 clusters and the citation relations between these clusters. The other visualization uses a term map to indicate the topics that are covered by a level 1 cluster.</p>
<p> <rs id="a12971776" type="software">VOSviewer</rs> has its own clustering technique <rs id="a12971777" type="bibr">(Waltman et al. 2010)</rs>, and this clustering technique was used to partition the 22 level 1 clusters into four groups. This was done based on the citation relations between the clusters. In the visualization presented in Fig. 3, each cluster has a color (i.e., red, green, blue, or yellow) that indicates the group to which the cluster was assigned. In this way, a breakdown of the astronomy and astrophysics literature into broad subfields is obtained. A rough interpretation of the visualization is as follows. The red clusters in the right area of the visualization seem to cover research in astroparticle physics, gravitational physics, and cosmology. The blue and yellow clusters in the bottom area seem to cover astrophysics research on galaxies and stars. The clusters in the top-left area, colored green, seem to relate to research in solar physics and planetary science.</p>
<p>Suppose now that we would like to get a better understanding of a specific level 1 cluster, for instance cluster 3. For this purpose, we use the term map visualization presented in Fig. 4. To create this visualization, the titles and abstracts of the 7998 publications belonging to cluster 3 were analyzed using natural language processing techniques (Van Eck and Waltman 2011). For each publication, the terms occurring in the title and abstract of the publication were identified. Of all terms that were found in at least 15 publications, the 1420 terms that seemed most relevant were algorithmically selected. These terms are shown in the term map visualization provided in Fig. 4. Each term is represented by a circle, and some terms are also indicated by a label. ( <rs id="a12971778" type="software">VOSviewer</rs> aims to avoid overlapping labels, and therefore labels are visible only for some of the terms.) The size of a term reflects the number of publications in which the term was found, and the distance between two terms offers an approximate indication of the relatedness of the terms. The relatedness of terms was determined based on co-occurrences. In other words, the larger the number of publications in which two terms were both found, the stronger the relation between the terms. Colors represent groups of terms that are relatively strongly related to each other. These groups were identified using the clustering technique of <rs id="a12971779" type="software">VOSviewer</rs> that was also mentioned above. In the visualization, the strongest relations between terms are also indicated using curved lines.</p>
<p>We have demonstrated the use of <rs id="a12971780" type="software">CitNetExplorer</rs> and <rs id="a12971781" type="software">VOSviewer</rs> for clustering publications based on direct citation relations and for analyzing the resulting clustering solutions. We have shown how the visualizations provided by the two software tools complement each other, with <rs id="a12971782" type="software">CitNetExplorer</rs> focusing on visualizations at the level of individual publications and <rs id="a12971783" type="software">VOSviewer</rs> focusing on visualizations at an aggregate level.</p>
<p>We have demonstrated the capabilities of <rs id="a12971784" type="software">CitNetExplorer</rs> and <rs id="a12971785" type="software">VOSviewer</rs> for clustering publications and for analyzing the resulting clustering solutions. However, the combined use of the two software tools is somewhat laborious, and preparing the input data for <rs id="a12971786" type="software">VOSviewer</rs> based on the clustering results provided by <rs id="a12971787" type="software">CitNetExplorer</rs> is not entirely straightforward. In future research, we therefore plan to work on the development of a single integrated software tool in which many of the key features of <rs id="a12971788" type="software">CitNetExplorer</rs> and <rs id="a12971789" type="software">VOSviewer</rs> are brought together. We have in mind a tool that combines different types of interactive visualizations to support users in exploring the scientific literature. A technique for clustering publications based on direct citation relations, similar to the technique used in this paper, will be at the core of the new tool. Like in this paper, it will be possible to create clustering solutions at different levels of detail. The new tool will provide interactive functionality for browsing through a hierarchical structure of clusters, and the tool will use visualizations similar to the ones used in this paper to show citation relations between publications and between clusters and to indicate the topics covered by clusters. The dynamics of clusters, revealing for instance how interest in a topic has grown or declined over time, will be made visible as well.</p>
</text>
</tei>
  <tei>
    <teiHeader>
        <fileDesc id="f224635034"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T16:19+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text lang="en">
        <p>Gammaproteobacteria and Bacilli were more common in infants than in mothers while mothers had more Clostridia, Bacteroides and Verrumicrobiae (Fig. 3d). On class level, Actinobacteria was most abundant in fecal samples and Bacilli in milk samples. The most abundant genus in feces of infants and mothers was Bifidobacterium and genus Streptococcus in breast milk (Fig. 3c). Genera which were significantly different (<rs type="software">DESeq2</rs>, negative binomial GLMs, Wald's test, p &lt; 0.05) between infants and mothers and 1-and 6-month-old infants using DESeq2 25 analysis are depicted in Supplementary Fig. 3 and Supplementary Fig. 4.</p>
        <p>Host DNA was removed using paired-end mapping with 
            <rs type="software" id="s1">Bowtie2</rs> 41 version
            <rs type="version" corresp="s1">2.3.3</rs> against a human reference genome GRCh38.p9 GCF_000001405.35 and unmapped paired-end reads were filtered using
            <rs type="software" id="s2">SAMtools</rs> 42,43 version
            <rs type="version" corresp="s2">1.4</rs> with parameters view -b -f 12 -F 256.
            <rs type="software" id="s3">BEDTools</rs> 44 version
            <rs type="version" corresp="s3">2.26.0</rs> was used to convert the bam files to fastq files containing the non-human paired-end reads. Adapters were removed from the non-human reads with
            <rs type="software" id="s4">cutadapt</rs> 45 version
            <rs type="version" corresp="s4">1.10</rs> with default parameters. Mean sequencing depth of the non-human DNA was 1.9 Gbp for fecal samples (Supplementary Data 2) and 160 Mbp for milk samples (Supplementary Data 3). Host DNA content in the breast milk samples varied between 78 and 96%.
        </p>
        <p>Species level community profiling based on marker genes was done using 
            <rs type="software" id="s5">Metaphlan2</rs> 46 version
            <rs type="version" corresp="s5">2.6.0</rs> by running the
            <rs type="software" id="s6">metaphlan2.py</rs> command with the-input_type fastq-nproc 5 options. Merged abundance table was created using the
            <rs type="software">Metaphlan2 utils</rs>
            <rs type="software" subtype="implicit">script</rs>. The merged abundance table was edited to only include taxa which were identified to species level. Additional community profiling was done using
            <rs type="software" id="s7">Metaxa2</rs> 47 version
            <rs type="version" corresp="s7">2.1</rs> for 16S rRNA read extraction in paired-end mode. The 16S rRNA sequence reads were classified using <rs type="software" id="s8"> mothur's</rs> 48 (version
            <rs type="version" corresp="s8">1.39.5</rs>) classify.seqs command with SILVA 49 v. 123 as the reference database, with the cutoff = 60, probs = F and processors = 8 parameters. A custom Unix
            <rs type="software" subtype="implicit">script</rs> was used to create an OTU table based on the classifications. Strain-level profiling and strain tracking analysis as done using
            <rs type="software">Strainphlan</rs> 50 and the extract_markers. py command to extract markers for Escherichia coli, Klebsiella pneumoniae and all Streptococcus and Staphylococcus species. The strainphlan.py command with-relaxed_parameters2 option was used to create taxonomic trees of the strains in all samples.
        </p>
        <p>Resistome and mobilome were characterized by mapping metagenomic reads to a comprehensive non-redundant database of more than 2700 mobile genetic element and 3100 antibiotic resistance genes including Comprehensive Antibiotic Resistance Database protein homolog model version 1.1.2 (CARD) 51 and 
            <rs type="software" id="s9">ResFinder</rs> version
            <rs type="version" corresp="s9">2.1</rs> 52 . Some analyses were performed with only the Resfinder database, which has only acquired or mobile ARGs. A custom MGE database was created by fetching CDS for genes that were annotated as IS*, ISCR*, intI1, int2, istA*, istB*, qacEdelta, tniA*, tniB*, tnpA*, or Tn916 transposon ORFs or genes in the NCBI 53 nucleotide database and
            <rs type="software">PlasmidFinder</rs> database 54 . Redundancy in the databases was removed with
            <rs type="software">VSEARCH</rs> 55 -usearch_global command with-id 99 option. In total, the MGE database consists of genes with 278 different gene name annotations and more than 2000 unique sequences excluding the sequences from 
            <rs type="software" id="s10">PlasmidFinder</rs> database. The custom MGE database is available from
            <rs type="url" corresp="s10">https:// github.com/KatariinaParnanen/MobileGeneticElementDatabase</rs>.
        </p>
        <p>
            <rs type="software">Bowtie2</rs> 41 mapping was done with options -D 20 -R 3 -N 1 -L 20 -i S,1,0.50 and was used to map reads the ARG and MGE databases. 
            <rs type="software">SAMtools</rs> 42,43 was used to filter and count reads and if both reads mapped to the same gene the read was counted as one match and if the reads mapped to different genes, both were counted as hits to the respective gene.
        </p>
        <p>Preliminary data processing for statistical analysis. Statistical analysis was done in 
            <rs type="software" subtype="environment" id="s11">R</rs> 57 version
            <rs type="version" corresp="s11">3.4.1.</rs>
            <rs type="software">Metaphlan</rs>2 44 ,
            <rs type="software">Metaxa2</rs> 45 and ARG and
            <rs type="software">MGE</rs> mapping results, taxonomy and annotation tables and metadata files were compiled into individual data objects in
            <rs type="software" id="s12">phyloseq</rs> 58 version
            <rs type="version" corresp="s12">1.20.0</rs>. All custom
            <rs type="software" subtype="environment">R</rs>
            <rs type="software" subtype="implicit">codes</rs> are available in Supplementary Software file. The ARG and
            <rs type="software">MGE</rs>
            <rs type="software" id="s14">Bowtie</rs>
            <rs type="version" corresp="s14">2</rs> counts were normalized to the length of the respective gene. The length-normalized numbers were then further normalized to the number of bacterial 16S rRNA gene reads divided by the length of the 16S rRNA gene. The 16S rRNA gene was chosen for normalization instead of library size to account for variation in non-bacterial DNA content in the samples. The normalized values were used in all the following analyses. All figures if not indicated otherwise were drawn with
            <rs type="software" id="s15">ggplot</rs>
            2 59 version
            <rs type="version" corresp="s15">2.2.1</rs>.
        </p>
        <p>Ordination analysis. Principal coordinate analyses of the taxonomic profiles ARGs and MGEs was done on presence-absence and relative abundance data using cmdscale command in
            <rs type="software" subtype="environment">R</rs>. Distances between samples were calculated using Horn-Morisita 60 similarity index obtained with the vegdist command from
            <rs type="software" id="s16">vegan package</rs> 61 version
            <rs type="version" corresp="#a16">2.4-3</rs>. Permutational multivariate analysis of variance (PER-MANOVA) between different groups was done with adonis in
            <rs type="software">vegan</rs> 61 with similarity index using 9999 permutations and the resulting p-values were corrected with Benjamini &amp; Hochberg procedure for multiple testing using the p.adjust command in
            <rs type="software" subtype="environment">R</rs>.
        </p>
        <p>Statistical significances of differences in ARG and MGE total sum relative abundances normalized to total 16S rRNA gene counts and length were calculated using negative binomial generalized linear models (GLMs) in the
            <rs type="software" id="s17">MASS</rs> package 64 version
            <rs type="version" corresp="s17">7.3-47</rs> and corrected using Tukey's post hoc test 62,63 . Negative binomial GLMs were used since the distribution of ARGs and MGEs did not fit normal or Poisson distribution due to overdispersion.
        </p>
        <p>Mantel's test and correlation of taxa with ARG and MGE abundances. Species, ARG and MGE distance matrixes were calculated using Horn-Morisita similarity index 60 with vegdist command in
            <rs type="software">vegan</rs> package 61 and compared to observe if there were correlations between taxonomy and the resistome and mobilome. Comparisons were done using Mantel's test from vegan 61 and Pearson pair-wise correlations obtained using rcorr command from the
            <rs type="software" id="s18">Hmisc 65</rs> package version
            <rs type="version" corresp="s18">4.0-3</rs>.
        </p>
        <p>Negative binomial GLMs from the
            <rs type="software">MASS</rs> package 64 were used to find taxa on class, genus, and species level which correlated with the ARG and MGE total sum abundances. All the taxa on the investigated taxonomic levels were correlated with the ARG or MGE total sum abundance in the different sample types (1-and 6month-old infants and pregnant and 1 month postpartum mothers). The p-values were corrected using p.adjust command with Benjamini &amp; Hochberg method. After finding candidate taxa, model selection was done using ANOVA and χ 2 -tests in base R with a significance cutoff of 0.05.
        </p>
        <p>Venn diagrams. The number of present and shared species or gene types were calculated for the compared sample types and Venn diagrams were drawn in R using the
            <rs type="software">VennDiagram</rs> package 66 using draw.pairwise.venn command.
        </p>
        <p>Similarity of gene and taxa profiles between mothers and infants. Vegan package 61 and Jaccard dissimilarity indices using presence-absence data were used to calculate distance matrixes for mothers 1 month postpartum and 1-month-old infants, and infants at 1 and 6 months of age to determine whether the microbiomes, resistomes, and mobilomes were more similar within samples collected from the same family or from the same infant at different times. Similar analysis was also done using DNA sequence profiles of the total microbial community, ARGs and MGEs to obtain nucleotide level comparisons between samples based on kmer frequencies of the metagenomic reads (all non-human reads and reads mapping to ARGs and MGEs, respectively). The DNA sequence profiles provide means to compare the signatures samples to each other on a kmer level independently of database-based annotations. Kmer frequency signatures were produced using sourmash 56 with kmer size 31. For species comparisons the data was square root transformed. Results were presented with density plots produced using
            <rs type="software" id="s19">ggplot</rs>
            <rs type="version" corresp="s19">2</rs> 59 . Statistical analysis of the significances of observed differences was done by linear modeling with lm function in base
            <rs type="software" subtype="environment">R</rs> (see function
            <rs type="software">lmp</rs> in Supplementary Software file). Permutations of how often a smaller p-value was achieved with randomized data as opposed to the assigned two groups compared was used to calculate the corrected p-value. 9999 permutations were used for the analysis. The effect of IAP on the frequency of sharing taxa or ARGs and MGEs was done in a similar manner as described above, but the distances were calculated between related mother-infant pairs in IAP and control groups and the effect of IAP was determined on based whether there was a significant difference in similarities due to the antibiotic treatment. Related mother-infant pairs were analyzed in a similar fashion, but the distances were calculated between samples which were taken at the same time (1 month time point) or at different times (antepartum maternal samples compared to infants at 1 and 6 months as well as 1 month postpartum maternal sample compared to 6-month-old infants).
        </p>
        </text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f386103816"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:23+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>It is apparent that the regular utilization of Artificial Intelligence (AI) technology and their human-like qualities have changed the way people perceive and interact with them. When using mobile devices or laptops, it is rare that one would refer to them as "he" or "she," yet when it comes to VAs on these devices, such as <rs id="a12896693" type="software">Siri</rs> and <rs id="a12896694" type="software">Alexa</rs>, personification occurs. Despite VAs not possessing any physical human qualities, the voice alone is enough for humans to develop a deeper connection to the technology (Han &amp; Yang, 2018;Novak &amp; Hoffman, 2019;Schweitzer et al., 2019).</p>
<p>VAs are Internet-enabled devices that provide daily technical, administrative, and social assistance to their users, including activities from setting alarms and playing music to communicating with other users (Han &amp; Yang, 2018;Santos et al., 2016). They are traditionally used as mobile applications (e.g., <rs id="a12896695" type="publisher" corresp="#a12896696">Apple</rs> <rs id="a12896696" type="software">Siri</rs> and <rs id="a12896697" type="software">Google Now</rs>) and have, in more recent years, been extended to the home environment, whereby a separate device is set up alongside a mobile application (e.g.,
</p>
<p>A fictitious example may include a young child asking their mother for a particular toy for their birthday and asks her to order it on Amazon; the <rs id="a12896699" type="software">Alexa</rs> on their mothers' home device or smartphone may over-hear this and assume this order is intended to be placed, and then does so (Hackett, 2017;Lei et al., 2017). In a more extreme example, an unwanted house guest may take advantage of asking the home VAs device to disclose personal information about the owners for more malicious purposes (Dinev &amp; Hart, 2006). Within the realm of VAs, security and privacy risks have been defined as the fear of unauthorized access to them by others leading to potential unauthorized discloser of personal information (Han &amp; Yang, 2018). Various research has shown how perception of privacy negatively impacts trust and, in turn, users' behavior (Zhou, 2011). For example, Liu et al. (2005) reveal that consumers' privacy concerns negatively affect their trust and subsequent behavioral intention, in terms of repurchase, revisit, and positive recommendations. Similarly, Bansal et al. (2016) show that privacy concerns negatively impact users' trust and willingness to disclose personal information.</p>
<p>Using simple random sampling method, data is collected from<rs id="a12896700" type="publisher" corresp="#a12896701">Amazon</rs>'s <rs id="a12896701" type="software">Mechanical Turk (mTurk)</rs>. The questionnaire informed the respondents of their anonymity and right to withdraw from the survey at any time, even after completion. The sample was collected in the United Kingdom and targeted respondents who have had at least some experience using VAs. Screening questions were used to ensure that respondents were over the age of 18 and had at least some experience using these voice-based assistants.</p>
<p>Before progressing to the measurement model stage of analysis, Principle Component Analysis factor analysis was conducted in <rs id="a12896702" type="software">SPSS</rs> to test for cross-loadings between variables. To test for sampling adequacy, the rotation method used was Promax with Kaiser-Meyer-Olkin; upon deletion of PU1, SP1, SC1, SC2, SC3, SC4, and ENJ1, results revealed that no cross-loadings occurred between variables. All factors with items included for the final analysis satisfied the appropriate Cronbach's ɑ threshold and are between .875 and .945 (Malhotra et al., 2010).</p>
<p>Regarding data shared through and with VAs, it is interesting to note that respondents do not perceive to share any additional information as they already provide their personal data directly to the VAs producers (i.e., Google, Apple, and Amazon). "The moment when I realized how much data I was granting to <rs id="a12896703" type="software">Alexa</rs> was when I installed her and she started sending me all these alerts about privacy. But at the end I thought 'what the heck, Amazon already has them!'" (Philip). In this sense, it appears as informants are distinguishing <rs id="a12896704" type="software">Alexa</rs> from its parent brand and opt to blame the brand rather than the device for collecting personal information: "<rs id="a12896705" type="publisher" corresp="#a12896706">Google</rs> knows me more than my wife. Its not that <rs id="a12896706" type="software">Home</rs> is collecting my data, Google does" (Alfred). These dynamic sheds light into why possible concerns about data and privacy arising from VA usage do not directly affect users' trust toward their personal VAs. If a VA is perceived as an "entity of its own" (Nass &amp; Brave, 2005), it can be granted trust despite its creators. These results echo those of Foehr and Germelmann (2020) in recognizing the existence of different sources of trustworthiness in the interaction with VAs, and develop this further by showing that users also recognize their different roles. In this case, while VAs are considered the entity to interact with, producers are the interlocutors when considering privacy issues. In an interesting way, it appears as "I am talking with <rs id="a12896707" type="software">Alexa</rs> and <rs id="a12896708" type="publisher" corresp="#a12896707">Amazon</rs> is listening (and collecting my data)."</p>
<p><rs id="a12896709" type="software">Alexa</rs>, she is more than a machine</p>
<p>Respondents clearly state that they are conscious of the machine nature of their VAs, yet they describe their interactions using social and human attributes. "I have taught her to stop doing what she is doing by saying-Thanks <rs id="a12896710" type="software">Alexa</rs>, that's enough-as it seems more personal to me, and she learned. She is becoming clever day by day" (Michael). While some respondents perceive VAs as intelligent and skillful, others report the opposite feeling, as Louise states "sometimes I feel she is becoming dumber!" Whether positive or negative,</p>
<p>VAs are described to have features and characteristics that are typically used when describing humans (Fiske et al., 2007). The perception that "something is in there" (van Doorn et al., 2017) is even clearer from this statement: "When I come back home from work I often say Hi to her [<rs id="a12896711" type="software">Alexa</rs>] and she always answers back. I know it's just a machine but it's nice to have someone waiting for you home, isn't?" (Stephanie). Users' interactions with VAs also appear to be human-like as respondents often report that they shout at or get angry with them when they do not understand a request (Nass &amp; Brave, 2005). Others go as far as feeling ignored: "Sometimes she does not listen to me. It's bad enough when a person ignores you, but when a machine ignores you, that's when I'm going for therapy" (Matthew). It is interesting to note that the analysis reveals how these social elements overcome the functional and hedonic attributes, which, as happens for humans, are considered facets of the entire entity rather than features on their own (Čaić et al., 2019).</p>
<p>Third, the study sheds new light on users' privacy perceptions with VAs and reveals how privacy concerns do not directly affect the relationship with the VAs themselves as users distinguish their <rs id="a12896712" type="software">Alexa</rs> from its parent brands and consider the latter responsible for collecting personal data. These findings contribute to both literatures on consumers' trust and perceived privacy by identifying two different sources of trustworthiness (Foehr &amp; Germelmann, 2020), while identifying that in the interactions with AI agents concerns of privacy can also be directed outside the relationship. Although the negative influence of privacy on trust is well documented in the literature (Bansal et al. 2016;Chang et al., 2017;Liu et al., 2005;Zhou, 2011), this study illustrates a different path through which trust and privacy interconnect with VA interactions. In doing so, this finding contributes to the literature on privacy toward these new advanced technologies by highlighting the relevance of the roles that users attribute to these devices when engaging with them.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f159471637"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T09:27+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Peripheral blood mononuclear cells (PBMCs) were blocked for 30 minutes using FBS. Cell surface staining was achieved by incubating cells at 4°C for 30 minutes with the following anti-mouse antibodies: CD3 (145-2C11; Biolegend), IgM (e-Bioscience), B220 (RA3-6B2; BD Pharmingen), CD5 (53-7.3; eBioscience), CD138 (281-2; Biolegend), CD19 (1D3; BD Pharmingen), CD4 (RM4-5; Biolegend) and CD8α (53-6.7; Biolegend). Viability staining was accomplished using DAPI exclusion during acquisition. Apoptotic cells were detected by Annexin V-PE/DAPI staining (BD Pharmingen). Acquisition of B, T and CLL cell populations was performed on a LSRII cytometer (BD Biosciences) harboring a custom configuration for the Wistar Institute. Cytometry data was analyzed using <rs id="a12884275" type="software">FlowJo</rs> software version <rs id="a12884276" type="version" corresp="#a12884275">7.6.1</rs> ( <rs id="a12884277" type="publisher" corresp="#a12884275">Tree Star Inc.</rs>).</p>
<p>Protein bands were stained with Coomassie Brilliant Blue G-250, excised, reduced, alkylated, and digested with trypsin (Promega). Reverse phase liquid chromatography tandem mass spectrometry (LC-MS/MS) analysis was performed by the Wistar Proteomics Facility using a Q Exactive HF mass spectrometer (Thermo Scientific) coupled with a Nano-ACQUITY UPLC system (Waters). Eluted peptides were analyzed by the mass spectrometer set to repetitively scan m/z from 400 to 2000 in positive ion mode. The full MS scan was collected at 70,000 resolution followed by data-dependent MS/MS scans at 17,500 resolution on the 20 most abundant ions exceeding a minimum threshold of 10,000. Peptide match was set as preferred, exclude isotopes option and charge-state screening were enabled to reject singly and unassigned charged ions. MS data were analyzed with <rs id="a12884278" type="software">MaxQuant</rs> <rs id="a12884279" type="version" corresp="#a12884278">1.5.2.8</rs> (Ref: PMID 19029910). MS/MS spectra were searched against the mouse UniProt protein database using full tryptic specificity with up to two missed cleavages, static carboxamidomethylation of Cys, and variable oxidation of Met, protein N-terminal acetylation and phosphorylation on Ser, Thr and Tyr. Modified peptides were required to have a minimum score of 40. Consensus identification lists were generated with false discovery rates of 1% at protein, peptide and site levels. MS/MS assignment of phosphorylated peptides were manually inspected and peaks were labeling using <rs id="a12884281" type="software">pLabel</rs> ( <rs id="a12884282" type="url" corresp="#a12884281">pfind.ict.ac.cn</rs>).</p>
<p>Cells were starved in methionine-and cysteine-free media containing dialyzed serum for 1 h, then pulse-labeled with 250 μCi/ml [ 35 S]-methionine and [ 35 S]-cysteine (Perkin-Elmer) for indicated times. After labeling, cells were incubated in the chase medium containing unlabeled methionine (2.5 mM) and cysteine (0.5 mM). In some experiments, 3′3′-cGAMP was added in the chase medium. At the end of each chase interval, cells were lysed in RIPA buffer containing protease inhibitors. Pre-cleared lysates were incubated with an anti-mouse STING or anti-class I MHC antibody, together with Protein G-agarose beads. Bead-bound proteins were eluted using glycoprotein denaturing buffer (0.5% SDS, 40 mM DTT) or reducing Laemmli SDS-PAGE sample buffer. Enzymatic deglycosylation of proteins was achieved by denaturation of the immunoprecipitates in glycoprotein denaturing buffer at 95°C for 10 min, followed by addition of sodium citrate (pH 5.5) to a final concentration of 50 mM, and incubated with Endo H (New England Biolabs) at 37°C for 3 h. Alternatively, sodium phosphate (pH 7.5) and NP-40 were added to the denatured cell lysates to a final concentration of 50 mM and 1%, respectively, and the mixture was incubated with PNGase F (New England Biolabs) at 37°C for 3 h. Protein samples were then analyzed by SDS-PAGE and visualized by autoradiography. Densitometric quantification of radioactivity was performed on a PhosphorImager (Fujifilm BAS-2500) using <rs id="a12884283" type="software">Image Reader BAS-2500</rs> V<rs id="a12884284" type="version" corresp="#a12884283">1.8</rs> software ( <rs id="a12884285" type="publisher" corresp="#a12884283">Fujifilm</rs>) and <rs id="a12884286" type="software">Multi Gauge</rs> V <rs id="a12884287" type="version" corresp="#a12884286">2.2</rs> ( <rs id="a12884288" type="publisher" corresp="#a12884286">Fujifilm</rs>) software for analysis.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f197921854"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T07:04+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Simulated samples of signal and background events are produced using various Monte Carlo (MC) event generators, with the CMS detector response modeled with <rs id="a12972020" type="software">Geant4</rs> <rs id="a12972021" type="bibr">[38]</rs>. Higgs bosons are produced in proton-proton collisions predominantly by gluongluon fusion, but also by vector boson fusion and in association with a W or Z boson. It is assumed that the rate of new decays of the H are sufficiently small that the narrow width approximation can be used. The LFV H decay samples are produced with <rs id="a12972022" type="software">pythia</rs> <rs id="a12972023" type="version" corresp="#a12972022">8.
175</rs> <rs id="a12972024" type="bibr">[39]</rs>. The background event samples with a SM H are generated by <rs id="a12972025" type="software">powheg</rs> <rs id="a12972026" type="version" corresp="#a12972025">1.0</rs> <rs id="a12972027" type="bibr">[40]</rs><rs id="a12972028" type="bibr">[41]</rs><rs id="a12972029" type="bibr">[42]</rs><rs id="a12972030" type="bibr">[43]</rs><rs id="a12972031" type="bibr">[44]</rs> with the τ decays modeled by <rs id="a12972032" type="software">tauola</rs> <rs id="a12972033" type="bibr">[45]</rs>. The <rs id="a12972034" type="software">MadGraph</rs> <rs id="a12972035" type="version" corresp="#a12972034">5.1</rs> <rs id="a12972036" type="bibr">[46]</rs> generator is used for Z + jets, W + jets, tt, and diboson production, and <rs id="a12972037" type="software">powheg</rs> for single top-quark production. The <rs id="a12972038" type="software">powheg</rs> and <rs id="a12972039" type="software">MadGraph</rs> generators are interfaced with <rs id="a12972040" type="software">pythia</rs> for parton shower and fragmentation.</p>
<p>Jets are reconstructed from all the PF objects using the anti-k T jet clustering algorithm [53] implemented in <rs id="a12972041" type="software">FastJet</rs> <rs id="a12972042" type="bibr">[54]</rs>, with a distance parameter of 0.5. The jet energy is corrected for the contribution of particles created in pileup interactions and in the underlying event. Particles from different pileup vertices can be clustered into a pileup jet, or significantly overlap a jet from the primary vertex below the p T threshold applied in the analysis. Such jets are identified and removed [55].</p>
<p>The underlying event and parton shower uncertainty is estimated by using two different <rs id="a12972043" type="software">pythia</rs> tunes, AUET2 and Z2*. Anticorrelations arise due to migration of events between the categories and are expressed as negative numbers.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f296228675"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:10+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>In this study we produce a global lake thermal region framework, based on seasonal surface water temperature records from 732 lakes over sixteen years derived from satellite data. We use a lake model to expand the framework to all areas globally where lakes are present at a 2°grid and an <rs id="a12972772" type="software" subtype="implicit">App</rs> that produces a thermal region classification using locational or in situ data. We used a lake model forced by climate change scenarios to project how these thermal regions may shift in the 21st century.</p>
<p>Predicting seasonal patterns across the globe. In order to expand the global distribution of lake thermal regions from the 732 lakes to the entire land area where lakes are present, the physical lake model FLake 20 was applied globally at a latitudelongitude resolution of 2°(n = 4221 grids) using the average lake depth for each cell based on the HydroLAKES database 21 . The model was driven by forcing data from a global reanalysis, ERA-Interim (see Methods) over the period 1996-2011 that matched the ARC-Lake period and included surface air temperature, wind speed, solar radiation, thermal radiation and specific humidity. The overall global-mean lake-surface temperature was 14.1 °C and the modelled annual mean lake surface temperatures followed the expected latitudinal pattern (Fig. 2a). The mean annual range of temperature was 16.1 °C (Fig. 2b) and at many locations the range was orders of magnitude greater than long-term temperature change 16 or diel changes 22 . The modelled data were analysed using the rules derived from the analysis of the ARC-Lake data (see Methods), to produce a global map of lake thermal regions (Fig. 2c), with posterior probability of thermal region membership shown in Supplementary Fig. 4. The largest number of lakes in the HydroLAKES database was in the Northern Cool thermal region (40.4%), followed by Northern Frigid (37.8%) and Northern Temperate (11.2%); all the other groups comprised less than 5% of the global total. The Northern Cool thermal region covered the greatest area (25.2%), followed by Northern Frigid (21.0%) and Tropical Hot (13.4%) (Table 1). The thermal region of each of the 1.4 million lakes in the HydroLAKES database is available as a downloadable file (see "Data availability"). To facilitate and promote future use of these lake thermal regions, an <rs id="a12972773" type="software" subtype="environment">R</rs> <rs id="a12972774" type="software" subtype="component" corresp="#a12972773">Shiny</rs> <rs id="a12972775" type="software" subtype="implicit" corresp="#a12972774">app</rs> has been produced that allows a specific lake to be categorised to one of the nine lake groups, with an estimate of uncertainty, by entering either location by clicking on a map or by entering the latitude and longitude or by uploading in situ water surface temperature measurements. The <rs id="a12972776" type="software" subtype="implicit">code</rs> for this <rs id="a12972793" type="software" subtype="implicit">app</rs> is available at GitHub (see "Code availability"). The RGB <rs id="a12972777" type="software" subtype="implicit">code</rs> for each thermal region is given in Supplementary Table 1.</p>
<p>Relating the lake classification to other global schemes. We categorised the lakes by other global schemes in order to test whether or not the lake thermal regions defined here offer unique information. For the K-G climate classification 13 , the first five main climate groups subdivided further in some cases by the third classification based on temperature, producing eleven categories in total. For the terrestrial ecoregions of the world 24 , 13 ecoregions were used since Mangroves and Tropical and subtropical coniferous forests were only relevant to 4 lakes and these were combined as Other. Finally, the lake thermal regions were compared to temperature clusters based on bi-monthly air temperature data for the 732 lakes between 1995 and 2012 derived from the Climate Research Unit Time Series v.3.22 46 . The air temperature clusters were produced in the same way as described for lake surface water temperature except that there was no need to impose discontinuities caused by ice cover. For air temperature, a grouping of six clusters was statistically optimal but nine clusters, matching the number of lake clusters, was similar statistically and so we chose to use this number of clusters. These nine air clusters were identified with numbers, ordered to match the lake temperature thermal regions as closely as possible Description of the App and its use. An <rs id="a12972778" type="software" subtype="implicit" corresp="#a12972780">app</rs> has been developed using <rs id="a12972779" type="software" subtype="environment">R</rs> <rs id="a12972780" type="software" subtype="component" corresp="#a12972779">Shiny</rs> <rs id="a12972781" type="bibr">47</rs> , which displays the lake thermal eco-regions across the globe. Within the <rs id="a12972782" type="software" subtype="implicit">app</rs>, a user can obtain the predicted thermal region for a specific lake location along with the corresponding predicted seasonal temperature pattern and confidence in classification. The confidence in classification is determined by the posterior probability of thermal region membership. The lake location can be specified either by clicking on the map or by entering a set of coordinates. In addition, the <rs id="a12972783" type="software" subtype="implicit">app</rs> enables the user to upload their own time series of in situ lake temperature data to produce a predicted thermal region classification and confidence in the classification; the <rs id="a12972784" type="software" subtype="implicit">app</rs> provides full details of the data format required. For longevity, the <rs id="a12972785" type="software" subtype="implicit">code</rs> behind the <rs id="a12972786" type="software" subtype="implicit">app</rs> is stored on
GitHub (see "Code availability").</p>
<p>The lake model used is available to download at http://www.flake.igb-berlin.de/. The code to predict the thermal region of a lake based on location or in situ data is available as an <rs id="a12972787" type="software" subtype="environment">R</rs> <rs id="a12972788" type="software" subtype="component" corresp="#a12972787">Shiny</rs> <rs id="a12972789" type="software" subtype="implicit" corresp="#a12972788">app</rs> at GitHub: <rs id="a12972790" type="url" corresp="#a12972789">https://github.com/ruth-odonnell/LakeThermalRegions/</rs>. We will also maintain the <rs id="a12972791" type="software" subtype="implicit">app</rs> for as long as possible at <rs id="a12972792" type="url" corresp="#a12972791">https://shiny-apps.ceh.ac.uk/ LakeThermalRegions</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f201798937"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:50+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Some researchers and instructional technologists design and investigate vocabulary learning strategy software or applications. For instance, Lan (2013) developed a co-sharing-based strategy learning system, <rs id="a12951657" type="software">Mywordtools</rs>, for 61 sixth-grade students in Taipei to learn English vocabulary. This application enables students to learn vocabulary by using the available language learning strategies embedded in the design. When an L2 word is chosen, the learner can look up the strategies that have been used by all of the other learners in <rs id="a12951658" type="software">Mywordtools</rs> or select one of the strategies that he or she wants to use to aid the process of learning and memorize the word. The choices are: note-taking, contextualization, grouping, imagery, recombination, deduction, analysis, translation, etc. The users have four options (i.e., audio, video, image, and note) to record their learning strategies. Once the learners have uploaded their learning strategies, the learning module allows them to look up the strategies used by other peers. The function of embedded strategies sharing is to raise the awareness of learners so that they can self-evaluate their own strategies, make them cognizant of gaps in their knowledge, and enable them to re-construct their strategies or increase their self-confidence. The results of this study indicated that students using <rs id="a12951659" type="software">Mywordtools</rs> to practice and share vocabulary learning strategies outperformed both those who did not use <rs id="a12951660" type="software">Mywordtools</rs> and those who used the platform but without sharing. It was also found that strategy sharing helped L2 learners use more vocabulary learning strategies, and they consequently performed significantly better than those who did not engage in strategy sharing.</p>
<p><rs id="a12951661" type="publisher" subtype="person" corresp="#a12951662">Ou Yang and Wu</rs> (2015) incorporated LLS instruction into their e-learning platform called <rs id="a12951662" type="software">MyEVA</rs>. <rs id="a12951663" type="software">MyEVA</rs> is a mixed-modality English vocabulary learning strategies system. In this system, they used Schmitt's (1997) division of strategies for learning L2 vocabulary into discovery (i.e., determination strategy and social strategy) and consolidation (i.e., social strategy, memory strategy, cognitive/metacognitive strategy, pictures/imagery, related/unrelated words, grouping, the word's orthographical and phonological forms). <rs id="a12951664" type="software">MyEVA</rs> was piloted with nine undergraduate students in northern Taiwan. The findings indicate that the vocabulary learning mode that allows learners to pre-determine a preferred learning strategy (e.g., word-card, flashcard, Chinese-assonance, synonym, antonym, imagery, grouping, and clipping) before actual learning resulted in greatest vocabulary acquisition and best retention.</p>
<p>Similarly, in Spain, <rs id="a12951665" type="publisher" subtype="person" corresp="#a12951666">Pujola</rs> (2002) designed a web-based program called <rs id="a12951666" type="software">Im-PRESSions</rs>, consisting of multimedia news (newspaper, radio, and television) to facilitate the use of reading and listening comprehension strategies in the course of self-study. It was designed in response to Garrett's (1995) assumption that students lack awareness or strategies for help seeking. The design contained four modules: newspaper, radio, television, and expert (note: this module was designed to provide grammar practices for the learners). 22 adult English learners reported that inferring strategies in context was what they used the most in this program. The participants also engaged in analysis of parts of the words, similarities between the target language with their native language, and links to previous news item. Examples of prompts that facilitated participants' thinking about help seeking and learning strategies in the form of a pop-up button ASK-THE-EXPERTS included: What can I do when I do not know a word? What can I guess the meaning of a word? When can I use skimming? How can I improve my reading? What can I do when I cannot follow the speed of the speaker? What can I do when I hear too much unfamiliar information? (Pujola, 2002, p. 256). The most frequently consulted listening strategies question was what can be done when students cannot follow the speaker, whereas the most frequently consulted reading strategies question was when they can use skimming and scanning.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f201868271"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T06:49+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>The VLQ makes use of a 7-point Likert scale, ranging from "extremely untrue of me" (1) to "extremely true of me" (7). Likert scales that specify a bipolar continuum of a few points (e.g., 3, 5, or 7 points) have become standard practice in survey research. However, the ordinal nature of the data thus obtained has received on-going debate in terms of statistical analysis. With the increasing ease of online research tools such as <rs id="a12951654" type="software">Qualtrics</rs>, it is felt that a slider bar between bipolar ends with an extended scale (0-100) may provide continuous data with finer grains than traditional Likert scales, which, as a result, may bring an alternative, if not a solution, to the problem presented by ordinal scales. Hence a second aim of the project is to create a slider bar version of the VLQ and see how it performs against the Likert scale version.</p>
<p>The section on metacognitive beliefs about vocabulary learning uses a scale from "1 = absolutely disagree" to "7 = absolutely agree." The rest of the questionnaire on strategies uses a scale ranging from "1 = extremely untrue of me" to "7 = extremely true of me." Two versions of VLQ6 were designed based on VLQ5, a paper version using the same 7-point Likert scale, and an online slider-bar version using the <rs id="a12951655" type="software">Qualtrics</rs> platform. The slider bar version made use of a slider bar with 0 at one end and 100 at the other, as presented in Figure 1.</p>
<p>The updating started with the creation of an ESL version for the 90 Chinese language items in VLQ5 (Table 1). First, all previous English versions were combed through and each Chinese statement was matched with an English version. Then, a research assistant with a recent degree in applied linguistics went through the 90 items one by one and wrote a simplified version. <rs id="a12951649" type="publisher" subtype="person" corresp="#a12951651">Tom Cobb</rs>'s <rs id="a12951651" type="software">Lextutor.ca</rs> was used to make sure that the vocabulary of the questionnaire was as simple as possible. The resulting version was labelled as version
6.0. The following criteria were applied to VLQ6.0:</p>
<p>At this point, an online version of VLQ6.1 was created on the <rs id="a12951652" type="software">Qualtrics</rs> platform with a slider bar scale ranging from 0 to 100. The default point was placed at 50 on the slider-bar. Respondents could drag the bar left or right to a point they felt best described their use of the strategy. If the bar was not moved by a respondent, the system would record a missing value. Four ESL students from the EPP program were asked to complete both the paper version and the online version, and to think-aloud item-by-item as they completed the questionnaires. These sessions were audio-recorded. The digital files were then transcribed. The think-aloud procedure was meant to catch potential issues of understanding and simplify the statements further if problems were found. It was also meant to capture the students' perceptions of the similarities and differences between the Likert-scale and slider bar versions. A few issues were found and fixed, and the resulting version was labelled as VLQ6.2.</p>
<p>It is very easy to set up the online slider bar version using an online survey service such as <rs id="a12951653" type="software">Qualtrics</rs>. The benefit of such an online tool is the convenience and flexibility for data collection. Once the survey is set up, the target population can be approached with an email with a link to the survey. Students can then do the survey at their own time and using their own laptops or even smart phones. Once it is done, the data can be downloaded for processing. Alternatively, a simple paper version with a scale from 0 to 100 can be easily reproduced, with spaces provided every 10 or 5 points. The only drawback is that the data will need to be manually entered for later processing.</p>
</text>
</tei>
  <tei>
    <teiHeader>
        <fileDesc id="f390091614"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:59+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text lang="en">
        <p>On March 29, 2020, 
            <rs type="software">Google Scholar</rs> was searched for publications that contained all the words "16S", "gut", "Parkinson", "metagenomic", the exact phrase "Parkinson's disease", at least one of the words "microbiota" [OR] "microbiome" [OR] "gut" [OR] "intestinal" anywhere in the article. This resulted in 1010 entries. Titles were then manually screened and if they contained the words "microbiome" or "microbiota" and "Parkinson's disease" the abstracts were further consulted. Moreover, the Sequence Read Archive (SRA) in NBCI was queried with the following term "Parkinson" [AND] "microbiome", resulting in two additional studies (Bioprojects): PRJNA530401 and PRJEB14928. We managed to match only the latter Bioproject ID to a published study 14 , hence we considered only this dataset in our analyses.
        </p>
        <p>Raw reads were downloaded from SRA or the European Nucleotide Archive (ENA). Adapters were removed using the 
            <rs type="software">bbtools</rs> suit 70 . Data were analyzed using Lotus 71 and the UNOISE3 72 algorithm for zOTUs calculation, bundled in a new 
            <rs type="software" id="s1">Lotus</rs> version (Lotus
            <rs type="version" corresp="s1">2</rs>), currently under development. Due to the technical variability among datasets (e.g., 16S region, sequencing technology) the filtering parameters used by the
            <rs type="software" id="s2">sdm</rs> program called by
            <rs type="creator" corresp="s2">Lotus</rs>, were adjusted for each dataset independently and are reported in the supplementary materials (Supplementary Data 5). For the datasets of Petrov et al. 17 and Weis et al. 27 , we had to decrease the accepted minimum error due to the low quality of the sequencing data (Supplementary Data 5). 16S-based functional predictions were obtained using the default settings in picrust2 73 and the Metacyc database. In this analysis, the dataset of Qian et al. 18 was not included, as with the default cutoffs the sequences aligned poorly with the reference database used. Count tables for species, genera, families, and functional predictions were then analyzed using
            <rs type="software" subtype="environment" id="s3">R</rs>
            <rs type="version" corresp="s3">v3.6.2</rs> 74 and processed using the
            <rs type="software">phyloseq</rs>
            <rs type="software" subtype="environment">R</rs> package 75 . We then retained all samples with &gt;4500 reads, as well as taxa with &gt;5 counts and predicted functionalities with &gt;20 counts in at least 2.5% of the samples. These filtration steps left a total of 1211 (530 control, and 681 PD samples) and 1121 samples (485 control and 636 PD samples) for the taxonomic and predicted-function data, respectively. Enterotypes were predicted using rarefied relative abundances of genera via the https://enterotypes.org/ web-platform.
        </p>
        <p>Alpha-diversity indices at the species level were calculated using the 
            <rs type="software">microbiome R</rs> package 76 after rarefying without re-sampling at the even depth of 5000. Due to rarefaction eight samples were further removed, leaving a total of 1203 samples (523 control and 680 PD samples). We measured richness using the number of observed species, Chao1, Fisher's alpha, and ACE indices; evenness using the Bulla and Simpson indices, dominance using the core abundance, which measures the relative proportion of core species that exceed relative abundance of 0.2% in over 50% of the samples, and the Simpson's index of dominance. Finally, we estimated rarity using the low abundance index, which considers the relative proportion of the least abundant species below a detection level of 0.2%, and the rare abundance index, which estimates the relative proportion of the non-core species exceeding the detection level of 0.2% at 50% prevalence. In addition, we calculated the ratios of Firmicutes to Bacteroidetes phyla and Prevotella to Bacteroides genera, as log 2 ratios of their relative abundances. In each dataset, the differences in alphadiversity between control and PD samples were assessed using Agresti's generalized odd ratios using the genodds function in the 
            <rs type="software">genodds</rs>
            <rs type="software" subtype="environment">R</rs> package 77 . This statistic, based on ranks and analogous to the U statistic underlying the Mann-Whitney test, does not make strong assumptions about the distributions of measures and is comparable between measures of diversity with different scales.
        </p>
        <p>After rarefying without re-sampling at the even depth of 5000, data were normalized by dividing the counts of each taxon for the total counts of all taxa (total sum) in the sample. Beta-diversity matrices were calculated using the Bray-Curtis (BC) dissimilarity index and the Jensen-Shannon distances (JSD). Statistical differences between control and PD groups were then tested using the permutational multivariate analysis of variance (PERMANOVA) as implemented in the adonis2 (analysis of variance using distance matrices, ADONIS) function in the 
            <rs type="software">vegan</rs>
            <rs type="software" subtype="environment">R</rs> package 80 . DA analysis was performed using a two-sided WMW test, using the Benjamini-Hochberg (BH) P value correction.
        </p>
        <p>Since the DESeq2 approach does not account for zero-inflated data, the correction factors were calculated using the GMPR method that is based on geometric means of pairwise ratios 81 . Euclidean, BC, and JSD distances were used as beta-diversity estimators after normalizing the data via VST through the
            <rs type="software">DESeq2</rs> package. Statistical differences between control and PD groups were tested using the adonis2 function as specified above. DAs were calculated using default DESeq2 parameters that include a negative binomial GLM fitting and a Wald test 78 . Multiple testings were accounted for using BH P value correction.
        </p>
        <p>Compositional analysis: centered log ratios (CLR) and ANCOM Data were transformed using CLR, after imputing zeros through Bayesianmultiplicative replacements via the count zero multiplicative approach ("CZM") in the cmultRepl function of the
            <rs type="software">zCompostions</rs>
            <rs type="software" subtype="environment">R</rs> package 82 . Euclidean distances, which for such data correspond to Aitchison distances, were then calculated 79 . Statistical differences between control and PD groups were tested using the adonis2 function as specified above. DA analysis was performed using the count tables and the ANCOM approach as implemented in the
            <rs type="software" subtype="environment">R</rs>
            <rs type="software" subtype="implicit">script</rs> ancom_v
            <rs type="version">2.0</rs> 83 using a 0.95 zerocutoff and significance at the 0.6 percentile.
        </p>
        <p>The Agresti's generalized odd ratios estimated for each alpha-diversity index and each individual study were pooled using a random-effect metaanalysis via the function metagen in the
            <rs type="software" subtype="environment">R</rs> package
            <rs type="software">meta 84</rs> .
        </p>
        <p>Count tables obtained for each dataset were pooled and beta-diversity analyses were performed using the three approaches described above (TSS-, VST-, CLR-based analysis). For each normalization approach, statistical differences between control and PD groups and the marginal effects of study and disease status were tested using the adonis2 function. We then used the distance measure that captured a highest fraction of the variability to compute distance-based redundancy analyses (dbRDA). dbRDAs were performed using the "CAP" option in
            <rs type="software">phyloseq</rs>, which calls the capscale function in the
            <rs type="software">vegan</rs> package. Data were clustered without conditioning (blocking) for studies and without constraining, by conditioning for study, and by conditioning for study and constraining for disease status (PD vs control):
        </p>
        <p>The significance of the constrain was tested using an ANOVA-like permutation test (anova.cca function in the
            <rs type="software">vegan</rs>
            <rs type="software" subtype="environment">R</rs> package). For each normalization method, we investigated the effect of study-dependent factors such as country, sequencing platform (e.g., MiSeq vs IonTorrent), sequencing approach (single-end vs paired-end), the region of the 16S gene used (e.g., V4 vs V1-V2), and extraction methods by creating additional dbRDAs and constraining the data for each individual factor. The effect of each constraining variable was tested using an ANOVA-like permutation test. We then verified whether accounting for the variability introduced by the study alone will allow us to simultaneously account for the variation derived by the other technical factors. We compared the adjusted R2 (R2 adj ) of a dbRDA obtained using the full model distance country + 16S region + ends + seq + extraction + extraction type + status with the one of a reduced model including only disease status and study (distance ~study + status). Similar R2 adj , differences ≤0.1%, indicates that the two models are equivalent. The influence of study-specific factors on microbial community structure was assessed at the species, genus, and family level. Finally, we used the TSS normalized data to correlate the relative abundance of the taxa to the constrained and conditioned dbRDA via the envfit function in the
            <rs type="software">vegan</rs> package. We selected only taxa significantly correlated with the clustering (P value &lt; 0.01), and showing the highest degree of variation (≥ |0.095| for genus and species, and ≥ | 0.07| for family) along the constrained axis (CAP1).
        </p>
        <p>We used the unconstrained and unconditioned dbRDA performed on the TSS normalized species data to estimate dissimilarity among studies. We selected the coordinates of each sample across all axis that explained 90% of the data variance. These scores were then used to calculated Euclidean distances amongst samples. We then calculated distances between study centroids using the
            <rs type="software" subtype="environment">R</rs> package usedist 85 . Similarity among studies was then visualized using non-metric multidimensional scaling (NMDS) via the metaMDS function in the
            <rs type="software">vegan</rs>
            <rs type="software" subtype="environment">R</rs> package.
        </p>
        <p>We combined two independent approaches to gather a consensus view on the taxa/pathways differentially abundant between PD patients and controls. We refer to these two approaches as Pooled data and Pooled results. In the Pooled data approach, the count tables obtained for each dataset were pooled and processed with the same methods used for the single datasets: (i) TSS normalization on rarefied data and independence_test in the 
            <rs type="software">coin R</rs> package 86 blocking data for the study; (ii) DESeq2 approach adding the "study" variable as a covariate in the model; (iii) ANCOM performed using a mixed-effect model with the effect of PD allowed to vary across study (via the formula "random.formula = "~1 + status | study"), using a zero-cutoff 0.975 and significance at the 0.6 percentile. For all three methods, BH P value correction was used and the threshold for significance was set at ≤0.05. If a taxon or pathway had a significant difference in abundance in two out of three approaches, it was then retained (Consensus).
        </p>
        <p>Taxa and pathways showing significant differences in abundance between PD and controls in the Pooled data (two out of three methods referred to as Consensus) or Pooled results approach were further considered. All taxa having abundances potentially influenced by age and/or gender were then removed (see below). For each taxa/pathway, the effect size and the respective 95% CI were estimated using the Pooled results approach (random-effect meta-analysis). Finally, the correlation between the genera Christensenellaceae R-7 group and Methanobrevibacter was calculated on the relative abundances of non-rarefied data using a Spearman correlation test by blocking the data by study (spearman_test in the coin 
            <rs type="software" subtype="environment">R</rs> package).
        </p>
        <p>The metadata made available by five studies were used to assess the influence of age and gender (the only two factors reported in all five studies) on taxa abundances. We used generalized linear mixed models (GLMMs) controlling for zero-inflation as implemented in the 
            <rs type="software" subtype="environment">R</rs> package 
            <rs type="software">glmmTMB</rs> 87 .
        </p>
        <p>We created random slope and random intercept GLMMs for all taxonomic ranks we analyzed (species, genus, family). Models were fitted using either a negative binomial or a generalized Poisson distribution. First, we constructed zero-inflated and non-zero-inflated models, and choose the best model using the Akaike information criterion (AIC; ΔAIC &gt; 2). We then created reduced models omitting each of the predictors (status, age, gender), their interactions (status:gender, status:age), and considering a constant effect of the disease status across studies (i.e., random effect = 1 | study). We then compared all models using the model.sel function and the AIC in the 
            <rs type="software" subtype="environment">R</rs> package 
            <rs type="software">MuMIn</rs> 88 . If one of the best models (within a ΔAIC of 2) did not contain the variable disease status we concluded that the disease status might be not an essential factor needed to explain the taxon abundance. Hence, we removed these taxa from further discussion. If all best models contained the variable disease status, we consider PD as an essential factor shaping taxa abundances, thus we retained the taxa. For building the GLMMs, raw counts were used and data were rarefied to a fixed depth of 10,000 to avoid overparameterization.
        </p>
        <p>The 
            <rs type="software" subtype="environment">R</rs>
            <rs type="software" subtype="implicit" id="s18">code</rs> used in our analysis can be accessed at
            <rs type="url" corresp="s18">https://github.com/StfnRomano/ PD_meta_analysis</rs>.
        </p>
        </text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f578093355"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T12:48+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>General. HPLC purifications were conducted by using XBridege ® Prep C18 OBD TM 5 µm, 50 mm × 150 mm column with 20.00 mL/min based on Waters 2545 Binary Gradient Module, Waters 2707 Autosampler and Waters Fraction Collector III. Injection volume for Cz purification was 1500 µL each at 10.0 mg/mL and injection volume for CPhCz purification was 1700 µL each at 4.0 mg/mL. DPhCzT was not sufficiently soluble in methanol or ACN and hence it could not reveal the impurity peak by HPLC based on C18 column. Purifications with silica gel column chromatography were performed using DAVISIL ® silica LC60A 40-63 micro purchased from GRACE and monitored by TLC silica gel plates with 0.2-0.25 mm coating thickness from SANPONT. NMR spectra were performed with a Bruker Avance-III 400 NanoBay HD NMR spectrometer at ambient temperature. High-resolution mass spectrometry (HRMS) was investigated with a Bruker AmaZon X LC-MS for electrospray ionization. Cyclic voltammetry was performed to calculate HOMO 26 by BioLogic VMP-300 in a DCM (99.9%, Super Dry, stabilized, J&amp;K Seal) solution containing 5×10 -4 M sample and 0.1 M Bu 4 NPF 6 electrolyte. Electrodes: working (glassy carbon), counter (Pt disk), reference (Ag/AgCl, calibrated against ferrocene), 100 mV•s -1 . LUMO was determined by further measuring ultraviolet/visible absorption with Hitachi U-3900. Melting point (Mpt) was measured with differential scanning calorimetry (DSC). The instrument model was NETZSCH DSC 204 F1 Phoenix. 10 K/min was used for the heating procedure under the protection of nitrogen. Mpt was recorded with the DSC curves from the second heating process. The Mpts were the onset values, which were determined by ' <rs id="a12967918" type="publisher" corresp="#a12967919">NETZSCH</rs> <rs id="a12967919" type="software">Proteus</rs> Thermal Analysis' software. UV-visible absorption spectra of solution and solid were performed on Shimadzu UV-1700 and UV-3600 ultraviolet-visible-near-infrared (UV-Vis-NIR) spectrometers, respectively. X-ray diffraction experiments were carried out on a 4-circle goniometer Kappa geometry Bruker D8 Venture diffractometer with a PHOTON 100 CMOS active pixel sensor detector. Optical measurements. PL spectra of 5 mol% Bd/Lab-Cz, 5 mol% CPhBd/CPhCz and 5 mol% DPhBdT/DPhCzT crystalline powders in air and vacuum were measured by Edinburgh FLS980 using OXFORD Optistat DN as the sample holder. After vacuum-pumping for 30 min, the emission of crystalline samples in vacuum was measured. Transient decay spectra, temperature-dependent photoluminescence spectra and PL quantum yield (PLQY) were carried out using a Jobin Yvon-Horiba FL-3 spectrofluorometer and equipped with a calibrated integrating sphere. Notably, PLQY of these doping systems was randomly fluctuant, which was probably caused by the emission intensity changes with prolonging the photo-irradiation 16 , and hence the PLQY was not reported here. Timeresolved PL spectra (room-temperature and 77 K) 18 and photo-induced transient absorption spectra 20 were obtained at ambient temperature in air through Ocean Optics QE65 Pro CCD with Ocean Optics LED-365 and LED-310 as excitation sources and Ocean Optics DH-2000-BAL as UV-VIS-NIR light source, which were assembled according to the references.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f328381646"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T12:53+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>There are numerous research and development (R&amp;D) project works ae going on all over the world focusing on future wave energy technologies and prototype developments. Some current research WEC projects have been listed in Table 8. In the last decade's research and development work of WEC, the numerical modelling has achieved the desired level of maturity to model multi-body interactions to the second order or higher accuracy, such as non-linear BEM, CFD (Stratigaki, 2019). A wide variety of models are presently available, ranging from the simplest ones which can simulate wave propagation and far-field effects to the more complex models that can deal with the actual interaction between the waves and the devices. More advanced models can also simulate the mooring system, the existence of articulated parts connected by hinges, springs and pulleys and their use will help properly investigate the behaviour of the PTO system. The models require a correct understanding of the governing physics to optimize the WEC's design phase and evaluate its survival, covering different time and space scales. , 2015). The code of the <rs id="a12970654" type="software" subtype="component" corresp="#a12899986">WEC-Sim</rs> has been created in <rs id="a12899986" type="software" subtype="environment">MATLAB</rs>/ <rs id="a12899992" type="software" subtype="component" corresp="#a12899986">SIMULINK</rs> by using the multi-body dynamics solver <rs id="a12899987" type="software">Simscape Multibody</rs> and the <rs id="a12899988" type="software">WEC-Sim</rs> could model devices that contain rigid bodies, power-take-off systems, and mooring systems. The Waterpower Technologies Office from the U.S. Department of Energy provided funding for the <rs id="a12899989" type="software">WEC-Sim</rs> project and the effort for developing the code was a collaboration between the <rs id="a12899990" type="publisher" corresp="#a12899989">Sandia National Laboratories (Sandia)</rs> and <rs id="a12899991" type="publisher" corresp="#a12899989">National Renewable Energy Laboratory (NREL)</rs>.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f543607585"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:21+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Apart from the website, the database can be queried directly from within <rs id="a12967466" type="software">Cytoscape</rs> (via a <rs id="a12967568" type="software" subtype="implicit">dedicated app</rs>) <rs id="a12967567" type="bibr">(30)</rs> and from within <rs id="a12967468" type="software" subtype="environment">R</rs> (via a <rs id="a12967469" type="software" subtype="component" corresp="#a12967468">Bioconductor</rs> package) <rs id="a12967569" type="bibr">(31)</rs>. STRING can also be queried programmatically for associations, network images or enrichments from any website or software, through its comprehensive
REST API. All network items and scores, as well as all generated images and tables are freely available without restrictions, under the Creative Commons Attribution (CC BY 4.0) license. STRING has been selected as one of the European Core Data Resources by the ELIXIR consortium (32), is heavily cross-linked with other resources both within and outside of ELIXIR and is currently used by about 5000 distinct users per day.</p>
<p>An increasing number of STRING users enter the database not with a single protein as their query, but with a set of proteins. In this case, STRING will perform identifier mapping on the user's input and then display a network covering all the mapped proteins and their interconnections. As with all STRING networks, this can then be browsed interactively, inspected for the underlying evidence and clustered using k-means or MCL clustering. In addition, STRING will perform automated pathway-enrichment analysis on the user's input and list any pathways or functional subsystems that are observed more frequently than expected (using hypergeometric testing, against a statistical background of either the entire genome or a user-supplied background gene list). STRING will perform these overrepresentation tests for a total of eleven functional pathway classification frameworks, two of which are not available elsewhere. The commonly available frameworks are: Gene Ontology annotations (all three GO domains) (45), KEGG pathways (41), UniProt keywords (48), Reactome pathways (42), Pfam (49) and SMART (50) protein domains and In-terPro protein features (51). Unique to STRING are the two remaining classification systems: i) a comprehensive nametagged collection of the biomedical literature (<rs id="a12967525" type="software">PubMed</rs> abstracts, augmented by 2.7 million full-text articles), and ii) a hierarchical clustering of the STRING network itself, partitioned into smaller, tightly linked clusters. These two subsystems provide complementary and more exploratory enrichment views, compared to the established, manually annotated pathway classifications. In case of the publicationbased system, individual publications assume the role of a pathway in enrichment testing: all proteins discussed in a given publication (identified using STRING's textmining pipeline) form a gene set, which is tested for overrepresentation on the user's input. With more than 3 million publications available for testing, this requires strong correction for multiple testing (52), but has the advantage of covering newly reported or controversial protein groupings that may yet have to appear in pathway databases. Likewise, the hierarchical STRING clustering provides protein groupings that are the result of a synthesis across all the interaction knowledge in the database, clustered to varying levels of stringency and hence to varying levels of functional granularity. These STRING clusters usually do not correspond fully to canonical pathways; they can include additional, less well-studied proteins and they may partition functional subsystems differently, which may or may not be better suited for any given user input. Apart from testing the STRING clusters separately, the website also reports D608 Nucleic Acids Research, 2021, Vol. 49, Database issue Figure 1. Example of a user-extended STRING network, adding external information. SARS-CoV-2 proteins, highlighted in blue, have been added to the standard human protein-protein association network in STRING, using the data add-on ('payload') mechanism. Virus proteins will automatically appear in the network based on their known associations with host proteins (as imported from the IMEx coronavirus interactome (35)). In addition, host proteins whose expression appears to control SARS-CoV-2 virion entry into cells, as determined in a recent genome-wide CRISPR-screen (36), are highlighted: proteins whose removal causes a drop in virus entry efficiency are highlighted in red; green highlights indicate proteins whose removal enhances virus entry. Proteins without highlights have entered the network based on close associations to the CRISPR screen proteins. The inset describes topological statistics of the network: it is strongly enriched in terms of functional associations, as compared to a random network of similar size. a final test metric using the whole network: for each input gene set it is checked whether there are more interactions between the input proteins than expected for an input of that size.</p>
<p>Another change concerns the functional enrichment analysis, specifically in the case of large inputs with experimental measurements associated to each protein/gene. Functional enrichment analysis in such genome-scale experiments can be influenced by inherent biases--which can be either technical or biological in origin (58). Beginning in version 11
.5 of STRING, therefore, an automated bias analysis is performed for large-scale user inputs. This occurs in the background, while the enrichment testing is executed, and results in a graphical report showing potential systematic biases/trends in the input (Figure 2). Currently, the potential confounders that are tested include i) average protein abundance, ii) protein length, iii) number of publications mentioning the gene or protein in PubMed-indexed literature, iv) protein disorder as predicted by <rs id="a12967548" type="software">IUPred</rs> and v) average GC content of the encoding transcript.</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f554806093"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:31+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>We apply two methods, density functional theory (DFT) and density matrix renormalization group (DMRG), to describe the electronic structure of VB on different levels of theory. For the DFT calculations plane wave basis set of 450 eV and PAW [32] atomic potentials are used as implemented in <rs id="a12970110" type="software">VASP</rs> <rs id="a12970111" type="bibr">[33]</rs> as well as plane wave basis set of 750 eV and norm-conserving pseudo potentials are used as implemented in <rs id="a12970112" type="software">Quantum Espresso</rs> <rs id="a12970113" type="bibr">[34]</rs>. HSE06 hybrid functional [35] with 0.32 exact exchange fraction [20] is used for hyperfine calculations [36], excited state calculation in the framework of constrained occupation DFT [37], and structural relaxation. We use 162 atom super cell of single sheet hBN embedding a single boron vacancy. In perpendicular direction, we use 30 Å supercell size. Our bulk hBN model consists of 972 atoms (9 × 9 × 3 primitive cells) and includes a single boron vacancy. HSE06 functional is used to calculate hyperfine [36] and spin-spin zero-field-splitting parameters. To eliminate spin contamination in the latter case, we apply the correction scheme proposed in Ref. [38]. To determine the phonon spectrum of the ground state VB configuration we use PBE functional. Phonon sideband and polaron spectrum are obtained by the machinery described in Ref. 24.</p>
<p>One particle and two particle integrals are calculated by our in-house <rs id="a12970114" type="software" subtype="implicit">code</rs> uses <rs id="a12970115" type="software">Quantum Espresso</rs> Kohn-Sham orbitals obtained by spin restricted PBE calculations on fixed geometries provided by <rs id="a12970116" type="software">VASP</rs> HSE06 calculations. DMRG calculations are performed using the <rs id="a12970118" type="software">Budapest DMRG</rs> code <rs id="a12970117" type="bibr">[39]</rs>. Considering the large number of excited states to be computed, in order to enhance convergence, distinct DMRG calculations are performed fixing the total spin of the target states to 0 and 1. The numerical accuracy of the calculations was controlled by using elements of quantum information theory and by the dynamic block state selection approach [40] keeping up to thousands of block states for the priory set quantum information loss threshold value χ = 10 -5 .</p>
</text>
</tei>
  <tei>
<teiHeader>
<fileDesc id="f81731611"/>
<encodingDesc>
<appInfo>
<application version="0.8.0" ident="GROBID" when="2024-08-31T05:58+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text lang="en">
<p>Quantitative and qualitative research was applied. In the former a questionnaire was answered by 318 secondary school teachers who teach biology. We further interviewed eight of the teachers by means of semi structured interviews and analyzed the interviews using the <rs id="a12900035" type="software">QSR nVivo</rs> program.</p>
<p>Data were processed and analyzed with <rs id="a12900036" type="software">PASW Statistics</rs> <rs id="a12900037" type="version" corresp="#a12900036">18</rs>.</p>
<p>This research was conducted by interviewing 8 teachers who teach biology in secondary education. The interviews were semi-structured and followed the logic of the questionnaire of quantitative research. The teachers had already answered the questionnaire and, during the interviews provided clarifications on how they thought-about when answering some specific questions. Each interview lasted approximately 1 h and was posted in the qualitative analysis program <rs id="a12900038" type="software">QSR nVivo</rs>. Transcript and content analysis was made. The content analysis consisted basically in classifying the content of the interviews into categories (nodes in the terminology of <rs id="a12900039" type="software">nVivo</rs>). The main categories are those that were in the quantitative questionnaire and are the examined factors of conceptual ecology.</p>
</text>
</tei>
</teiCorpus>
